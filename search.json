[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aprendizado Supervisionado, uma abordagem com implementações em R",
    "section": "",
    "text": "Prefácio\nApostila de Aprendizado Supervisionado com Implementações em R.",
    "crumbs": [
      "Prefácio"
    ]
  },
  {
    "objectID": "AS_Intro.html",
    "href": "AS_Intro.html",
    "title": "1  Introdução ao aprendizado supervisionado",
    "section": "",
    "text": "1.1 Aprendizado\nNo contexto da ciência de dados (data science) o aprendizado consiste em adquirir um determinado comportamento a partir dos dados disponíveis. Mais especificamente, este comportamento a ser aprendido está relacionado a previsão de uma determinada resposta de interesse em função de outras variáveis ou atributos disponíveis nos dados. Esta resposta pode ser quantitativa, ou seja, medida em uma esclaa real, ou qualitativa, constando de um conjunto finito de possibilidades. Para tal, obviamente faz-se necessário a utilização de dados. Estes, por sua vez, apesar de estarem atualmente disponíveis em abundância, tanto nas organizações privadas quanto via acesso público, nem sempre estão prontos para análise.\nExistem dois campos que regem a teoria do Aprendizado:\nAlguns métodos surgiram no contexto do Aprendizado estatístico, subcampo da Estatística, tais como as árvores de decisão, aprendizado por reforço e máquinas de vetores de suporte, enquanto outros surgiram no contexto da Inteligência Artificial, subcampo das Ciências da computação, tais como as redes neurais e o aprendizado profundo. Hoje é difícil separar ambos campos, apesar de o aprendizado de máquina ser muito mais popular.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao aprendizado supervisionado</span>"
    ]
  },
  {
    "objectID": "AS_Intro.html#aprendizado",
    "href": "AS_Intro.html#aprendizado",
    "title": "1  Introdução ao aprendizado supervisionado",
    "section": "",
    "text": "Estatístico (statistical learning);\nde Máquina (machine learning).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao aprendizado supervisionado</span>"
    ]
  },
  {
    "objectID": "AS_Intro.html#um-pouco-de-história",
    "href": "AS_Intro.html#um-pouco-de-história",
    "title": "1  Introdução ao aprendizado supervisionado",
    "section": "1.2 Um pouco de história",
    "text": "1.2 Um pouco de história\nInicialmente serão citados alguns teóricos importantes para a estatística frequentista e paramétrica. Porém, alguns métodos propostos, por exemplo a análise discriminante linear de Fisher é utilizada hoje como método de aprendizado supervisionado para classificação.\n\nWillian Gosset (Student), 1908-1909: criou o teste t e a distribuição t de Student quando trabalhava na cervejaria Guiness. Sua intenção era criar uma aproximação da distribuição normal para amostras de tamanhos limitados.\nRonald Fisher, 1920-1940: Criou vários testes e conceitos estatísticos importantes, como a ANOVA, análise discriminante linear, p-valor, entre outros. Seus principais desenvolvimentos foram realizados especialmente enquanto trabalhava na estação agrícola Rothamsted Research no Reino Unido.\nGeorge Box, 1948-1992: Considerado uma dos maiores pesquisadores em estatística do século XX, desenvolveu trabalhos e métodos em controle de qualidade, planejamento de experimentos, séries temporais e inferência Bayesiana. Cunhou a famosa frase: “All models are wrong, some are usefull”.\n\n\n\n\n\n\nGosset, Fisher e Box\n\n\n\n\nAlguns estatísticos foram importantes para definir termos que hoje são populares no contexto da teoria do aprendizado supervisionado e mais amplamente da ciência e análise de dados.\n\nJohn Tukey (1962, 1977): Cunhou o termo análise exploratória de dados, com o objetivo de incentivar a ênfase em gráficos, tabelas e limpeza de dados para resumir dados e apontar suas tendências.\nJeff Wu (1980): Formulou o termo data science e inclusive recomendou que a área de conhecimento estatística fosse renomeada para ciência de dados.\n\nA Figura 1.1 expõe fotos dos estatísticos Tukey e Wu.\n\n\n\n\n\n\n\n\nFigura 1.1: Tukey e wu\n\n\n\n\n\nVários bioestatísticos de Stanford tiveram contribuições importantes no campo do aprendizado estatístico. Bradley Efron desenvolveu nas décadas de 70 e 80 o método bootstrap, um método que visa estimar o erro a partir da amostragem com reposição amplamente usado em inferência e em aprendizado de máquina. Jerome Friedman desenvolveu o método floresta aleatória e o reforço de gradiente. Trevor Hastie propôs os modelos aditivos generalizados e Robert Tibshirani propôs a regularização via LASSO. Estes autores, Figura 1.2, tiveram outras contribuições importantes na estatística e computação.\n\n\n\n\n\n\n\n\nFigura 1.2: Efron, Friedman, Hastie e Tibishirani\n\n\n\n\n\nA densa teoria do aprendizado estatístico foi cunhada pelo matemático russo Vladmir Vapnik, Figura 1.3, visando obter um modelo preditivo a partir dos dados. Inicialmente não foi proposto um método específico, mas o arcabouço teórico necessário para sustentar a capacidade de generalização de modelos obtidos a partir de amostras limitadas, porém suficientes. Posteriormente Vapnik e co-autores propuseram as máquinas de vetores de suporte, método aplicado com sucesso até hoje em problemas de aprendizado supervisionado. Seus principais trabalhos foram publicados na década de 90.\n\n\n\n\n\n\n\n\nFigura 1.3: Vapnik\n\n\n\n\n\nNomes importantes da computação incluem Christopher Bishop, com grande contribuição em redes neurais e Andrew Ng, Figura 1.4 com contribuições e militância recentes em aplicações, pesquisa e ensino de aprendizado profundo e inteligência artificial.\n\n\n\n\n\n\n\n\nFigura 1.4: Bishop e Andrew\n\n\n\n\n\n\n1.2.1 Um brasileiro importante\nCarlos Guestrin, Figura 1.5, é um brasileiro que tem feito um excelente trabalho na área de aprendizado por reforço, é professor da Universidade de Stanford, foi diretor sênior de Machine Learning da Apple (2016-2021). Co-criador do método recente de reforço por gradiente extremo (extreme gradient boosting), usado com sucesso em aprendizado supervisionado.\n\n\n\n\n\n\n\n\nFigura 1.5: Carlos Guestrin\n\n\n\n\n\nA seguir será classificado o aprendizado supervisionado, sendo expostos exemplos práticos de aplicação.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao aprendizado supervisionado</span>"
    ]
  },
  {
    "objectID": "AS_Intro.html#aprendizado-supervisionado",
    "href": "AS_Intro.html#aprendizado-supervisionado",
    "title": "1  Introdução ao aprendizado supervisionado",
    "section": "1.3 Aprendizado Supervisionado",
    "text": "1.3 Aprendizado Supervisionado\nSeja um conjunto de variáveis de entrada, independentes ou preditores \\(\\mathbf{x} = [x_1, x_2, ..., x_k]^T\\) e uma variável dependente ou supervisora \\(y\\). Dado uma amostra de observações para tais variáveis, o aprendizado supervisionado visa prever o comportamento ou resultado de \\(y\\), considerando valores futuros de \\(\\mathbf{x}\\), \\(\\mathbf{x}_0\\). O aprendizado supervisionado pode ser classificado em dois tipos:\n\nRegressão, \\(y \\in \\mathbb{R}\\), ou seja, quando a resposta ou supervisor pode ser medida em uma escala real (há casos para variáveis de processos de contagem em uma taxa média de ocorrência, entre outros);\nClassificação, \\(y \\in \\{A, B, C, ...\\}\\), ou seja, quando a resposta pertence a um conjunto finito de categorias.\n\nA Figura 1.6 ilustra o resultado de um exemplo de aplicação de regressão linear simples para prever o consumo em função da renda. O modelo foi treinado com um conjunto de dados de 33 observações e aplicado em 11 observações de teste. É importante observar uma boa aproximação do modelo aos dados, especialmente para os dados de teste ou futuros. É importante a utilização de métricas de ajuste para melhor avaliar a acuracidade dos modelos. Algumas métricas comumente usadas serão abordadas posteriormente.\n\n\n\n\n\n\n\n\nFigura 1.6: Consumo e renda nos EUA de 1950-1993\n\n\n\n\n\nA Figura 1.7 apresenta o resultado gráfico de um exemplo de aplicação de classificação, sendo estimado um modelo de regressão logística para classificação de pessoas com diabetes em relação ao nível de glicose, onde “1” = diabético e “0” = não diabético. O modelo prevê a probabilidade de pertencer a classe 1, isto é, \\(P(y=1|x)\\). A discriminação é realizada considerando a probabilidade intermediária, isto é,\n\\[\n\\bigg\\{\\begin{matrix}\ny = 1, \\text{ se } p&gt;0,5, \\\\\ny = 0, \\text{ cc}.\n\\end{matrix}\n\\]\n\n\n\n\n\n\n\n\nFigura 1.7: Regressão logística para diabetes em função do nível de glicose\n\n\n\n\n\nNeste segundo exemplo da Figura 1.8 considera-se além da glicose a idade do paciente para obter um modelo de regressão logística de forma a classificar pacientes com diabetes. Foram consideradas 294 observações para treinamento e 98 para teste do modelo.\n\n\n\n\n\n\n\n\nFigura 1.8: Regressão logística para diabetes em função do nível de glicose e idade",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao aprendizado supervisionado</span>"
    ]
  },
  {
    "objectID": "AS_Intro.html#aprendizado-não-supervisionado",
    "href": "AS_Intro.html#aprendizado-não-supervisionado",
    "title": "1  Introdução ao aprendizado supervisionado",
    "section": "1.4 Aprendizado Não-supervisionado",
    "text": "1.4 Aprendizado Não-supervisionado\nSeja um conjunto de \\(N\\) observações, \\(\\mathbf{x}_1\\), \\(\\mathbf{x}_2\\), …, \\(\\mathbf{x}_N\\), de \\(k\\) variáveis independentes \\(\\mathbf{x}=[x_1, x_2, ..., x_k]^T\\). O aprendizado não-supervisionado visa obter informações a partir dos próprios dados, sem a necessidade de um supervisor ou variável dependente. Constitui-se de técnicas de agrupamento e de redução de dimensionalidade.\nA Figura 1.9 expõe um gráfico do resultado de um agrupamento por \\(k\\)-médias considerando distintos índices de demografia dos EUA. São plotados os dois índices mais importantes.\n\n\n\n\n\n\n\n\nFigura 1.9: Algoritmo k-means para agrupar estados americanos segundo índices de criminalidade\n\n\n\n\n\nA Figura 1.10 expõe um gráfico de dispersão para idade e peso de órgãos retirados de 30 focas do Cabo que morreram como consequência não intencional da pesca comercial. Devido a alta correção entre as variáveis, \\(R = 0.95\\), foi realizada uma análise de componentes principais para obter uma nova variável ou componente principal que represente ambas as variáveis, reduzindo a dimensionalidade do problema. A nova variável obtida, plotada em azul escuro, representa 98% da variabilidade das variáveis originais.\n\n\n\n\n\n\n\n\nFigura 1.10: Análise de componentes principais para encontrar uma nova variável que represente o peso e a idade",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao aprendizado supervisionado</span>"
    ]
  },
  {
    "objectID": "AS_Intro.html#implementação-em-r",
    "href": "AS_Intro.html#implementação-em-r",
    "title": "1  Introdução ao aprendizado supervisionado",
    "section": "1.5 Implementação em R",
    "text": "1.5 Implementação em R\n\n1.5.1 Exemplo de problema de regressão\nCarregando as bibliotecas (pacotes) para análise.\n\nlibrary(AER) # para base de dados\nlibrary(ggplot2) # para gráficos\n\nCarregando base de dados.\n\ndata(USConsump1993)\n# ?USConsump1993\n\n\nConsumo &lt;- data.frame(USConsump1993)\nhead(Consumo)\n\nSeparando 75% dos dados para treino do modelo e 25% para teste.\n\ntr &lt;- round(0.75*nrow(Consumo))\n\nset.seed(9)\ntreino &lt;- sample(nrow(Consumo), tr, replace = F)\n\nConsumo.tr &lt;- Consumo[treino,]\nConsumo.te &lt;- Consumo[-treino,]\n\n\nhead(Consumo.tr)\n\n\nhead(Consumo.te)\n\nTreinando um modelo de regressão linear simples.\n\nlm1 &lt;- lm(expenditure ~ income, data = Consumo.tr)\nsummary(lm1)\n\nPlotando o modelo com dados de treino.\n\nggplot(data = Consumo.tr, aes(x = income, y = expenditure)) + \n   geom_point(color = 'red', size = 2) +\n   geom_smooth(method = \"lm\", formula = y ~ x) +\n   xlab(\"renda\") + \n   ylab(\"consumo\") + theme_bw()\n\nRealizando previsão com o modelo.\n\npredict(lm1, newdata = data.frame(income = 9000))\n\nPrevisão para todos dados de teste.\n\nConsumo.te$exp_pred &lt;- predict(lm1, \n                               newdata = \n                                 data.frame(income = \n                                              Consumo.te$income))\n\n# head(Consumo.te)\n\nPlotando o modelo com os dados de teste.\n\nggplot() + \n  geom_point(data = Consumo.te, aes(x = income, y = expenditure), size = 2) +\n  geom_smooth(method = \"lm\", formula = y ~ x, \n              data = Consumo.tr,\n              aes(x = income, y = expenditure)) +\n  xlab(\"renda\") + \n  ylab(\"consumo\") + theme_bw()\n\nCriando uma função de métricas de desempenho.\n\nmetrics &lt;- function(obs, pred) {\n  \n  RSE &lt;- sum((obs - pred)^2)\n  SST &lt;- sum((obs - mean(obs))^2)\n  R2 &lt;- 1 - RSE/SST \n  \n  MAE &lt;-  mean(abs(obs - pred))\n  \n  RMSE &lt;- sqrt(mean((obs - pred)^2))\n  \n  return(\n    data.frame(RMSE = RMSE,\n               MAE = MAE,\n               R2 = R2))\n}\n\nDesempenho do modelo para dados de teste.\n\nmetrics(Consumo.te$expenditure, Consumo.te$exp_pred)\n\n\n\n1.5.2 Exemplo de problema de classificação\n\nlibrary(mlbench)\n\n\ndata(PimaIndiansDiabetes2)\n# ?PimaIndiansDiabetes2\n\n\nPimaIndiansDiabetes2 &lt;- na.omit(PimaIndiansDiabetes2)\nPimaIndiansDiabetes2$diabetes &lt;- ifelse(PimaIndiansDiabetes2$diabetes==\"neg\",0,1)\n\ndados &lt;- PimaIndiansDiabetes2\n\nhead(dados)\n\n\nset.seed(7)\ntreino &lt;- sample(nrow(dados), 0.75*nrow(dados))\ndados_treino &lt;- dados[treino,]\ndados_test &lt;- dados[-treino,]\n\nObtendo um modelo de regressão logística simples a partir dos dados de treino considerando apenas uma variável regressora, o nível de glicose.\n\nmodel1 &lt;- glm( diabetes ~ glucose, data = dados_treino, family = binomial)\nsummary(model1)\n\nPlotando o modelo com os dados de treino.\n\nggplot(dados_treino, aes(glucose, diabetes)) +\n  geom_point(aes(col = as.factor(diabetes)), alpha = 0.5) +\n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\"), fill = \"grey\", col = \"black\") +\n  labs(x = \"Concentracao de glicose\", y = \"Probabilidade de ter diabetes\", col = \"diabetes\") +\n  theme_bw()\n\nPlotando o modelo com os dados de teste.\n\nggplot() +\n  geom_point(data = dados_test, \n             mapping = aes(glucose, diabetes, col = as.factor(diabetes)), \n             alpha = 0.5) +\n  geom_smooth(data = dados_treino, \n              mapping = aes(glucose, diabetes), \n              method = \"glm\", \n              method.args = list(family = \"binomial\"), \n              col = \"black\") +\n  labs(x = \"Concentracao de glicose\", y = \"Probabilidade de ter diabetes\", col = \"diabetes\") + theme_bw()\n\nPrevisão com os dados de teste.\n\ndados_test$prob &lt;- predict(model1, \n                           newdata = data.frame(glucose = dados_test$glucose), \n                           type = 'response')\ndados_test$y_pred &lt;- ifelse(dados_test$prob &gt; 0.5, 1, 0)\n\nhead(dados_test)\n\nMatriz de confusão para dados de teste.\n\ncm1 &lt;- table(data = dados_test$diabetes, model = dados_test$y_pred)\ncm1\n\nProporção de observações de teste classificadas corretamente.\n\nmean(dados_test$diabetes == dados_test$y_pred)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao aprendizado supervisionado</span>"
    ]
  },
  {
    "objectID": "AS_Intro.html#referências",
    "href": "AS_Intro.html#referências",
    "title": "1  Introdução ao aprendizado supervisionado",
    "section": "Referências",
    "text": "Referências\nBishop, Christopher M. “Neural networks for pattern recognition”. Oxford university press, 1995.\nBox, George EP, and Kenneth B. Wilson. “On the experimental attainment of optimum conditions.” Breakthroughs in statistics: methodology and distribution. New York, NY: Springer New York, 1992. 270-310.\nChen, Tianqi, and Carlos Guestrin. “Xgboost: A scalable tree boosting system.” Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining. 2016.\nChipman, Hugh A., and V. Roshan Joseph. “A conversation with Jeff Wu.” (2016): 624-636.\nFisher, Ronald Aylmer. “Statistical methods for research workers.” Statistical methods for research workers. 6th Ed (1936).\nStudent. “The probable error of a mean.” Biometrika (1908): 1-25.\nTukey, John W. “Exploratory data analysis.” Reading/Addison-Wesley (1977).\nVapnik, Vladimir. “The nature of statistical learning theory.” John Wiley google schola 2 (1995): 259-275.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao aprendizado supervisionado</span>"
    ]
  },
  {
    "objectID": "AS_reg_ols.html",
    "href": "AS_reg_ols.html",
    "title": "3  Regressão linear simples, múltipla e polinomial",
    "section": "",
    "text": "3.1 Regressão linear simples, múltipla e polinomial",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regressão linear simples, múltipla e polinomial</span>"
    ]
  },
  {
    "objectID": "AS_reg_ols.html#regressão-linear-simples-múltipla-e-polinomial",
    "href": "AS_reg_ols.html#regressão-linear-simples-múltipla-e-polinomial",
    "title": "3  Regressão linear simples, múltipla e polinomial",
    "section": "",
    "text": "3.1.1 Regressão linear simples\nSeja um problema onde deseja-se prever uma resposta contínua, \\(y \\in \\mathbb{R}\\), em função de uma única variável independente também contínua, \\(x \\in \\mathbb{R}\\). Conforme observado graficamente na Figura 3.1, pode-se considerar em diversos casos a aproximação de uma função linear para tal relação.\n\n\n\n\n\n\n\n\nFigura 3.1: Exemplo de conjunto de dados para regressão linear simples\n\n\n\n\n\nTal aproximação pode ser descrita pela Equação à seguir, onde \\(\\hat{y}\\) consiste no valor predito de \\(y\\), \\(\\beta_0\\) e \\(\\beta_1\\) são coeficientes chamados de intercepto ou constante e coeficiente linear ou inclinação, respectivamente. Enquanto \\(\\beta_0\\) mede o valor da resposta prevista para \\(x=0\\), \\(\\beta_1\\) consiste na mudança média da resposta para o incremento de uma unidade de \\(x\\).\n\\[\n\\hat{y} = \\beta_0 + \\beta_1x\n\\]\nA Figura 3.2 plota para os dados plotados anteriormente a linha azul do modelo de regressão linear simples obtido.\n\n\n\n\n\n\n\n\nFigura 3.2: Modelo de regressão linear simples\n\n\n\n\n\nPara este caso inicial os coeficientes de regressão estimados são:\n\n\n(Intercept)           x \n  10.927762    2.728817 \n\n\nA primeira pergunta a ser feita seria como estimar tais coeficientes de regressão. Pode-se pensar em estimativas que minimizem o erro de previsão. Conforme, plotado na Figura 3.3 em linhas verticais vermelhas, o erro de previsão seria a diferença entre o valor experimental e o previsto, \\(\\varepsilon_i = y_i - \\hat{y}_i\\), \\(i = 1, ...., N\\).\n\n\n\n\n\n\n\n\nFigura 3.3: Erros do modelo de regressão linear simples\n\n\n\n\n\nNeste sentido, as observações da variável dependente ou resposta podem ser descritas conforme segue.\n\\[\n\\begin{aligned}\ny_i = \\hat{y}_i + \\varepsilon_i \\\\\ny_i = \\beta_0 + \\beta_1x_i + \\varepsilon_i \\\\\n\\end{aligned}\n\\]\nTomando \\(N\\) observações retiradas da população de interesse, \\((x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\\), pode-se pensar em um modelo que minimize os erros de previsão para a amostra disponível. Uma vez que o erro é normalmente distribuído, com média nula e variância \\(\\sigma_\\varepsilon^2\\), sendo os resíduos normalmente distribuídos, com média nula e variância igual a \\(\\sigma_\\varepsilon^2\\), \\(\\varepsilon \\sim N(0,\\sigma_\\varepsilon^2)\\), pode-se trabalhar a minimização da soma dos quadrados dos erros de previsão, \\(\\sum_{i=1}^{N}\\varepsilon_i^2\\).\nA Tabela 3.1 expõe algumas das observações para os dados plotados anteriormente, bem como os valores preditos e erros associados.\n\n\n\n\nTabela 3.1: Dados para regressão linear simples e previsões\n\n\n\n\n\n\nx\ny\ny_hat\nerro\n\n\n\n\n1.093333\n9.389117\n13.91127\n-4.5221504\n\n\n4.106335\n29.785746\n22.13320\n7.6525477\n\n\n9.138367\n29.774229\n35.86469\n-6.0904613\n\n\n24.613730\n78.467084\n78.09412\n0.3729679\n\n\n38.963445\n126.273606\n117.25185\n9.0217521\n\n\n56.628047\n155.103936\n165.45531\n-10.3513755\n\n\n\n\n\n\n\n\nA sintaxe anteriormente apresentada pode ser escrita de forma matricial, conforme segue, onde \\(\\mathbf{x}_{[2 \\times N]}\\) consiste em uma matriz relacionada às observações independentes, com uma coluna de valores unitários associada à \\(\\beta_0\\) e outra com as observações de \\(x\\), portanto associada a \\(\\beta_1\\). \\(\\mathbf{y}_{[N\\times1]}\\) consiste no vetor de observações da resposta, \\(\\mathbf{\\varepsilon}_{[N\\times1]}\\) consiste no vetor de erros ou resíduos de previsão e \\(\\mathbf{\\beta}_{[2\\times1]}\\) consiste em um vetor de coeficientes.\n\\[\n\\begin{aligned}\n\\mathbf{y} = \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\varepsilon}\n\\end{aligned}\n\\]\nTais elementos matriciais podem ser escritos de forma genérica conforme segue.\n\\[\n\\mathbf{X} =\n\\begin{bmatrix}\n1 & x_{1}\\\\\n1 & x_{2}\\\\\n\\vdots & \\vdots \\\\\n1 & x_{N}\\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 1.093\\\\\n1 & 4.106\\\\\n\\vdots & \\vdots \\\\\n1 & 96.206\\\\\n\\end{bmatrix}\n\\]\n\\[\n\\mathbf{y} =\n\\begin{bmatrix}\ny_{1}\\\\\ny_{2}\\\\\n\\vdots \\\\\ny_{N}\\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n9.389\\\\\n29.786\\\\\n\\vdots \\\\\n290.028\\\\\n\\end{bmatrix}\n\\]\n\\[\n\\mathbf{\\varepsilon} =\n\\begin{bmatrix}\n\\varepsilon_{1}\\\\\n\\varepsilon_{2}\\\\\n\\vdots \\\\\n\\varepsilon_{N}\\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n-4.522\\\\\n7.653\\\\\n\\vdots \\\\\n16.571\\\\\n\\end{bmatrix}\\text{,    e} \\\\\n\\]\n\\[\n\\mathbf{\\beta}^T =\n\\begin{bmatrix}\n\\beta_0 & \\beta_1\\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n10.928  & 2.729 \\\\\n\\end{bmatrix}\n\\]\nTomando tal notação, a soma dos quadrados dos erros pode ser descrita como \\(\\sum_{i=1}^{N}\\varepsilon_i^2 = \\mathbf{\\varepsilon}^T\\mathbf{\\varepsilon}\\). Desenvolvendo tal expressão tem-se:\n\\[\n\\begin{aligned}\nL(\\mathbf{\\beta}) = \\mathbf{\\varepsilon}^T\\mathbf{\\varepsilon} = (\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta}) \\\\\n\\mathbf{y}^T\\mathbf{y} - 2\\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{\\beta}\n\\end{aligned}\n\\]\nPara minimizar \\(L\\) em relação à estimativa de \\(\\mathbf{\\beta}\\), pode-se diferenciar tal quantidade em relação à \\(\\mathbf{\\beta}\\) e igualar a zero:\n\\[\n\\begin{aligned}\n\\frac{\\partial L}{\\partial \\mathbf{\\beta}} = -2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\mathbf{\\beta} = 0 \\\\\n\\hat{\\mathbf{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}(\\mathbf{X}^T\\mathbf{y})\n\\end{aligned}\n\\]\nTal solução constitui as chamadas equações normais de mínimos quadrados.\nAo obter um modelo de regressão é sempre importante observar os resíduos, os quais devem ser normalmente distribuídos, independentes e homocedásticos. Tais pressuposições implicam que o modelo obtido por mínimos quadrados é paramétrico, uma vez que pressupõe-se uma distribuição para os resíduos. Neste curso de aprendizado não supervisionado, serão estudados diversos modelos que não implicam qualquer distribuição acerca dos resíduos ou dados sendo, portanto, livres de distribuição e ditos não-paramétricos. A Figura 3.4 expõe alguns gráficos dos resíduos os erros do modelo obtido, os quais podem servir para avaliação da normalidade do modelo, bem como para identificação de observações não usuais.\n\n\n\n\n\n\n\n\nFigura 3.4: Resíduos do modelo de regressão linear simples\n\n\n\n\n\n\n\n3.1.2 Regressão linear múltipla\nNo caso de onde há múltiplas variáveis independentes ou regressoras de interesse, \\(x_1, x_2, ..., x_k\\) pode-se considerar o modelo com um coeficiente linear associado a cada variável, isto é:\n\\[\n\\hat{y}_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\cdots + \\beta_kx_{ik} = \\beta_0 + \\sum_{j=1}^{k}\\beta_jx_{ij},  \n\\]\nou de forma matricial com \\(\\mathbf{X}_{[N\\times (k+1)]}\\) e \\(\\mathbf{\\beta}_{[(k+1) \\times 1]}\\):\n\\[\n\\begin{aligned}\n\\hat{\\mathbf{y}} = \\mathbf{X}\\mathbf{\\beta}\n\\end{aligned},\n\\] com:\n\\[\n\\mathbf{X} =\n\\begin{bmatrix}\n1 & x_{11} & x_{12} & \\cdots & x_{1k}\\\\\n1 & x_{21} & x_{22} & \\cdots & x_{2k} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n1 & x_{N1} & x_{N2} & \\cdots & x_{Nk} \\\\\n\\end{bmatrix}, e\\\\\n\\]\n\\[\n\\mathbf{\\beta}^T =\n\\begin{bmatrix}\n\\beta_0 & \\beta_1 & \\cdots & \\beta_k\\\\\n\\end{bmatrix}. \\\\\n\\]\nAs estimativas de mínimos quadrados, deduzidas para o caso simples, \\(\\hat{\\mathbf{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}(\\mathbf{X}^T\\mathbf{y})\\), também atendem ao caso múltiplo. Uma forma de medir o ajuste do modelo obtido aos dados seria a partir do cálculo do coeficiente de determinação múltipla, \\(R^2\\), conforme segue,\n\\[\n\\begin{align}\nR^2 = 1- SS_{E}/SS_T \\\\\nR^2 = 1- \\frac{\\sum_{i=1}^{N}(y_i-\\hat{y}_i)^2}{\\sum_{i=1}^{N}(y_i-\\overline{y}_i)^2},\n\\end{align}\n\\]\nou utilizando outras métricas de ajuste. É interessante que tais métricas sejam também calculadas para dados futuros ou de teste, de forma a evitar sobreajuste do modelo. Uma observação importante é relacionada à utilização da Análise de variância (ANOVA) para obtenção de tais métricas. Além do \\(R^2\\), ao se utilizar a ANOVA, há a possibilidade de calcular o coeficiente de determinação ajustado, \\(R^2_{adj}\\), isto é:\n\\[\nR^2_{adj} = 1 - \\frac{SS_{E}/(N-k)}{SS_T/(N-1)}\n\\]\nEsta métrica é mais honesta uma vez que penaliza o modelo pela adição de mais coeficientes. O \\(R^2\\) sempre aumentará com adição de novos coeficientes, enquanto o \\(R^2_{adj}\\) será mais baixo casos novos termos adicionados não apresentem significância estatística. Entretanto, quando outros métodos de aprendizado não paramétricos são utilizados, especialmente os que não tem origem na estatística mas na computação, tal métrica não pode ser calculada. No contexto de aprendizado supervisionado é mais interessante realizar a validação cruzada e estimar o desempenho do modelo em dados futuros, viabilizando a comparação de tipos distintos de modelos.\nO teste t para os coeficientes de regressão pode ser calculado para medir a significância de cada coeficiente e testar as seguintes hipóteses para cada coeficiente de regressão.\n\\[\n\\begin{matrix}\nH_0: \\beta_j = 0 \\\\\nH_1: \\beta_j \\neq 0, j = 1, ..., k\\\\\n\\end{matrix}\n\\]\nO teste é calculado conforme segue, onde \\(C_j\\) é o valor da \\(j\\)-ésima linha e \\(j\\)-ésima coluna da matriz \\((\\mathbf{X}^T\\mathbf{X})^{-1}\\), correspondente a \\(\\beta_j\\). A hipótese nula é rejeitada se \\(t_{0j} &gt; t_{[\\alpha/2,N-k]}\\), onde \\(\\alpha\\) é o nível de significância de interesse.\n\\[\nt_{0j} = \\frac{\\hat{\\beta}_j}{\\sqrt{C_jSS_E/(N-k)}}\n\\]\nAnalogamente, estimativas intervalares para os coeficientes podem ser obtidas como segue, as quais consistem em intervalos que garantem \\(\\gamma = 1-\\alpha\\) de confiança de encontrar os verdadeiros valores dos coeficientes de regressão.\n\\[\n\\hat{\\beta}_j \\pm t_{[\\alpha/2,N-k]}\\sqrt{C_jSS_E/(N-k)}\n\\]\nA Figura 3.5 ilustra graficamente um modelo de regressão linear múltipla para prever o preço de carros usados em função da idade e quilometragem.\n\n\n\n\n\n\n\n\nFigura 3.5: Modelo de regressão linear múltipla para prever o preço de carros usados em função da quilometragem e idade\n\n\n\n\n\nPara este caso, os coeficientes estimados são apresentados a seguir. Estes foram estimados nas unidades originais das variáveis. Porém, para fins de inferência, é importante padronizar as variáveis regressoras.\n\n\n(Intercept)     Mileage         Age \n21.54307990 -0.05314626 -0.84234949 \n\n\nO modelo pode ser escrito conforme segue.\n\\[\n\\hat{y} = 21,543 - 0,0531x_1 - 0,842x_2\n\\]\nO teste t para os coeficientes de regressão resultante é apresentado a seguir. As duas variáveis regressoras foram antes padronizadas para evitar efeito de escala e unidade de medida. Como \\(|t_{\\alpha/2,N-k}|\\) = 1.987, tem-se que todos os coeficientes são significativos, uma vez que \\(t_{0j} &gt; t_{\\alpha/2,N-k}\\), \\(\\forall j = 1, ..., k\\). Pode-se também considerar o \\(p-value\\) que é a probabilidade de erro na rejeição da hipótese nula, \\(H_0\\), associada ao valor calculado \\(t_0\\). Se \\(p-value &lt; \\alpha\\), rejeita-se \\(H_0\\).\n\n\n\nCall:\nlm(formula = Price ~ Mileage + Age, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.5429 -1.2795 -0.2982  1.5275  7.1967 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  13.7478     0.2483  55.364  &lt; 2e-16 ***\nMileage      -1.9401     0.4406  -4.404 3.02e-05 ***\nAge          -3.3175     0.4406  -7.530 4.42e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.356 on 87 degrees of freedom\nMultiple R-squared:  0.8239,    Adjusted R-squared:  0.8198 \nF-statistic: 203.5 on 2 and 87 DF,  p-value: &lt; 2.2e-16\n\n\nA Tabela 3.2 apresentam os intervalos de confiança de 0,95 para os coeficientes codificados.\n\n\n\n\nTabela 3.2: Intervalos de confiança para os coeficientes de regressão múltipla\n\n\n\n\n\n\n\nX2.5..\nX97.5..\n\n\n\n\n(Intercept)\n13.254224\n14.241331\n\n\nMileage\n-2.815845\n-1.064452\n\n\nAge\n-4.193231\n-2.441839\n\n\n\n\n\n\n\n\nAlém de considerar termos de múltiplas variáveis, é possível considerar a interação entre estas, colocando na matriz \\(\\mathbf{X}\\) colunas com multiplicação ou produto das variáveis de interesse e no modelo termos da forma \\(\\beta_{ij}x_ix_j\\). Para o caso em estudo um modelo de regressão múltipla com interação ficaria conforme segue.\n\n\n\nCall:\nlm(formula = Price ~ Mileage * Age, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.1277 -1.4495 -0.2671  1.7765  6.1130 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  13.0272     0.3381  38.532  &lt; 2e-16 ***\nMileage      -2.0909     0.4247  -4.924 4.06e-06 ***\nAge          -3.7683     0.4477  -8.417 7.55e-13 ***\nMileage:Age   0.8844     0.2952   2.997  0.00357 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.255 on 86 degrees of freedom\nMultiple R-squared:  0.8405,    Adjusted R-squared:  0.835 \nF-statistic: 151.1 on 3 and 86 DF,  p-value: &lt; 2.2e-16\n\n\nÉ importante esclarecer que no caso de modelos de regressão múltipla com termos de interação, a matriz \\(\\mathbf{X}\\) apresentará mais que \\(k+1\\) colunas, sendo adicionadas as colunas das respectivas interações, como sendo o produto das colunas das variáveis envolvidas em cada termo adicionado.\n\n\n3.1.3 Codificação de variáveis categóricas em regressão múltipla\nRetomando o problema de regressão do preço de revenda de carros considerando o ano e a quilometragem, imagine uma terceira variável regressora que determina o modelo ou tipo do veículo. Tal variável apresenta três categorias, Mazda6, Accord e Maxima. A Tabela 3.3 expõe algumas observações do conjuunto de dados.\n\n\n\n\nTabela 3.3: Dados de preços de carros usados\n\n\n\n\n\n\n\nCarType\nAge\nMileage\nPrice\n\n\n\n\n1\nMazda6\n3\n17.8\n15.9\n\n\n2\nMazda6\n2\n19.0\n16.4\n\n\n50\nAccord\n10\n150.5\n7.9\n\n\n51\nAccord\n5\n65.2\n11.7\n\n\n70\nMaxima\n1\n38.6\n20.0\n\n\n71\nMaxima\n1\n42.1\n20.0\n\n\n\n\n\n\n\n\nPara trabalhar com a variável modelo e qualquer outra variável qualitativa ou categórica em regressão múltipla, pode-se utilizar de variáveis dummy também conhecidas como dicotômicas ou binárias. No caso de três categorias, como no exemplo acima, duas variáveis dummy seriam suficientes. Ao criar uma coluna denominada Mazda6, com 1, se Mazda6 e 0, caso contrário e, de forma análoga, uma coluna para Accord, caso uma determinada observação receba 0 em ambas colunas, o modelo do carro seria o Maxima.\nSeja \\(x_1\\) o ano, \\(x_2\\) a quiometragem,\n\\[\nx_3 = \\bigg\\{\n\\begin{matrix}\n1, \\text{ se Mazda6} \\\\\n0, \\text{ cc}\\\\\n\\end{matrix}\n\\] e\n\\[\nx_4 = \\bigg\\{\n\\begin{matrix}\n1, \\text{ se Accord} \\\\\n0, \\text{ cc}\\\\\n\\end{matrix}\n\\]\nO modelo de regressão pode ser escrito conforme segue.\n\\[\n\\hat{y}_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\beta_3x_{i3} + \\beta_4x_{i4}\n\\]\nFica claro que, neste tipo de modelo, o coeficiente \\(\\beta_3\\) é uma constante adicionada a \\(\\beta_0\\) caso o modelo do carro seja Mazda6. Uma explicação análoga pode ser feita para \\(\\beta_4\\). Portanto, tais termos não mudam a inclinação do modelo, apenas o intercepto. É possível adicionar termos de interação entre variáveis dicotômicas e variáveis contínuas, o que na prática serviria para mudar a inclinação ou coeficientes das variáveis contínuas. Retomando a criação de variáveis dicotômicas para o exemplo em questão, tem-se as colunas com tais variáveis criadas na Tabela 3.4.\n\n\n\n\nTabela 3.4: Dados de carros usados com colunas das variáveis dicotômicas\n\n\n\n\n\n\n\nCarType\nAge\nPrice\nMileage\nMazda6\nAccord\nMaxima\n\n\n\n\n1\nMazda6\n3\n15.9\n17.8\n1\n0\n0\n\n\n2\nMazda6\n2\n16.4\n19.0\n1\n0\n0\n\n\n50\nAccord\n10\n7.9\n150.5\n0\n1\n0\n\n\n51\nAccord\n5\n11.7\n65.2\n0\n1\n0\n\n\n70\nMaxima\n1\n20.0\n38.6\n0\n0\n1\n\n\n71\nMaxima\n1\n20.0\n42.1\n0\n0\n1\n\n\n\n\n\n\n\n\nO modelo com estas variáveis e as duas consideradas anteriormente ficaria conforme exposto a seguir.\n\n\n\nCall:\nlm(formula = Price ~ Mileage + Age + Mazda6 + Accord, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.588 -1.632 -0.178  1.196  6.861 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  14.5616     0.4082  35.672  &lt; 2e-16 ***\nMileage      -1.8975     0.4226  -4.490 2.23e-05 ***\nAge          -3.2185     0.4253  -7.567 4.17e-11 ***\nMazda6       -2.1158     0.5750  -3.680 0.000409 ***\nAccord       -0.3257     0.5873  -0.555 0.580651    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.187 on 85 degrees of freedom\nMultiple R-squared:  0.8517,    Adjusted R-squared:  0.8447 \nF-statistic:   122 on 4 and 85 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n3.1.4 Regressão polinomial\nÉ possível em regressão simples ou múltipla realizar transformações nos preditores de forma a incluir termos polinomiais associados à uma ou mais variáveis independentes. Para o caso simples, um modelo de regressão polinomial pode ser escrito conforme segue, onde \\(p\\) é a ordem do modelo de regressão polinomial.\n\\[\n\\hat{y} = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\beta_3x^3 + ... \\beta_px^p\n\\]\nConsiderando a notação matricial, a matriz \\(\\mathbf{X}\\) fica conforme segue, podendo-se utilizar novamente as equações normais de mínimos quadrados para estimar os coeficientes, \\(\\hat{\\mathbf{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}(\\mathbf{X}^T\\mathbf{y})\\).\n\\[\n\\mathbf{X} =\n\\begin{bmatrix}\n1 & x_{11} & x_{11}^2 & \\cdots & x_{p1}^p\\\\\n1 & x_{12} & x_{12}^2 & \\cdots & x_{p2}^p \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n1 & x_{1N} & x_{1N}^2 & \\cdots & x_{pN}^p \\\\\n\\end{bmatrix}\\\\\n\\]\nSejam os dados da massa de um paciente em kg medidos ao longo de 8 meses de um programa de perda de peso, conforme Figura 3.6.\n\n\n\n\n\n\n\n\nFigura 3.6: Dados de perda de peso em função do tempo no programa\n\n\n\n\n\nConsiderando um modelo linear para tais dados, os resíduos obtidos são plotados na Figura 3.7. Pode-se observar claramente um padrão de não linearidade nos resíduos em relação aos valores ajustados, indicando o ajuste de um modelo não linear.\n\n\n\n\n\n\n\n\nFigura 3.7\n\n\n\n\n\nPode-se pensar, portanto, em um modelo de regressão quadrático para aproximar o peso em função de dias. A curva plotada na Figura 3.6 consiste em tal modelo.\n\n\n\n\n\n\n\n\nFigura 3.8\n\n\n\n\n\nO modelo quadrático obtido e associado ao gráfico anterior é exposto a seguir.\n\n\n\nCall:\nlm(formula = Weight ~ Days + I(Days^2), data = rehab)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9034 -0.5842 -0.1188  0.4774  2.6315 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.833e+02  3.521e-01  520.72   &lt;2e-16 ***\nDays        -4.565e-01  6.520e-03  -70.03   &lt;2e-16 ***\nI(Days^2)    6.930e-04  2.614e-05   26.51   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9386 on 49 degrees of freedom\nMultiple R-squared:  0.9981,    Adjusted R-squared:  0.998 \nF-statistic: 1.287e+04 on 2 and 49 DF,  p-value: &lt; 2.2e-16\n\n\nOs resíduos para o modelo quadrático são plotados na Figura 3.7.\n\n\n\n\n\n\n\n\nFigura 3.9\n\n\n\n\n\nFinalmente, tal modelo pode ser escrito conforme segue.\n\\[\n\\hat{y} = 183,3 -0,456x + 6,930\\times10^{-4}x^2\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regressão linear simples, múltipla e polinomial</span>"
    ]
  },
  {
    "objectID": "AS_vicio_var.html",
    "href": "AS_vicio_var.html",
    "title": "2  Conflito entre vício e variância no Aprendizado supervisionado",
    "section": "",
    "text": "2.1 Conflito entre vício e variância\nSeja uma variável regressora ou independente \\(x \\in \\mathbb{R}\\) e uma resposta ou variável dependente igualmente medida em uma escala contínua, \\(y \\in \\mathbb{R}\\).\nDeseja-se aproximar uma função desconhecida, \\(f(x)\\), que relaciona \\(y\\) e \\(x\\). Tal aproximação pode ser feita minimizando uma função perda, \\(L(y,f(x))\\), que mede os erros de previsão.\nA função perda mais comum para problemas de regressão é a função perda quadrática, conforme segue.\n\\[\nL(y,f(x)) = (y-f(x))^2\n\\]\nSeja um conjunto de dados disponível, \\(D = {(x_1,y_1), (x_2,y_2), ..., (x_n,y_n)}\\), conforme o observado graficamente na Figura 3.1. As observações da variável independente podem ser descritas em relação à função desconhecida adicionada de um termo de erro, \\(y = f(x) + \\varepsilon\\), \\(E(\\varepsilon)=0\\), \\(Var(\\varepsilon) = \\sigma^2_\\varepsilon\\).\nFigura 2.1: Exemplo de dados para regressão linear simples\nÉ possível decompor a esperança da função perda considerando um modelo estimado, \\(\\hat f(x)\\), conforme segue.\n\\[\nE[(y-\\hat f(x))^2] = Var[\\hat f(x)] + Vicio^2[\\hat f(x)] + Var[\\varepsilon]\n\\]\nPara um conjunto de dados disponíveis, \\(D\\), tem-se para o vício:\n\\[\n\\begin{aligned}\nVicio_D[\\hat f(x)] &= E_D[\\hat f(x) - f(x)] = E_D[\\hat f(x) - y + \\varepsilon] \\\\\nVicio_D[\\hat f(x)] &= E_D[\\hat f(x)] - E_D[y]\n\\end{aligned}\n\\]\nE para a variância:\n\\[\n\\begin{aligned}\nVar_D[\\hat f(x)] &= E_D[(\\hat f(x) - \\hat f(x))^2]\n\\end{aligned}\n\\]\nO erro associado às medições, \\(Var(\\varepsilon) = \\sigma^2_\\varepsilon\\), consiste no erro irredutível. Este erro está associado à qualidade dos dados. Portanto sua variabilidade terá implicação no erro do modelo estimado.\nSeja o ajuste aos dados disponíveis de três modelos de regressão com distintas complexidades, um linear, um cúbico e um modelo polinomial de décima ordem.\nTais modelos são da forma que segue:\n\\[\n\\begin{matrix}\ny_1 = \\beta_0 + \\beta_1x\\\\\ny_3 = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\beta_3x^3\\\\\ny_{10} = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\beta_3x^3 + \\beta_4x^4 + ... + \\beta_{10}x^{10} \\end{matrix}\n\\]\nOs modelos são estimados por mínimos quadrados e plotados com os dados na Figura 2.2.\nFigura 2.2: Modelo de regressão linear simples\nTomando um valor arbitrário de \\(x = x_0\\), e \\(M\\) conjuntos distintos de observações futuras, \\(d_1=(x_1,y_1)_1, ..., (x_n,y_n)_1\\), \\(d_2=(x_1,y_1)_2, ..., (x_n,y_n)_2\\), \\(\\ldots, d_M=(x_1,y_1)_M, ..., (x_n,y_n)_M\\), é possível medir o vício e a variância das estimativas obtidas por \\(\\hat f(x)\\) para um \\(x=x_0\\) específico. A Figura 2.3 compara graficamente o resultado de tais modelos em observações futuras via boxplots, enquanto a Figura 2.4 faz a mesma comparação usando gráficos de densidade amostral. Pelos gráficos pode-se observar que o modelo linear apresenta maior vício e menor variância, enquanto o modelo polinomial de décima ordem apresenta baixo vício, porém maior variância. Este exemplo ilustra o conflito entre vício e variância. Deve-se buscar um modelo que apresente equilíbrio entre ambas medidas.\nFigura 2.3: Box-plots do erro dos modelos em distintos conjuntos de dados\nFigura 2.4: Distribuição do erro dos modelos em distintos conjuntos de dados\nA Tabela 2.1 apresenta valores de vício, enquanto a Tabela 2.2 apresenta valores de variância dos modelos segundo a ordem ou complexidade entre os mesmos. Observa-se que o modelo de menor grau e, portanto, menor complexidade, apresenta alto vício, porém baixa variância. Já o modelo de maior grau, ou maior complexidade, apresenta baixo vício, porém alta variância.\nTabela 2.1: Vício versus complexidade dos modelos\n\n\n\n\n\n\nordem\nvicio\n\n\n\n\n1\n0.3134804\n\n\n3\n0.0002074545\n\n\n10\n0.0002207692\nTabela 2.2: Variância versus complexidade dos modelos\n\n\n\n\n\n\nordem\nvar\n\n\n\n\n1\n0.07020973\n\n\n3\n0.1268333\n\n\n10\n0.2922431\nEm casos onde o número de observações é baixo o conflito entre vício e variância fica mais claro. Considerando o exemplo plotado na Figura 2.5, pode-se observar que enquanto o modelo linear apresenta um baixo ajuste, o modelo polinomial de décima ordem apresenta um sobreajuste, uma vez que praticamente interpola os dados disponíveis. Este último apresentará um ajuste muito além do real, uma vez que sua capacidade não se confirmará para dados futuros.\nFigura 2.5: Modelos com ordens distintas ajustados a um conjunto de dados com poucas observações\nA Figura 2.6 apresenta o erro ou valor da função perda de tais modelos para dados de treino e dados de teste ou futuros.\nFigura 2.6: Erro para dados de treino e teste\nPode-se observar que para os dados de treino, à medida que se aumenta a complexidade do modelo ajustado o erro diminui. Entretanto, o mesmo não acontece para os dados futuros ou de teste do modelo.\nQuando um modelo apresenta alto erro tanto para os dados de treino quanto para os dados de teste, ele é considerado um modelo com baixo ajuste. Quando um modelo apresenta baixo erro para os dados de treino, mas alto erro para os dados de teste, ele é considerado um modelo com sobreajuste. É importante diagnosticar o sobreajuste uma vez que o bom ajuste aos dados de treino pode dar a ilusão de um modelo perfeito, porém, a avaliação do modelo em dados futuros confirma a dificuldade de generalização deste tipo de modelo.\nO melhor modelo seria aquele que apresenta mais proximidade entre os erros para os dados de treinamento e teste, que, no exemplo dado, seria o de terceira ordem.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conflito entre vício e variância no Aprendizado supervisionado</span>"
    ]
  },
  {
    "objectID": "AS_vicio_var.html#conflito-entre-vício-e-variância",
    "href": "AS_vicio_var.html#conflito-entre-vício-e-variância",
    "title": "2  Conflito entre vício e variância no Aprendizado supervisionado",
    "section": "",
    "text": "2.1.1 Referências\nBishop, Christopher M., and Hugh Bishop. “Deep learning: foundations and concepts.” (2024).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conflito entre vício e variância no Aprendizado supervisionado</span>"
    ]
  },
  {
    "objectID": "AS_vicio_var.html#implementação-em-r",
    "href": "AS_vicio_var.html#implementação-em-r",
    "title": "2  Conflito entre vício e variância no Aprendizado supervisionado",
    "section": "2.2 Implementação em R",
    "text": "2.2 Implementação em R\nCarregando as bibliotecas (pacotes) para análise.\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nCriando uma função para simular o conflito. Na prática tal função é desconhecida; portanto deseja-se aproximá-la apartir dos dados.\n\nf_sin &lt;- function(x) {\n  f &lt;- sin(2*pi*x)\n  return(f)\n}\n\nSimulando dados. x foi simulado segundo uma distribuição uniforme com limites entre 0 e 1. y foi simulado avaliando os valores de x na função criada previamente adicionando um termo de erro segundo a distribuição normal com média nula e desvio-padrão igual a 0.2.\n\nset.seed(7)\nx &lt;- runif(100,0,1)\ndata &lt;- data.frame(x = x,\n                   y = f_sin(x) + rnorm(100, 0, .2))\n\nknitr::kable(head(data))\n\nPlotando.\n\nggplot(data, aes(x,y)) + \n  geom_point(col=\"red\") +\n  theme_bw()\n\nPlotando os dados disponíveis acompanhados de três modelos de regressão polinomial simples, um linear, um cúbico e um de décima ordem.\n\ncolors &lt;- c(\"y\" = \"red3\", \"linear\" = \"grey\", \"cúbico\" = \"green2\", \"poly-10\" = \"purple\")\n\nggplot(data, aes(x,y)) + \n  geom_point(aes(color = \"y\")) +\n  geom_smooth(method = lm, formula = y ~ x, se = F, aes(color = \"linear\")) +\n  geom_smooth(method = lm, formula = y ~ poly(x, degree = 3, raw = TRUE), \n              se = F, aes(color = \"cúbico\")) +\n  geom_smooth(method = lm, formula = y ~ poly(x, degree = 10, raw = TRUE), \n              se = F, aes(color = \"poly-10\")) +\n  labs(color = \"Legend\") +\n    scale_color_manual(values = colors) +\n  theme_bw()\n\nTomando um valor arbitrário de \\(x = 075\\), e conjuntos distintos de observações futuras para medir o vício e a variância de cada modelo.\n\nset.seed(7)\n\npred &lt;- data.frame(model = numeric(3000),\n                   pred = numeric(3000),\n                   vicio = numeric(3000))\n\nx0=0.75\n\ni&lt;-1\n\nE_y &lt;- f_sin(x0)\nE_y\n\n\nfor (k in 1:999) {\n\ndata_ &lt;- data.frame(x = x,\n                    y = f_sin(x) + rnorm(100, 0, 2))\n\nlm1 &lt;- lm(y ~ x, data_)\nlm3 &lt;- lm(y ~ poly(x, degree = 3, raw = TRUE), data_)\nlm10 &lt;- lm(y ~ poly(x, degree = 10, raw = TRUE), data_)\n                    \npred1 &lt;- predict(lm1, newdata = data.frame(x=x0)) \npred3 &lt;- predict(lm3, newdata = data.frame(x=x0)) \npred10 &lt;- predict(lm10, newdata = data.frame(x=x0))  \n\nvicio1 &lt;- pred1 - E_y\nvicio3 &lt;- pred3 - E_y\nvicio10 &lt;- pred10 - E_y\n\npred$model[i] &lt;- 1\npred$model[i+1] &lt;- 3\npred$model[i+2] &lt;- 10\n\npred$pred[i] &lt;- pred1\npred$pred[i+1] &lt;- pred3\npred$pred[i+2] &lt;- pred10\n\npred$vicio[i]   &lt;- vicio1\npred$vicio[i+1] &lt;- vicio3\npred$vicio[i+2] &lt;- vicio10\n\ni &lt;- i + 3\n\n} \n\n\nggplot(pred, aes(x=as.factor(model),y=vicio, col = as.factor(model))) +\n  geom_boxplot() + \n  geom_jitter(alpha = .1) +\n  geom_hline(yintercept = 0, linetype=2) +\n  labs(x = \"ordem\", y = \"erro\") +\n  theme_bw() +\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\n\nggplot(pred, aes(x=vicio, fill = as.factor(model))) +\n  geom_density(alpha = .3, col = \"white\") + \n  geom_vline(xintercept = 0, linetype=2) +\n  labs(y=\"densidade\", x = \"erro\", fill = \"ordem\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nSimulando o conflito para um caso com poucas observações disponíveis.\n\nset.seed(7)\nxa &lt;- runif(15,0,1)\ndataa &lt;- data.frame(x = xa,\n                    y = f_sin(xa) + rnorm(15, 0, .2))\n\ncolors &lt;- c(\"y\" = \"red3\", \"linear\" = \"grey\", \"cúbico\" = \"green2\", \"poly-10\" = \"purple\")\n\nggplot(dataa, aes(x,y)) + \n  geom_point(aes(color = \"y\")) +\n  geom_smooth(method = lm, formula = y ~ x, se = F, aes(color = \"linear\")) +\n  geom_smooth(method = lm, formula = y ~ poly(x, degree = 3, raw = TRUE), \n              se = F, aes(color = \"cúbico\")) +\n  geom_smooth(method = lm, formula = y ~ poly(x, degree = 10, raw = TRUE), \n              se = F, aes(color = \"poly-10\")) +\n  labs(color = \"Legend\") +\n    scale_color_manual(values = colors) +\n  theme_bw()\n\nErro ou valor da função perda de tais modelos para dados de treino e dados de teste.\n\nset.seed(7)\nxa &lt;- runif(20,0,1)\ndataa &lt;- data.frame(x = xa,\n                    y = f_sin(xa) + rnorm(20, 0, .2))\n\nlm1a &lt;- lm(y ~ x, dataa)\nlm2a &lt;- lm(y ~ poly(x, degree = 2, raw = TRUE), dataa)\nlm3a &lt;- lm(y ~ poly(x, degree = 3, raw = TRUE), dataa)\nlm5a &lt;- lm(y ~ poly(x, degree = 5, raw = TRUE), dataa)\nlm10a &lt;- lm(y ~ poly(x, degree = 10, raw = TRUE), dataa)\n\nset.seed(27)\nxb &lt;- runif(20,0,1)\ndatab &lt;- data.frame(x = xb,\n                    y = f_sin(xb) + rnorm(20, 0, .2))\n\nMSE &lt;- function(obs, pred) {\n  MSE &lt;- mean((obs - pred)^2)\n  return(MSE)\n}\n\nMSE1_T &lt;- MSE(dataa$y, lm1a$fitted.values)\nMSE2_T &lt;- MSE(dataa$y, lm2a$fitted.values)\nMSE3_T &lt;- MSE(dataa$y, lm3a$fitted.values)\nMSE5_T &lt;- MSE(dataa$y, lm5a$fitted.values)\nMSE10_T &lt;- MSE(dataa$y, lm10a$fitted.values)\n\nyhat1 &lt;- predict(lm1a, newdata = datab)\nyhat2 &lt;- predict(lm2a, newdata = datab)\nyhat3 &lt;- predict(lm3a, newdata = datab)\nyhat5 &lt;- predict(lm5a, newdata = datab)\nyhat10 &lt;- predict(lm10a, newdata = datab)\n\nMSE1_t &lt;- MSE(datab$y, yhat1)\nMSE2_t &lt;- MSE(datab$y, yhat2)\nMSE3_t &lt;- MSE(datab$y, yhat3)\nMSE5_t &lt;- MSE(datab$y, yhat5)\nMSE10_t &lt;- MSE(datab$y, yhat10)\n\ndata_MSE &lt;- data.frame(model = rep(c(1,2,3,5,10),2),\n                       dados = c(rep(\"treino\",5),rep(\"teste\",5)),\n                       MSE = c(MSE1_T, MSE2_T, MSE3_T, MSE5_T, MSE10_T, \n                               MSE1_t, MSE2_t, MSE3_t, MSE5_t, MSE10_t))\n\nggplot(data_MSE, aes(x=model, y = MSE, \n                     col = dados,\n                     linetype = dados,\n                     pch = dados)) + \n  geom_point(size=2) +\n  geom_line() +\n  labs(x = \"ordem\") +\n  scale_x_continuous(breaks = 1:10) +\n  theme_bw()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conflito entre vício e variância no Aprendizado supervisionado</span>"
    ]
  },
  {
    "objectID": "AS_reg_ols.html#implementação-em-r",
    "href": "AS_reg_ols.html#implementação-em-r",
    "title": "3  Regressão linear simples, múltipla e polinomial",
    "section": "3.2 Implementação em R",
    "text": "3.2 Implementação em R\nA seguir será expostos as implementações necessárias para obter os resultados do capítulo. Alguns exemplos aqui expostos são distintos dos do capítulo, sendo os resultados das implementações apresentados integralmente.\n\n3.2.1 Caso 1 - regressão linear simples\nUm exemplo de regressão linear simples já foi proposto no primeiro capítulo do livro.\n\n\n3.2.2 Regressão linear múltipla\nCarregando pacote para dados e gráfico.\n\nlibrary(caTools)\nlibrary(GGally)\n\nA ?fig-dataecn expõe algumas das observações dados macroeconômicos para regressão linear múltipla.\n\ndata(longley)\n# ?longley\n# dim(longley)\nhead(longley) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGNP.deflator\nGNP\nUnemployed\nArmed.Forces\nPopulation\nYear\nEmployed\n\n\n\n\n1947\n83.0\n234.289\n235.6\n159.0\n107.608\n1947\n60.323\n\n\n1948\n88.5\n259.426\n232.5\n145.6\n108.632\n1948\n61.122\n\n\n1949\n88.2\n258.054\n368.2\n161.6\n109.773\n1949\n60.171\n\n\n1950\n89.5\n284.599\n335.1\n165.0\n110.929\n1950\n61.187\n\n\n1951\n96.2\n328.975\n209.9\n309.9\n112.075\n1951\n63.221\n\n\n1952\n98.1\n346.999\n193.2\n359.4\n113.270\n1952\n63.639\n\n\n\n\n\nNa Figura 3.10 visualiza-se a correlação entre as variáveis presentes no conjunto de dados.\n\nggpairs(longley) + theme_bw()\n\n\n\n\n\n\n\nFigura 3.10\n\n\n\n\n\nPadronizando os dados para evitar efeitos de escala e unidades de medida.\n\nlongley_scaled &lt;- data.frame(scale(longley))\n\nSeprando dados de treino e teste.\n\nset.seed(45)\ntr &lt;- round(0.8*nrow(longley_scaled),0)\ntreino &lt;- sample(nrow(longley_scaled), tr, replace = F)\nlongley.tr &lt;- longley_scaled[treino,]\nlongley.te &lt;- longley_scaled[-treino,]\n\nRegressão múltipla para o número de pessoas empregadas em função das outras variáveis.\n\nlm_mult &lt;- lm(Employed ~., longley.tr)\nsummary(lm_mult)\n\n\nCall:\nlm(formula = Employed ~ ., data = longley.tr)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.116622 -0.033371  0.002484  0.031364  0.121138 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  -0.006058   0.025665  -0.236  0.82123   \nGNP.deflator  0.154006   0.487336   0.316  0.76269   \nGNP          -1.252420   1.218841  -1.028  0.34379   \nUnemployed   -0.594009   0.165152  -3.597  0.01141 * \nArmed.Forces -0.233556   0.061149  -3.819  0.00877 **\nPopulation   -0.092484   0.677281  -0.137  0.89585   \nYear          2.629894   0.676141   3.890  0.00808 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09174 on 6 degrees of freedom\nMultiple R-squared:  0.9959,    Adjusted R-squared:  0.9917 \nF-statistic: 241.1 on 6 and 6 DF,  p-value: 7.003e-07\n\n\nObtendo os coeficientes com as equações normais de mínimos quadrados passo a passo.\n\nX &lt;- model.matrix(~., data =longley.tr[,-7])\nX\n\n     (Intercept) GNP.deflator        GNP Unemployed Armed.Forces Population\n1951           1   -0.5079204 -0.5908091 -1.1710587   0.70742726 -0.7689652\n1957           1    0.6225934  0.5540580 -0.2753583   0.27490604  0.4342950\n1960           1    1.1600508  1.1560203  0.7894229  -0.13318708  1.1420190\n1953           1   -0.2484582 -0.2244927 -1.4161189   1.35117978 -0.3349577\n1956           1    0.2704662  0.3167321 -0.3973534   0.35968594  0.1883239\n1949           1   -1.2492409 -1.3043364  0.5229601  -1.42356602 -1.0998977\n1959           1    1.0117867  0.9558390  0.6631474  -0.07858307  0.8542141\n1952           1   -0.3318568 -0.4094719 -1.3497707   1.41871632 -0.5971736\n1950           1   -1.1287763 -1.0372705  0.1687464  -1.37470980 -0.9337126\n1954           1   -0.1557931 -0.2473611  0.4116664   1.06810111 -0.1732292\n1958           1    0.8449896  0.5719362  1.5920219   0.04355747  0.6506518\n1947           1   -1.7310992 -1.5434331 -0.8960348  -1.46092666 -1.4111352\n1962           1    1.4102465  1.6821336  0.8707530   0.31657752  1.8195537\n           Year\n1951 -0.7351470\n1957  0.5251050\n1960  1.1552311\n1953 -0.3150630\n1956  0.3150630\n1949 -1.1552311\n1959  0.9451891\n1952 -0.5251050\n1950 -0.9451891\n1954 -0.1050210\n1958  0.7351470\n1947 -1.5753151\n1962  1.5753151\nattr(,\"assign\")\n[1] 0 1 2 3 4 5 6\n\n\n\ny &lt;- longley.tr$Employed\nbeta_mat &lt;- solve(t(X)%*%X)%*%t(X)%*%y\nbeta_mat\n\n                     [,1]\n(Intercept)  -0.006058494\nGNP.deflator  0.154006386\nGNP          -1.252419887\nUnemployed   -0.594008870\nArmed.Forces -0.233556488\nPopulation   -0.092484035\nYear          2.629893534\n\n\nExistem algoritmos de seleção de variáveis que permitem a melhora do ajuste do modelo considerando a remoção de coeficientes não significativos. Um deles é a eliminação para trás. O critério de informação de Akaike, o qual leva em conta o erro dos modelos e a complexidade, é usado para selecionar os modelos.\n\nlm_mult_red &lt;- step(lm_mult, direction = \"back\")\n\nStart:  AIC=-58.16\nEmployed ~ GNP.deflator + GNP + Unemployed + Armed.Forces + Population + \n    Year\n\n               Df Sum of Sq      RSS     AIC\n- Population    1  0.000157 0.050653 -60.120\n- GNP.deflator  1  0.000840 0.051337 -59.946\n&lt;none&gt;                      0.050496 -58.160\n- GNP           1  0.008886 0.059382 -58.053\n- Unemployed    1  0.108874 0.159370 -45.219\n- Armed.Forces  1  0.122775 0.173271 -44.132\n- Year          1  0.127324 0.177820 -43.795\n\nStep:  AIC=-60.12\nEmployed ~ GNP.deflator + GNP + Unemployed + Armed.Forces + Year\n\n               Df Sum of Sq     RSS     AIC\n- GNP.deflator  1   0.00533 0.05598 -60.820\n&lt;none&gt;                      0.05065 -60.120\n- GNP           1   0.04735 0.09801 -53.540\n- Year          1   0.13005 0.18070 -45.586\n- Armed.Forces  1   0.16981 0.22046 -43.001\n- Unemployed    1   0.32078 0.37143 -36.219\n\nStep:  AIC=-60.82\nEmployed ~ GNP + Unemployed + Armed.Forces + Year\n\n               Df Sum of Sq     RSS     AIC\n&lt;none&gt;                      0.05598 -60.820\n- GNP           1  0.044043 0.10002 -55.275\n- Year          1  0.154642 0.21062 -45.594\n- Armed.Forces  1  0.188671 0.24465 -43.647\n- Unemployed    1  0.315797 0.37178 -38.207\n\nsummary(lm_mult_red)\n\n\nCall:\nlm(formula = Employed ~ GNP + Unemployed + Armed.Forces + Year, \n    data = longley.tr)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.124653 -0.039041 -0.002178  0.039448  0.114711 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.006174   0.023368  -0.264  0.79830    \nGNP          -1.336743   0.532824  -2.509  0.03644 *  \nUnemployed   -0.604450   0.089976  -6.718  0.00015 ***\nArmed.Forces -0.216546   0.041703  -5.193  0.00083 ***\nYear          2.779766   0.591313   4.701  0.00154 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08365 on 8 degrees of freedom\nMultiple R-squared:  0.9954,    Adjusted R-squared:  0.9931 \nF-statistic: 434.8 on 4 and 8 DF,  p-value: 2.19e-09\n\n\nAvaliando o modelo considerando os dados de treino.\n\nmetrics &lt;- function(obs, pred) {\n  \n  RSE &lt;- sum((obs - pred)^2)\n  SST &lt;- sum((obs - mean(obs))^2)\n  R2 &lt;- 1 - RSE/SST \n  \n  MAE &lt;-  mean(abs(obs - pred))\n  \n  RMSE &lt;- sqrt(mean((obs - pred)^2))\n  \n  return(\n    data.frame(RMSE = RMSE,\n               MAE = MAE,\n               R2 = R2))\n}\n\n\nmetrics(longley.tr$Employed, lm_mult_red$fitted.values)\n\n        RMSE        MAE        R2\n1 0.06562145 0.05009978 0.9954209\n\n\nAvaliando os resíduos do modelo.\n\nshapiro.test(lm_mult_red$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  lm_mult_red$residuals\nW = 0.96685, p-value = 0.854\n\n\nA Figura 3.11 expõe os gráficos de resíduos do modelo.\n\npar(mfrow=c(2,2))\nplot(lm_mult_red)\n\n\n\n\n\n\n\nFigura 3.11\n\n\n\n\n\nAvaliando o modelo com dados de teste.\n\npred_mult.te &lt;- predict(lm_mult_red, newdata=longley.te)\nmetrics(longley.te$Employed, pred_mult.te)\n\n        RMSE        MAE        R2\n1 0.08438488 0.06111778 0.9922761\n\n\n\n\n3.2.3 Regressão múltipla com variáveis categóricas\nPacote para dados e para codificação de variáveis dummy ou dicotômicas.\n\nlibrary(AER)\nlibrary(fastDummies)\nlibrary(dplyr)\n\nCarregando conjunto de dados com variáveis contínuas e uma categórica.\n\ndata(Grunfeld)\n# ?Grunfeld\nhead(Grunfeld)\n\n  invest  value capital           firm year\n1  317.6 3078.5     2.8 General Motors 1935\n2  391.8 4661.7    52.6 General Motors 1936\n3  410.6 5387.1   156.9 General Motors 1937\n4  257.7 2792.2   209.2 General Motors 1938\n5  330.8 4313.2   203.4 General Motors 1939\n6  461.2 4643.9   207.2 General Motors 1940\n\n\nA Figura 3.12 expõe gráficos de densidade, correlação aos pares e coeficientes de correlação de Pearson para cada nível da variável categórica firm (firma). Foram consideradas apenas três empresas para não dificultar a interpretação. Consegue fazer para as outras empresas?\n\nggpairs(Grunfeld[1:60,], columns = 1:4, aes(color = firm, alpha = 0.5)) + theme_bw()\n\n\n\n\n\n\n\nFigura 3.12\n\n\n\n\n\n\nlevels(Grunfeld$firm)\n\n [1] \"General Motors\"    \"US Steel\"          \"General Electric\" \n [4] \"Chrysler\"          \"Atlantic Refining\" \"IBM\"              \n [7] \"Union Oil\"         \"Westinghouse\"      \"Goodyear\"         \n[10] \"Diamond Match\"     \"American Steel\"   \n\n\nCodificando a variável firm em variáveis binárias.\n\nGrunfeld2 &lt;- dummy_cols(Grunfeld, select_columns = c(\"firm\"))\n\nGrunfeld2 &lt;- Grunfeld2[,-c(4,16)] # removendo coluna da variável \"firm\" e coluna da última variável binária criada, a qual é desnecessária\n\nhead(Grunfeld)\n\n  invest  value capital           firm year\n1  317.6 3078.5     2.8 General Motors 1935\n2  391.8 4661.7    52.6 General Motors 1936\n3  410.6 5387.1   156.9 General Motors 1937\n4  257.7 2792.2   209.2 General Motors 1938\n5  330.8 4313.2   203.4 General Motors 1939\n6  461.2 4643.9   207.2 General Motors 1940\n\n\nPadronizando variáveis regressoras contínuas.\n\nGrunfeld2[,2:4] &lt;- scale(Grunfeld2[,2:4])\n\nDividindo dados de treino e teste.\n\nset.seed(87)\ntr &lt;- round(0.8*nrow(Grunfeld2),0)\ntreino &lt;- sample(nrow(Grunfeld2), tr, replace = F)\nGrunfeld.tr &lt;- Grunfeld2[treino,]\nGrunfeld.te &lt;- Grunfeld2[-treino,]\n\nModelo de regressão linear múltipla com variáveis contínuas e categóricas.\n\nlm_invest &lt;- lm(invest ~ .-year, Grunfeld.tr)\nsummary(lm_invest)\n\n\nCall:\nlm(formula = invest ~ . - year, data = Grunfeld.tr)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-177.977  -12.831   -0.035   13.412  197.819 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               158.512     14.627  10.837  &lt; 2e-16 ***\nvalue                     141.105     14.368   9.821  &lt; 2e-16 ***\ncapital                    76.787      5.533  13.877  &lt; 2e-16 ***\n`firm_General Motors`     -25.829     47.263  -0.546  0.58547    \n`firm_US Steel`           125.052     26.010   4.808 3.44e-06 ***\n`firm_General Electric`  -202.654     25.570  -7.926 3.38e-13 ***\nfirm_Chrysler              -7.712     17.307  -0.446  0.65646    \n`firm_Atlantic Refining`  -76.358     17.943  -4.256 3.51e-05 ***\nfirm_IBM                   -3.460     16.713  -0.207  0.83625    \n`firm_Union Oil`          -29.516     16.107  -1.833  0.06869 .  \nfirm_Westinghouse         -34.198     17.161  -1.993  0.04795 *  \nfirm_Goodyear             -56.218     17.078  -3.292  0.00122 ** \n`firm_Diamond Match`       10.857     15.651   0.694  0.48883    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 46.73 on 163 degrees of freedom\nMultiple R-squared:  0.9443,    Adjusted R-squared:  0.9402 \nF-statistic: 230.5 on 12 and 163 DF,  p-value: &lt; 2.2e-16\n\n\nOutra forma de inclusão de variáveis dummy em modelos de regressão múltipla é na interação. A diferença é que no caso anterior as variáveis dummy mudam apenas a constante, enquanto neste caso, vão mudar a inclinação.\nSupondo o caso da empresa General Electric que no gráfico pairs aparenta ter inclinação distinta das demais.\n\nlm_invest2 &lt;- lm(invest ~ . + I(capital*`firm_General Electric`), Grunfeld.tr)\nsummary(lm_invest2)\n\n\nCall:\nlm(formula = invest ~ . + I(capital * `firm_General Electric`), \n    data = Grunfeld.tr)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-179.22  -12.00    1.06   12.39  184.42 \n\nCoefficients:\n                                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                           163.389     14.679  11.131  &lt; 2e-16 ***\nvalue                                 137.675     14.076   9.781  &lt; 2e-16 ***\ncapital                                87.834      7.203  12.194  &lt; 2e-16 ***\nyear                                   -4.245      4.641  -0.915 0.361718    \n`firm_General Motors`                 -34.125     47.124  -0.724 0.470018    \n`firm_US Steel`                       121.936     25.838   4.719 5.11e-06 ***\n`firm_General Electric`              -193.220     26.307  -7.345 9.71e-12 ***\nfirm_Chrysler                          -8.519     16.928  -0.503 0.615497    \n`firm_Atlantic Refining`              -92.113     18.817  -4.895 2.37e-06 ***\nfirm_IBM                               -3.521     16.284  -0.216 0.829092    \n`firm_Union Oil`                      -38.768     16.397  -2.364 0.019253 *  \nfirm_Westinghouse                     -33.498     16.749  -2.000 0.047185 *  \nfirm_Goodyear                         -64.341     17.112  -3.760 0.000237 ***\n`firm_Diamond Match`                   12.949     15.266   0.848 0.397568    \nI(capital * `firm_General Electric`)  -44.741     14.660  -3.052 0.002662 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 45.53 on 161 degrees of freedom\nMultiple R-squared:  0.9478,    Adjusted R-squared:  0.9433 \nF-statistic: 208.9 on 14 and 161 DF,  p-value: &lt; 2.2e-16\n\n\nA Figura 3.13 a seguir ilustra a mudança da inclinação para a empresa GE em relação a outras três selecionadas arbitrariamente. Em um caso com tantos níveis de variáveis dummy como este é difícil explorar todas possibilidades. Uma sugestão seria fazer um modelo com todas interações possíveis e posteriormente usar a função step para realizar eliminação para trás (backward elimination) para remover os termos não significativos.\n\nGrunfeld_select &lt;- Grunfeld |&gt; \n  filter(firm %in% c(\"General Motors\",\n                     \"US Steel\",\n                     \"General Electric\",\n                     \"IBM\"))\n\ng &lt;- ggplot(Grunfeld_select, aes(x=capital,\n                  y=invest)) + \n  geom_jitter(aes(color = firm, size=value), alpha =0.5) + \n  geom_smooth(aes(col=firm), method=\"lm\", se=F) +\n  theme_bw()\n\ng\n\n\n\n\n\n\n\nFigura 3.13\n\n\n\n\n\nAvaliando o modelo considerando os dados de treino.\n\nmetrics(Grunfeld.tr$invest, lm_invest$fitted.values)\n\n      RMSE      MAE        R2\n1 44.97067 25.96427 0.9443401\n\n\nAvaliando o modelo considerando os dados de teste.\n\npred.invest &lt;- predict(lm_invest, newdata = Grunfeld.te)\n\nmetrics(Grunfeld.te$invest, pred.invest)\n\n      RMSE      MAE        R2\n1 66.90304 36.76732 0.9391516\n\n\nAdicionando termo de interaçao ao modelo.\n\nlm_invest_int &lt;- lm(invest ~ . + value*capital, Grunfeld.tr)\nsummary(lm_invest_int)\n\n\nCall:\nlm(formula = invest ~ . + value * capital, data = Grunfeld.tr)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-146.016  -11.418   -2.304   12.401  192.342 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               66.0183    16.3423   4.040 8.27e-05 ***\nvalue                    102.4319    12.5440   8.166 8.77e-14 ***\ncapital                   -0.3027    10.8409  -0.028 0.977757    \nyear                      21.3233     4.8910   4.360 2.31e-05 ***\n`firm_General Motors`    157.6237    45.0929   3.496 0.000611 ***\n`firm_US Steel`          249.4813    26.0617   9.573  &lt; 2e-16 ***\n`firm_General Electric`  -58.0340    27.4135  -2.117 0.035797 *  \nfirm_Chrysler             39.1575    15.1944   2.577 0.010861 *  \n`firm_Atlantic Refining`  69.6205    23.1453   3.008 0.003053 ** \nfirm_IBM                  25.2365    14.0326   1.798 0.073983 .  \n`firm_Union Oil`          53.0164    16.8449   3.147 0.001964 ** \nfirm_Westinghouse         -2.1632    14.5174  -0.149 0.881735    \nfirm_Goodyear             29.0504    17.4580   1.664 0.098054 .  \n`firm_Diamond Match`      -7.8207    12.9832  -0.602 0.547777    \nvalue:capital             29.3232     3.2597   8.996 6.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 38.2 on 161 degrees of freedom\nMultiple R-squared:  0.9633,    Adjusted R-squared:  0.9601 \nF-statistic: 301.5 on 14 and 161 DF,  p-value: &lt; 2.2e-16\n\n\nConsegue fazer os próximos passos para avaliar o modelo com interação nos dados de teste?\n\n\n3.2.4 Regressão polinomial\nCarregando pacote e dados.\n\nlibrary(MASS)\n\n\nrehab &lt;- wtloss\n# ?wtloss\n\nVisualizando o comportamento do peso (massa) em função de dias de treinamento.\n\nggplot(rehab, aes(x = Days, y = Weight)) + \n  geom_point(color = \"deepskyblue3\", size = 2) +\n  xlab(\"Dias\") + \n  ylab(\"Peso\") + theme_bw()\n\nSeparando dados de treino e teste.\n\nset.seed(53)\ntr &lt;- round(0.75*nrow(rehab),0)\ntreino &lt;- sample(nrow(rehab), tr, replace = F)\nrehab.tr &lt;- rehab[treino,]\nrehab.te &lt;- rehab[-treino,]\n\nEstimando modelos linear e quadrático.\n\nlm1 &lt;- lm(Weight ~ Days, rehab.tr)\nsummary(lm1)\n\n\nlm2 &lt;- lm(Weight ~ Days + I(Days^2), rehab.tr)\nsummary(lm2)\n\nDesempenho para dados de treino.\n\nmetrics(rehab.tr$Weight, lm1$fitted.values)\n\n\nmetrics(rehab.tr$Weight, lm2$fitted.values)\n\nDesempenho dos modelos para dados de teste.\n\npred.lm1 &lt;- predict(lm1, newdata = rehab.te)\nmetrics(rehab.te$Weight, pred.lm1)\n\n\npred.lm2 &lt;- predict(lm2, newdata = rehab.te)\nmetrics(rehab.te$Weight, pred.lm2)\n\nPlotando o modelo com dados de teste.\n\nggplot() + \n  geom_point(data = rehab.tr, mapping = aes(x = Days, y = Weight), color = \"deepskyblue3\", size = 2) +\n  geom_smooth(data = rehab.te, mapping = aes(x = Days, y = Weight), \n              method = \"lm\", formula = y ~ x + I(x^2), se = F, col = \"mediumvioletred\") +\n  ggtitle(\"Peso vs Dias (dados de teste)\") + \n  xlab(\"Dias\") + \n  ylab(\"Peso\") + theme_bw()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regressão linear simples, múltipla e polinomial</span>"
    ]
  },
  {
    "objectID": "AS_reg_ols.html#referências",
    "href": "AS_reg_ols.html#referências",
    "title": "3  Regressão linear simples, múltipla e polinomial",
    "section": "Referências",
    "text": "Referências\nHastie, T., Tibshirani, R., Friedman, J. H., & Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction (Vol. 2, pp. 1-758). New York: springer.\nGareth, J., Daniela, W., Trevor, H., & Robert, T. (2013). An introduction to statistical learning: with applications in R. Spinger.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regressão linear simples, múltipla e polinomial</span>"
    ]
  },
  {
    "objectID": "AS_vicio_var.html#referências",
    "href": "AS_vicio_var.html#referências",
    "title": "2  Conflito entre vício e variância no Aprendizado supervisionado",
    "section": "Referências",
    "text": "Referências\nBishop, Christopher M., and Hugh Bishop. “Deep learning: foundations and concepts.” (2024).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conflito entre vício e variância no Aprendizado supervisionado</span>"
    ]
  },
  {
    "objectID": "AS_Intro.html#implementações-em-r",
    "href": "AS_Intro.html#implementações-em-r",
    "title": "1  Introdução ao aprendizado supervisionado",
    "section": "1.5 Implementações em R",
    "text": "1.5 Implementações em R\n\n1.5.1 Exemplo de problema de regressão\nA seguir serão expostas as implementações necessárias para obter os resultados do capítulo.\nCarregando as bibliotecas (pacotes) para análise.\n\nlibrary(AER) # para base de dados\nlibrary(ggplot2) # para gráficos\n\nCarregando base de dados.\n\ndata(USConsump1993)\n# ?USConsump1993\n\n\nConsumo &lt;- data.frame(USConsump1993)\nhead(Consumo)\n\nSeparando 75% dos dados para treino do modelo e 25% para teste.\n\ntr &lt;- round(0.75*nrow(Consumo))\n\nset.seed(9)\ntreino &lt;- sample(nrow(Consumo), tr, replace = F)\n\nConsumo.tr &lt;- Consumo[treino,]\nConsumo.te &lt;- Consumo[-treino,]\n\n\nhead(Consumo.tr)\n\n\nhead(Consumo.te)\n\nTreinando um modelo de regressão linear simples.\n\nlm1 &lt;- lm(expenditure ~ income, data = Consumo.tr)\nsummary(lm1)\n\nPlotando o modelo com dados de treino.\n\nggplot(data = Consumo.tr, aes(x = income, y = expenditure)) + \n   geom_point(color = 'red', size = 2) +\n   geom_smooth(method = \"lm\", formula = y ~ x) +\n   xlab(\"renda\") + \n   ylab(\"consumo\") + theme_bw()\n\nRealizando previsão com o modelo.\n\npredict(lm1, newdata = data.frame(income = 9000))\n\nPrevisão para todos dados de teste.\n\nConsumo.te$exp_pred &lt;- predict(lm1, \n                               newdata = \n                                 data.frame(income = \n                                              Consumo.te$income))\n\n# head(Consumo.te)\n\nPlotando o modelo com os dados de teste.\n\nggplot() + \n  geom_point(data = Consumo.te, aes(x = income, y = expenditure), size = 2) +\n  geom_smooth(method = \"lm\", formula = y ~ x, \n              data = Consumo.tr,\n              aes(x = income, y = expenditure)) +\n  xlab(\"renda\") + \n  ylab(\"consumo\") + theme_bw()\n\nCriando uma função de métricas de desempenho.\n\nmetrics &lt;- function(obs, pred) {\n  \n  RSE &lt;- sum((obs - pred)^2)\n  SST &lt;- sum((obs - mean(obs))^2)\n  R2 &lt;- 1 - RSE/SST \n  \n  MAE &lt;-  mean(abs(obs - pred))\n  \n  RMSE &lt;- sqrt(mean((obs - pred)^2))\n  \n  return(\n    data.frame(RMSE = RMSE,\n               MAE = MAE,\n               R2 = R2))\n}\n\nDesempenho do modelo para dados de teste.\n\nmetrics(Consumo.te$expenditure, Consumo.te$exp_pred)\n\n\n\n1.5.2 Exemplo de problema de classificação\n\nlibrary(mlbench)\n\n\ndata(PimaIndiansDiabetes2)\n# ?PimaIndiansDiabetes2\n\n\nPimaIndiansDiabetes2 &lt;- na.omit(PimaIndiansDiabetes2)\nPimaIndiansDiabetes2$diabetes &lt;- ifelse(PimaIndiansDiabetes2$diabetes==\"neg\",0,1)\n\ndados &lt;- PimaIndiansDiabetes2\n\nhead(dados)\n\n\nset.seed(7)\ntreino &lt;- sample(nrow(dados), 0.75*nrow(dados))\ndados_treino &lt;- dados[treino,]\ndados_test &lt;- dados[-treino,]\n\nObtendo um modelo de regressão logística simples a partir dos dados de treino considerando apenas uma variável regressora, o nível de glicose.\n\nmodel1 &lt;- glm( diabetes ~ glucose, data = dados_treino, family = binomial)\nsummary(model1)\n\nPlotando o modelo com os dados de treino.\n\nggplot(dados_treino, aes(glucose, diabetes)) +\n  geom_point(aes(col = as.factor(diabetes)), alpha = 0.5) +\n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\"), fill = \"grey\", col = \"black\") +\n  labs(x = \"Concentracao de glicose\", y = \"Probabilidade de ter diabetes\", col = \"diabetes\") +\n  theme_bw()\n\nPlotando o modelo com os dados de teste.\n\nggplot() +\n  geom_point(data = dados_test, \n             mapping = aes(glucose, diabetes, col = as.factor(diabetes)), \n             alpha = 0.5) +\n  geom_smooth(data = dados_treino, \n              mapping = aes(glucose, diabetes), \n              method = \"glm\", \n              method.args = list(family = \"binomial\"), \n              col = \"black\") +\n  labs(x = \"Concentracao de glicose\", y = \"Probabilidade de ter diabetes\", col = \"diabetes\") + theme_bw()\n\nPrevisão com os dados de teste.\n\ndados_test$prob &lt;- predict(model1, \n                           newdata = data.frame(glucose = dados_test$glucose), \n                           type = 'response')\ndados_test$y_pred &lt;- ifelse(dados_test$prob &gt; 0.5, 1, 0)\n\nhead(dados_test)\n\nMatriz de confusão para dados de teste.\n\ncm1 &lt;- table(data = dados_test$diabetes, model = dados_test$y_pred)\ncm1\n\nProporção de observações de teste classificadas corretamente.\n\nmean(dados_test$diabetes == dados_test$y_pred)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao aprendizado supervisionado</span>"
    ]
  },
  {
    "objectID": "AS_vicio_var.html#implementações-em-r",
    "href": "AS_vicio_var.html#implementações-em-r",
    "title": "2  Conflito entre vício e variância no Aprendizado supervisionado",
    "section": "2.2 Implementações em R",
    "text": "2.2 Implementações em R\nA seguir serão expostas as implementações necessárias para obter os resultados do capítulo.\nCarregando as bibliotecas (pacotes) para análise.\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nCriando uma função para simular o conflito. Na prática tal função é desconhecida; portanto deseja-se aproximá-la apartir dos dados.\n\nf_sin &lt;- function(x) {\n  f &lt;- sin(2*pi*x)\n  return(f)\n}\n\nSimulando dados. x foi simulado segundo uma distribuição uniforme com limites entre 0 e 1. y foi simulado avaliando os valores de x na função criada previamente adicionando um termo de erro segundo a distribuição normal com média nula e desvio-padrão igual a 0.2.\n\nset.seed(7)\nx &lt;- runif(100,0,1)\ndata &lt;- data.frame(x = x,\n                   y = f_sin(x) + rnorm(100, 0, .2))\n\nknitr::kable(head(data))\n\nPlotando.\n\nggplot(data, aes(x,y)) + \n  geom_point(col=\"red\") +\n  theme_bw()\n\nPlotando os dados disponíveis acompanhados de três modelos de regressão polinomial simples, um linear, um cúbico e um de décima ordem.\n\ncolors &lt;- c(\"y\" = \"red3\", \"linear\" = \"grey\", \"cúbico\" = \"green2\", \"poly-10\" = \"purple\")\n\nggplot(data, aes(x,y)) + \n  geom_point(aes(color = \"y\")) +\n  geom_smooth(method = lm, formula = y ~ x, se = F, aes(color = \"linear\")) +\n  geom_smooth(method = lm, formula = y ~ poly(x, degree = 3, raw = TRUE), \n              se = F, aes(color = \"cúbico\")) +\n  geom_smooth(method = lm, formula = y ~ poly(x, degree = 10, raw = TRUE), \n              se = F, aes(color = \"poly-10\")) +\n  labs(color = \"Legend\") +\n    scale_color_manual(values = colors) +\n  theme_bw()\n\nTomando um valor arbitrário de \\(x = 075\\), e conjuntos distintos de observações futuras para medir o vício e a variância de cada modelo.\n\nset.seed(7)\n\npred &lt;- data.frame(model = numeric(3000),\n                   pred = numeric(3000),\n                   vicio = numeric(3000))\n\nx0=0.75\n\ni&lt;-1\n\nE_y &lt;- f_sin(x0)\nE_y\n\n\nfor (k in 1:999) {\n\ndata_ &lt;- data.frame(x = x,\n                    y = f_sin(x) + rnorm(100, 0, 2))\n\nlm1 &lt;- lm(y ~ x, data_)\nlm3 &lt;- lm(y ~ poly(x, degree = 3, raw = TRUE), data_)\nlm10 &lt;- lm(y ~ poly(x, degree = 10, raw = TRUE), data_)\n                    \npred1 &lt;- predict(lm1, newdata = data.frame(x=x0)) \npred3 &lt;- predict(lm3, newdata = data.frame(x=x0)) \npred10 &lt;- predict(lm10, newdata = data.frame(x=x0))  \n\nvicio1 &lt;- pred1 - E_y\nvicio3 &lt;- pred3 - E_y\nvicio10 &lt;- pred10 - E_y\n\npred$model[i] &lt;- 1\npred$model[i+1] &lt;- 3\npred$model[i+2] &lt;- 10\n\npred$pred[i] &lt;- pred1\npred$pred[i+1] &lt;- pred3\npred$pred[i+2] &lt;- pred10\n\npred$vicio[i]   &lt;- vicio1\npred$vicio[i+1] &lt;- vicio3\npred$vicio[i+2] &lt;- vicio10\n\ni &lt;- i + 3\n\n} \n\n\nggplot(pred, aes(x=as.factor(model),y=vicio, col = as.factor(model))) +\n  geom_boxplot() + \n  geom_jitter(alpha = .1) +\n  geom_hline(yintercept = 0, linetype=2) +\n  labs(x = \"ordem\", y = \"erro\") +\n  theme_bw() +\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\n\nggplot(pred, aes(x=vicio, fill = as.factor(model))) +\n  geom_density(alpha = .3, col = \"white\") + \n  geom_vline(xintercept = 0, linetype=2) +\n  labs(y=\"densidade\", x = \"erro\", fill = \"ordem\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nSimulando o conflito para um caso com poucas observações disponíveis.\n\nset.seed(7)\nxa &lt;- runif(15,0,1)\ndataa &lt;- data.frame(x = xa,\n                    y = f_sin(xa) + rnorm(15, 0, .2))\n\ncolors &lt;- c(\"y\" = \"red3\", \"linear\" = \"grey\", \"cúbico\" = \"green2\", \"poly-10\" = \"purple\")\n\nggplot(dataa, aes(x,y)) + \n  geom_point(aes(color = \"y\")) +\n  geom_smooth(method = lm, formula = y ~ x, se = F, aes(color = \"linear\")) +\n  geom_smooth(method = lm, formula = y ~ poly(x, degree = 3, raw = TRUE), \n              se = F, aes(color = \"cúbico\")) +\n  geom_smooth(method = lm, formula = y ~ poly(x, degree = 10, raw = TRUE), \n              se = F, aes(color = \"poly-10\")) +\n  labs(color = \"Legend\") +\n    scale_color_manual(values = colors) +\n  theme_bw()\n\nErro ou valor da função perda de tais modelos para dados de treino e dados de teste.\n\nset.seed(7)\nxa &lt;- runif(20,0,1)\ndataa &lt;- data.frame(x = xa,\n                    y = f_sin(xa) + rnorm(20, 0, .2))\n\nlm1a &lt;- lm(y ~ x, dataa)\nlm2a &lt;- lm(y ~ poly(x, degree = 2, raw = TRUE), dataa)\nlm3a &lt;- lm(y ~ poly(x, degree = 3, raw = TRUE), dataa)\nlm5a &lt;- lm(y ~ poly(x, degree = 5, raw = TRUE), dataa)\nlm10a &lt;- lm(y ~ poly(x, degree = 10, raw = TRUE), dataa)\n\nset.seed(27)\nxb &lt;- runif(20,0,1)\ndatab &lt;- data.frame(x = xb,\n                    y = f_sin(xb) + rnorm(20, 0, .2))\n\nMSE &lt;- function(obs, pred) {\n  MSE &lt;- mean((obs - pred)^2)\n  return(MSE)\n}\n\nMSE1_T &lt;- MSE(dataa$y, lm1a$fitted.values)\nMSE2_T &lt;- MSE(dataa$y, lm2a$fitted.values)\nMSE3_T &lt;- MSE(dataa$y, lm3a$fitted.values)\nMSE5_T &lt;- MSE(dataa$y, lm5a$fitted.values)\nMSE10_T &lt;- MSE(dataa$y, lm10a$fitted.values)\n\nyhat1 &lt;- predict(lm1a, newdata = datab)\nyhat2 &lt;- predict(lm2a, newdata = datab)\nyhat3 &lt;- predict(lm3a, newdata = datab)\nyhat5 &lt;- predict(lm5a, newdata = datab)\nyhat10 &lt;- predict(lm10a, newdata = datab)\n\nMSE1_t &lt;- MSE(datab$y, yhat1)\nMSE2_t &lt;- MSE(datab$y, yhat2)\nMSE3_t &lt;- MSE(datab$y, yhat3)\nMSE5_t &lt;- MSE(datab$y, yhat5)\nMSE10_t &lt;- MSE(datab$y, yhat10)\n\ndata_MSE &lt;- data.frame(model = rep(c(1,2,3,5,10),2),\n                       dados = c(rep(\"treino\",5),rep(\"teste\",5)),\n                       MSE = c(MSE1_T, MSE2_T, MSE3_T, MSE5_T, MSE10_T, \n                               MSE1_t, MSE2_t, MSE3_t, MSE5_t, MSE10_t))\n\nggplot(data_MSE, aes(x=model, y = MSE, \n                     col = dados,\n                     linetype = dados,\n                     pch = dados)) + \n  geom_point(size=2) +\n  geom_line() +\n  labs(x = \"ordem\") +\n  scale_x_continuous(breaks = 1:10) +\n  theme_bw()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conflito entre vício e variância no Aprendizado supervisionado</span>"
    ]
  },
  {
    "objectID": "AS_reg_ols.html#implementações-em-r",
    "href": "AS_reg_ols.html#implementações-em-r",
    "title": "3  Regressão linear simples, múltipla e polinomial",
    "section": "3.2 Implementações em R",
    "text": "3.2 Implementações em R\nA seguir será expostos as implementações necessárias para obter os resultados do capítulo. Alguns exemplos aqui expostos são distintos dos do capítulo, sendo os resultados das implementações apresentados integralmente.\n\n3.2.1 Caso 1 - regressão linear simples\nUm exemplo de regressão linear simples já foi proposto no primeiro capítulo do livro.\n\n\n3.2.2 Regressão linear múltipla\nCarregando pacote para dados e gráfico.\n\nlibrary(caTools)\nlibrary(GGally)\n\nA ?fig-dataecn expõe algumas das observações dados macroeconômicos para regressão linear múltipla.\n\ndata(longley)\n# ?longley\n# dim(longley)\nhead(longley) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGNP.deflator\nGNP\nUnemployed\nArmed.Forces\nPopulation\nYear\nEmployed\n\n\n\n\n1947\n83.0\n234.289\n235.6\n159.0\n107.608\n1947\n60.323\n\n\n1948\n88.5\n259.426\n232.5\n145.6\n108.632\n1948\n61.122\n\n\n1949\n88.2\n258.054\n368.2\n161.6\n109.773\n1949\n60.171\n\n\n1950\n89.5\n284.599\n335.1\n165.0\n110.929\n1950\n61.187\n\n\n1951\n96.2\n328.975\n209.9\n309.9\n112.075\n1951\n63.221\n\n\n1952\n98.1\n346.999\n193.2\n359.4\n113.270\n1952\n63.639\n\n\n\n\n\nNa Figura 3.10 visualiza-se a correlação entre as variáveis presentes no conjunto de dados.\n\nggpairs(longley) + theme_bw()\n\n\n\n\n\n\n\nFigura 3.10\n\n\n\n\n\nPadronizando os dados para evitar efeitos de escala e unidades de medida.\n\nlongley_scaled &lt;- data.frame(scale(longley))\n\nSeprando dados de treino e teste.\n\nset.seed(45)\ntr &lt;- round(0.8*nrow(longley_scaled),0)\ntreino &lt;- sample(nrow(longley_scaled), tr, replace = F)\nlongley.tr &lt;- longley_scaled[treino,]\nlongley.te &lt;- longley_scaled[-treino,]\n\nRegressão múltipla para o número de pessoas empregadas em função das outras variáveis.\n\nlm_mult &lt;- lm(Employed ~., longley.tr)\nsummary(lm_mult)\n\n\nCall:\nlm(formula = Employed ~ ., data = longley.tr)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.116622 -0.033371  0.002484  0.031364  0.121138 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  -0.006058   0.025665  -0.236  0.82123   \nGNP.deflator  0.154006   0.487336   0.316  0.76269   \nGNP          -1.252420   1.218841  -1.028  0.34379   \nUnemployed   -0.594009   0.165152  -3.597  0.01141 * \nArmed.Forces -0.233556   0.061149  -3.819  0.00877 **\nPopulation   -0.092484   0.677281  -0.137  0.89585   \nYear          2.629894   0.676141   3.890  0.00808 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09174 on 6 degrees of freedom\nMultiple R-squared:  0.9959,    Adjusted R-squared:  0.9917 \nF-statistic: 241.1 on 6 and 6 DF,  p-value: 7.003e-07\n\n\nObtendo os coeficientes com as equações normais de mínimos quadrados passo a passo.\n\nX &lt;- model.matrix(~., data =longley.tr[,-7])\nX\n\n     (Intercept) GNP.deflator        GNP Unemployed Armed.Forces Population\n1951           1   -0.5079204 -0.5908091 -1.1710587   0.70742726 -0.7689652\n1957           1    0.6225934  0.5540580 -0.2753583   0.27490604  0.4342950\n1960           1    1.1600508  1.1560203  0.7894229  -0.13318708  1.1420190\n1953           1   -0.2484582 -0.2244927 -1.4161189   1.35117978 -0.3349577\n1956           1    0.2704662  0.3167321 -0.3973534   0.35968594  0.1883239\n1949           1   -1.2492409 -1.3043364  0.5229601  -1.42356602 -1.0998977\n1959           1    1.0117867  0.9558390  0.6631474  -0.07858307  0.8542141\n1952           1   -0.3318568 -0.4094719 -1.3497707   1.41871632 -0.5971736\n1950           1   -1.1287763 -1.0372705  0.1687464  -1.37470980 -0.9337126\n1954           1   -0.1557931 -0.2473611  0.4116664   1.06810111 -0.1732292\n1958           1    0.8449896  0.5719362  1.5920219   0.04355747  0.6506518\n1947           1   -1.7310992 -1.5434331 -0.8960348  -1.46092666 -1.4111352\n1962           1    1.4102465  1.6821336  0.8707530   0.31657752  1.8195537\n           Year\n1951 -0.7351470\n1957  0.5251050\n1960  1.1552311\n1953 -0.3150630\n1956  0.3150630\n1949 -1.1552311\n1959  0.9451891\n1952 -0.5251050\n1950 -0.9451891\n1954 -0.1050210\n1958  0.7351470\n1947 -1.5753151\n1962  1.5753151\nattr(,\"assign\")\n[1] 0 1 2 3 4 5 6\n\n\n\ny &lt;- longley.tr$Employed\nbeta_mat &lt;- solve(t(X)%*%X)%*%t(X)%*%y\nbeta_mat\n\n                     [,1]\n(Intercept)  -0.006058494\nGNP.deflator  0.154006386\nGNP          -1.252419887\nUnemployed   -0.594008870\nArmed.Forces -0.233556488\nPopulation   -0.092484035\nYear          2.629893534\n\n\nExistem algoritmos de seleção de variáveis que permitem a melhora do ajuste do modelo considerando a remoção de coeficientes não significativos. Um deles é a eliminação para trás. O critério de informação de Akaike, o qual leva em conta o erro dos modelos e a complexidade, é usado para selecionar os modelos.\n\nlm_mult_red &lt;- step(lm_mult, direction = \"back\")\n\nStart:  AIC=-58.16\nEmployed ~ GNP.deflator + GNP + Unemployed + Armed.Forces + Population + \n    Year\n\n               Df Sum of Sq      RSS     AIC\n- Population    1  0.000157 0.050653 -60.120\n- GNP.deflator  1  0.000840 0.051337 -59.946\n&lt;none&gt;                      0.050496 -58.160\n- GNP           1  0.008886 0.059382 -58.053\n- Unemployed    1  0.108874 0.159370 -45.219\n- Armed.Forces  1  0.122775 0.173271 -44.132\n- Year          1  0.127324 0.177820 -43.795\n\nStep:  AIC=-60.12\nEmployed ~ GNP.deflator + GNP + Unemployed + Armed.Forces + Year\n\n               Df Sum of Sq     RSS     AIC\n- GNP.deflator  1   0.00533 0.05598 -60.820\n&lt;none&gt;                      0.05065 -60.120\n- GNP           1   0.04735 0.09801 -53.540\n- Year          1   0.13005 0.18070 -45.586\n- Armed.Forces  1   0.16981 0.22046 -43.001\n- Unemployed    1   0.32078 0.37143 -36.219\n\nStep:  AIC=-60.82\nEmployed ~ GNP + Unemployed + Armed.Forces + Year\n\n               Df Sum of Sq     RSS     AIC\n&lt;none&gt;                      0.05598 -60.820\n- GNP           1  0.044043 0.10002 -55.275\n- Year          1  0.154642 0.21062 -45.594\n- Armed.Forces  1  0.188671 0.24465 -43.647\n- Unemployed    1  0.315797 0.37178 -38.207\n\nsummary(lm_mult_red)\n\n\nCall:\nlm(formula = Employed ~ GNP + Unemployed + Armed.Forces + Year, \n    data = longley.tr)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.124653 -0.039041 -0.002178  0.039448  0.114711 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.006174   0.023368  -0.264  0.79830    \nGNP          -1.336743   0.532824  -2.509  0.03644 *  \nUnemployed   -0.604450   0.089976  -6.718  0.00015 ***\nArmed.Forces -0.216546   0.041703  -5.193  0.00083 ***\nYear          2.779766   0.591313   4.701  0.00154 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08365 on 8 degrees of freedom\nMultiple R-squared:  0.9954,    Adjusted R-squared:  0.9931 \nF-statistic: 434.8 on 4 and 8 DF,  p-value: 2.19e-09\n\n\nAvaliando o modelo considerando os dados de treino.\n\nmetrics &lt;- function(obs, pred) {\n  \n  RSE &lt;- sum((obs - pred)^2)\n  SST &lt;- sum((obs - mean(obs))^2)\n  R2 &lt;- 1 - RSE/SST \n  \n  MAE &lt;-  mean(abs(obs - pred))\n  \n  RMSE &lt;- sqrt(mean((obs - pred)^2))\n  \n  return(\n    data.frame(RMSE = RMSE,\n               MAE = MAE,\n               R2 = R2))\n}\n\n\nmetrics(longley.tr$Employed, lm_mult_red$fitted.values)\n\n        RMSE        MAE        R2\n1 0.06562145 0.05009978 0.9954209\n\n\nAvaliando os resíduos do modelo.\n\nshapiro.test(lm_mult_red$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  lm_mult_red$residuals\nW = 0.96685, p-value = 0.854\n\n\nA Figura 3.11 expõe os gráficos de resíduos do modelo.\n\npar(mfrow=c(2,2))\nplot(lm_mult_red)\n\n\n\n\n\n\n\nFigura 3.11\n\n\n\n\n\nAvaliando o modelo com dados de teste.\n\npred_mult.te &lt;- predict(lm_mult_red, newdata=longley.te)\nmetrics(longley.te$Employed, pred_mult.te)\n\n        RMSE        MAE        R2\n1 0.08438488 0.06111778 0.9922761\n\n\n\n\n3.2.3 Regressão múltipla com variáveis categóricas\nPacote para dados e para codificação de variáveis dummy ou dicotômicas.\n\nlibrary(AER)\nlibrary(fastDummies)\nlibrary(dplyr)\n\nCarregando conjunto de dados com variáveis contínuas e uma categórica.\n\ndata(Grunfeld)\n# ?Grunfeld\nhead(Grunfeld)\n\n  invest  value capital           firm year\n1  317.6 3078.5     2.8 General Motors 1935\n2  391.8 4661.7    52.6 General Motors 1936\n3  410.6 5387.1   156.9 General Motors 1937\n4  257.7 2792.2   209.2 General Motors 1938\n5  330.8 4313.2   203.4 General Motors 1939\n6  461.2 4643.9   207.2 General Motors 1940\n\n\nA Figura 3.12 expõe gráficos de densidade, correlação aos pares e coeficientes de correlação de Pearson para cada nível da variável categórica firm (firma). Foram consideradas apenas três empresas para não dificultar a interpretação. Consegue fazer para as outras empresas?\n\nggpairs(Grunfeld[1:60,], columns = 1:4, aes(color = firm, alpha = 0.5)) + theme_bw()\n\n\n\n\n\n\n\nFigura 3.12\n\n\n\n\n\n\nlevels(Grunfeld$firm)\n\n [1] \"General Motors\"    \"US Steel\"          \"General Electric\" \n [4] \"Chrysler\"          \"Atlantic Refining\" \"IBM\"              \n [7] \"Union Oil\"         \"Westinghouse\"      \"Goodyear\"         \n[10] \"Diamond Match\"     \"American Steel\"   \n\n\nCodificando a variável firm em variáveis binárias.\n\nGrunfeld2 &lt;- dummy_cols(Grunfeld, select_columns = c(\"firm\"))\n\nGrunfeld2 &lt;- Grunfeld2[,-c(4,16)] # removendo coluna da variável \"firm\" e coluna da última variável binária criada, a qual é desnecessária\n\nhead(Grunfeld)\n\n  invest  value capital           firm year\n1  317.6 3078.5     2.8 General Motors 1935\n2  391.8 4661.7    52.6 General Motors 1936\n3  410.6 5387.1   156.9 General Motors 1937\n4  257.7 2792.2   209.2 General Motors 1938\n5  330.8 4313.2   203.4 General Motors 1939\n6  461.2 4643.9   207.2 General Motors 1940\n\n\nPadronizando variáveis regressoras contínuas.\n\nGrunfeld2[,2:4] &lt;- scale(Grunfeld2[,2:4])\n\nDividindo dados de treino e teste.\n\nset.seed(87)\ntr &lt;- round(0.8*nrow(Grunfeld2),0)\ntreino &lt;- sample(nrow(Grunfeld2), tr, replace = F)\nGrunfeld.tr &lt;- Grunfeld2[treino,]\nGrunfeld.te &lt;- Grunfeld2[-treino,]\n\nModelo de regressão linear múltipla com variáveis contínuas e categóricas.\n\nlm_invest &lt;- lm(invest ~ .-year, Grunfeld.tr)\nsummary(lm_invest)\n\n\nCall:\nlm(formula = invest ~ . - year, data = Grunfeld.tr)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-177.977  -12.831   -0.035   13.412  197.819 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               158.512     14.627  10.837  &lt; 2e-16 ***\nvalue                     141.105     14.368   9.821  &lt; 2e-16 ***\ncapital                    76.787      5.533  13.877  &lt; 2e-16 ***\n`firm_General Motors`     -25.829     47.263  -0.546  0.58547    \n`firm_US Steel`           125.052     26.010   4.808 3.44e-06 ***\n`firm_General Electric`  -202.654     25.570  -7.926 3.38e-13 ***\nfirm_Chrysler              -7.712     17.307  -0.446  0.65646    \n`firm_Atlantic Refining`  -76.358     17.943  -4.256 3.51e-05 ***\nfirm_IBM                   -3.460     16.713  -0.207  0.83625    \n`firm_Union Oil`          -29.516     16.107  -1.833  0.06869 .  \nfirm_Westinghouse         -34.198     17.161  -1.993  0.04795 *  \nfirm_Goodyear             -56.218     17.078  -3.292  0.00122 ** \n`firm_Diamond Match`       10.857     15.651   0.694  0.48883    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 46.73 on 163 degrees of freedom\nMultiple R-squared:  0.9443,    Adjusted R-squared:  0.9402 \nF-statistic: 230.5 on 12 and 163 DF,  p-value: &lt; 2.2e-16\n\n\nOutra forma de inclusão de variáveis dummy em modelos de regressão múltipla é na interação. A diferença é que no caso anterior as variáveis dummy mudam apenas a constante, enquanto neste caso, vão mudar a inclinação.\nSupondo o caso da empresa General Electric que no gráfico pairs aparenta ter inclinação distinta das demais.\n\nlm_invest2 &lt;- lm(invest ~ . + I(capital*`firm_General Electric`), Grunfeld.tr)\nsummary(lm_invest2)\n\n\nCall:\nlm(formula = invest ~ . + I(capital * `firm_General Electric`), \n    data = Grunfeld.tr)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-179.22  -12.00    1.06   12.39  184.42 \n\nCoefficients:\n                                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                           163.389     14.679  11.131  &lt; 2e-16 ***\nvalue                                 137.675     14.076   9.781  &lt; 2e-16 ***\ncapital                                87.834      7.203  12.194  &lt; 2e-16 ***\nyear                                   -4.245      4.641  -0.915 0.361718    \n`firm_General Motors`                 -34.125     47.124  -0.724 0.470018    \n`firm_US Steel`                       121.936     25.838   4.719 5.11e-06 ***\n`firm_General Electric`              -193.220     26.307  -7.345 9.71e-12 ***\nfirm_Chrysler                          -8.519     16.928  -0.503 0.615497    \n`firm_Atlantic Refining`              -92.113     18.817  -4.895 2.37e-06 ***\nfirm_IBM                               -3.521     16.284  -0.216 0.829092    \n`firm_Union Oil`                      -38.768     16.397  -2.364 0.019253 *  \nfirm_Westinghouse                     -33.498     16.749  -2.000 0.047185 *  \nfirm_Goodyear                         -64.341     17.112  -3.760 0.000237 ***\n`firm_Diamond Match`                   12.949     15.266   0.848 0.397568    \nI(capital * `firm_General Electric`)  -44.741     14.660  -3.052 0.002662 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 45.53 on 161 degrees of freedom\nMultiple R-squared:  0.9478,    Adjusted R-squared:  0.9433 \nF-statistic: 208.9 on 14 and 161 DF,  p-value: &lt; 2.2e-16\n\n\nA Figura 3.13 a seguir ilustra a mudança da inclinação para a empresa GE em relação a outras três selecionadas arbitrariamente. Em um caso com tantos níveis de variáveis dummy como este é difícil explorar todas possibilidades. Uma sugestão seria fazer um modelo com todas interações possíveis e posteriormente usar a função step para realizar eliminação para trás (backward elimination) para remover os termos não significativos.\n\nGrunfeld_select &lt;- Grunfeld |&gt; \n  filter(firm %in% c(\"General Motors\",\n                     \"US Steel\",\n                     \"General Electric\",\n                     \"IBM\"))\n\ng &lt;- ggplot(Grunfeld_select, aes(x=capital,\n                  y=invest)) + \n  geom_jitter(aes(color = firm, size=value), alpha =0.5) + \n  geom_smooth(aes(col=firm), method=\"lm\", se=F) +\n  theme_bw()\n\ng\n\n\n\n\n\n\n\nFigura 3.13\n\n\n\n\n\nAvaliando o modelo considerando os dados de treino.\n\nmetrics(Grunfeld.tr$invest, lm_invest$fitted.values)\n\n      RMSE      MAE        R2\n1 44.97067 25.96427 0.9443401\n\n\nAvaliando o modelo considerando os dados de teste.\n\npred.invest &lt;- predict(lm_invest, newdata = Grunfeld.te)\n\nmetrics(Grunfeld.te$invest, pred.invest)\n\n      RMSE      MAE        R2\n1 66.90304 36.76732 0.9391516\n\n\nAdicionando termo de interaçao ao modelo.\n\nlm_invest_int &lt;- lm(invest ~ . + value*capital, Grunfeld.tr)\nsummary(lm_invest_int)\n\n\nCall:\nlm(formula = invest ~ . + value * capital, data = Grunfeld.tr)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-146.016  -11.418   -2.304   12.401  192.342 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               66.0183    16.3423   4.040 8.27e-05 ***\nvalue                    102.4319    12.5440   8.166 8.77e-14 ***\ncapital                   -0.3027    10.8409  -0.028 0.977757    \nyear                      21.3233     4.8910   4.360 2.31e-05 ***\n`firm_General Motors`    157.6237    45.0929   3.496 0.000611 ***\n`firm_US Steel`          249.4813    26.0617   9.573  &lt; 2e-16 ***\n`firm_General Electric`  -58.0340    27.4135  -2.117 0.035797 *  \nfirm_Chrysler             39.1575    15.1944   2.577 0.010861 *  \n`firm_Atlantic Refining`  69.6205    23.1453   3.008 0.003053 ** \nfirm_IBM                  25.2365    14.0326   1.798 0.073983 .  \n`firm_Union Oil`          53.0164    16.8449   3.147 0.001964 ** \nfirm_Westinghouse         -2.1632    14.5174  -0.149 0.881735    \nfirm_Goodyear             29.0504    17.4580   1.664 0.098054 .  \n`firm_Diamond Match`      -7.8207    12.9832  -0.602 0.547777    \nvalue:capital             29.3232     3.2597   8.996 6.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 38.2 on 161 degrees of freedom\nMultiple R-squared:  0.9633,    Adjusted R-squared:  0.9601 \nF-statistic: 301.5 on 14 and 161 DF,  p-value: &lt; 2.2e-16\n\n\nConsegue fazer os próximos passos para avaliar o modelo com interação nos dados de teste?\n\n\n3.2.4 Regressão polinomial\nCarregando pacote e dados.\n\nlibrary(MASS)\n\n\nrehab &lt;- wtloss\n# ?wtloss\n\nVisualizando o comportamento do peso (massa) em função de dias de treinamento.\n\nggplot(rehab, aes(x = Days, y = Weight)) + \n  geom_point(color = \"deepskyblue3\", size = 2) +\n  xlab(\"Dias\") + \n  ylab(\"Peso\") + theme_bw()\n\nSeparando dados de treino e teste.\n\nset.seed(53)\ntr &lt;- round(0.75*nrow(rehab),0)\ntreino &lt;- sample(nrow(rehab), tr, replace = F)\nrehab.tr &lt;- rehab[treino,]\nrehab.te &lt;- rehab[-treino,]\n\nEstimando modelos linear e quadrático.\n\nlm1 &lt;- lm(Weight ~ Days, rehab.tr)\nsummary(lm1)\n\n\nlm2 &lt;- lm(Weight ~ Days + I(Days^2), rehab.tr)\nsummary(lm2)\n\nDesempenho para dados de treino.\n\nmetrics(rehab.tr$Weight, lm1$fitted.values)\n\n\nmetrics(rehab.tr$Weight, lm2$fitted.values)\n\nDesempenho dos modelos para dados de teste.\n\npred.lm1 &lt;- predict(lm1, newdata = rehab.te)\nmetrics(rehab.te$Weight, pred.lm1)\n\n\npred.lm2 &lt;- predict(lm2, newdata = rehab.te)\nmetrics(rehab.te$Weight, pred.lm2)\n\nPlotando o modelo com dados de teste.\n\nggplot() + \n  geom_point(data = rehab.tr, mapping = aes(x = Days, y = Weight), color = \"deepskyblue3\", size = 2) +\n  geom_smooth(data = rehab.te, mapping = aes(x = Days, y = Weight), \n              method = \"lm\", formula = y ~ x + I(x^2), se = F, col = \"mediumvioletred\") +\n  ggtitle(\"Peso vs Dias (dados de teste)\") + \n  xlab(\"Dias\") + \n  ylab(\"Peso\") + theme_bw()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regressão linear simples, múltipla e polinomial</span>"
    ]
  }
]