[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aprendizado Supervisionado, uma abordagem com implementações em R",
    "section": "",
    "text": "Prefácio\nApostila de Aprendizado Supervisionado com Implementações em R.",
    "crumbs": [
      "Prefácio"
    ]
  },
  {
    "objectID": "AS_Intro.html",
    "href": "AS_Intro.html",
    "title": "1  Introdução ao aprendizado supervisionado",
    "section": "",
    "text": "1.1 Aprendizado\nNo contexto da ciência de dados (data science) o aprendizado consiste em adquirir um determinado comportamento a partir dos dados disponíveis. Mais especificamente, este comportamento a ser aprendido está relacionado a previsão de uma determinada resposta de interesse em função de outras variáveis ou atributos disponíveis nos dados. Esta resposta pode ser quantitativa, ou seja, medida em uma esclaa real, ou qualitativa, constando de um conjunto finito de possibilidades. Para tal, obviamente faz-se necessário a utilização de dados. Estes, por sua vez, apesar de estarem atualmente disponíveis em abundância, tanto nas organizações privadas quanto via acesso público, nem sempre estão prontos para análise.\nExistem dois campos que regem a teoria do Aprendizado:\nAlguns métodos surgiram no contexto do Aprendizado estatístico, subcampo da Estatística, tais como as árvores de decisão, aprendizado por reforço e máquinas de vetores de suporte, enquanto outros surgiram no contexto da Inteligência Artificial, subcampo das Ciências da computação, tais como as redes neurais e o aprendizado profundo. Hoje é difícil separar ambos campos, apesar de o aprendizado de máquina ser muito mais popular.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao aprendizado supervisionado</span>"
    ]
  },
  {
    "objectID": "AS_Intro.html#aprendizado",
    "href": "AS_Intro.html#aprendizado",
    "title": "1  Introdução ao aprendizado supervisionado",
    "section": "",
    "text": "Estatístico (statistical learning);\nde Máquina (machine learning).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao aprendizado supervisionado</span>"
    ]
  },
  {
    "objectID": "AS_Intro.html#um-pouco-de-história",
    "href": "AS_Intro.html#um-pouco-de-história",
    "title": "1  Introdução ao aprendizado supervisionado",
    "section": "1.2 Um pouco de história",
    "text": "1.2 Um pouco de história\nInicialmente serão citados alguns teóricos importantes para a estatística frequentista e paramétrica. Porém, alguns métodos propostos, por exemplo a análise discriminante linear de Fisher é utilizada hoje como método de aprendizado supervisionado para classificação.\n\nWillian Gosset (Student), 1908-1909: criou o teste t e a distribuição t de Student quando trabalhava na cervejaria Guiness. Sua intenção era criar uma aproximação da distribuição normal para amostras de tamanhos limitados.\nRonald Fisher, 1920-1940: Criou vários testes e conceitos estatísticos importantes, como a ANOVA, análise discriminante linear, p-valor, entre outros. Seus principais desenvolvimentos foram realizados especialmente enquanto trabalhava na estação agrícola Rothamsted Research no Reino Unido.\nGeorge Box, 1948-1992: Considerado uma dos maiores pesquisadores em estatística do século XX, desenvolveu trabalhos e métodos em controle de qualidade, planejamento de experimentos, séries temporais e inferência Bayesiana. Cunhou a famosa frase: “All models are wrong, some are usefull”.\n\n\n\n\n\n\nGosset, Fisher e Box\n\n\n\n\nAlguns estatísticos foram importantes para definir termos que hoje são populares no contexto da teoria do aprendizado supervisionado e mais amplamente da ciência e análise de dados.\n\nJohn Tukey (1962, 1977): Cunhou o termo análise exploratória de dados, com o objetivo de incentivar a ênfase em gráficos, tabelas e limpeza de dados para resumir dados e apontar suas tendências.\nJeff Wu (1980): Formulou o termo data science e inclusive recomendou que a área de conhecimento estatística fosse renomeada para ciência de dados.\n\nA Figura 1.1 expõe fotos dos estatísticos Tukey e Wu.\n\n\n\n\n\n\n\n\nFigura 1.1: Tukey e wu\n\n\n\n\n\nVários bioestatísticos de Stanford tiveram contribuições importantes no campo do aprendizado estatístico. Bradley Efron desenvolveu nas décadas de 70 e 80 o método bootstrap, um método que visa estimar o erro a partir da amostragem com reposição amplamente usado em inferência e em aprendizado de máquina. Jerome Friedman desenvolveu o método floresta aleatória e o reforço de gradiente. Trevor Hastie propôs os modelos aditivos generalizados e Robert Tibshirani propôs a regularização via LASSO. Estes autores, Figura 1.2, tiveram outras contribuições importantes na estatística e computação.\n\n\n\n\n\n\n\n\nFigura 1.2: Efron, Friedman, Hastie e Tibishirani\n\n\n\n\n\nA densa teoria do aprendizado estatístico foi cunhada pelo matemático russo Vladmir Vapnik, Figura 1.3, visando obter um modelo preditivo a partir dos dados. Inicialmente não foi proposto um método específico, mas o arcabouço teórico necessário para sustentar a capacidade de generalização de modelos obtidos a partir de amostras limitadas, porém suficientes. Posteriormente Vapnik e co-autores propuseram as máquinas de vetores de suporte, método aplicado com sucesso até hoje em problemas de aprendizado supervisionado. Seus principais trabalhos foram publicados na década de 90.\n\n\n\n\n\n\n\n\nFigura 1.3: Vapnik\n\n\n\n\n\nNomes importantes da computação incluem Christopher Bishop, com grande contribuição em redes neurais e Andrew Ng, Figura 1.4 com contribuições e militância recentes em aplicações, pesquisa e ensino de aprendizado profundo e inteligência artificial.\n\n\n\n\n\n\n\n\nFigura 1.4: Bishop e Andrew\n\n\n\n\n\n\n1.2.1 Um brasileiro importante\nCarlos Guestrin, Figura 1.5, é um brasileiro que tem feito um excelente trabalho na área de aprendizado por reforço, é professor da Universidade de Stanford, foi diretor sênior de Machine Learning da Apple (2016-2021). Co-criador do método recente de reforço por gradiente extremo (extreme gradient boosting), usado com sucesso em aprendizado supervisionado.\n\n\n\n\n\n\n\n\nFigura 1.5: Carlos Guestrin\n\n\n\n\n\nA seguir será classificado o aprendizado supervisionado, sendo expostos exemplos práticos de aplicação.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao aprendizado supervisionado</span>"
    ]
  },
  {
    "objectID": "AS_Intro.html#aprendizado-supervisionado",
    "href": "AS_Intro.html#aprendizado-supervisionado",
    "title": "1  Introdução ao aprendizado supervisionado",
    "section": "1.3 Aprendizado Supervisionado",
    "text": "1.3 Aprendizado Supervisionado\nSeja um conjunto de variáveis de entrada, independentes ou preditores \\(\\mathbf{x} = [x_1, x_2, ..., x_k]^T\\) e uma variável dependente ou supervisora \\(y\\). Dado uma amostra de observações para tais variáveis, o aprendizado supervisionado visa prever o comportamento ou resultado de \\(y\\), considerando valores futuros de \\(\\mathbf{x}\\), \\(\\mathbf{x}_0\\). O aprendizado supervisionado pode ser classificado em dois tipos:\n\nRegressão, \\(y \\in \\mathbb{R}\\), ou seja, quando a resposta ou supervisor pode ser medida em uma escala real (há casos para variáveis de processos de contagem em uma taxa média de ocorrência, entre outros);\nClassificação, \\(y \\in \\{A, B, C, ...\\}\\), ou seja, quando a resposta pertence a um conjunto finito de categorias.\n\nA Figura 1.6 ilustra o resultado de um exemplo de aplicação de regressão linear simples para prever o consumo em função da renda. O modelo foi treinado com um conjunto de dados de 33 observações e aplicado em 11 observações de teste. É importante observar uma boa aproximação do modelo aos dados, especialmente para os dados de teste ou futuros. É importante a utilização de métricas de ajuste para melhor avaliar a acuracidade dos modelos. Algumas métricas comumente usadas serão abordadas posteriormente.\n\n\n\n\n\n\n\n\nFigura 1.6: Consumo e renda nos EUA de 1950-1993\n\n\n\n\n\nA Figura 1.7 apresenta o resultado gráfico de um exemplo de aplicação de classificação, sendo estimado um modelo de regressão logística para classificação de pessoas com diabetes em relação ao nível de glicose, onde “1” = diabético e “0” = não diabético. O modelo prevê a probabilidade de pertencer a classe 1, isto é, \\(P(y=1|x)\\). A discriminação é realizada considerando a probabilidade intermediária, isto é,\n\\[\n\\bigg\\{\\begin{matrix}\ny = 1, \\text{ se } p&gt;0,5, \\\\\ny = 0, \\text{ cc}.\n\\end{matrix}\n\\]\n\n\n\n\n\n\n\n\nFigura 1.7: Regressão logística para diabetes em função do nível de glicose\n\n\n\n\n\nNeste segundo exemplo da Figura 1.8 considera-se além da glicose a idade do paciente para obter um modelo de regressão logística de forma a classificar pacientes com diabetes. Foram consideradas 294 observações para treinamento e 98 para teste do modelo.\n\n\n\n\n\n\n\n\nFigura 1.8: Regressão logística para diabetes em função do nível de glicose e idade",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao aprendizado supervisionado</span>"
    ]
  },
  {
    "objectID": "AS_Intro.html#aprendizado-não-supervisionado",
    "href": "AS_Intro.html#aprendizado-não-supervisionado",
    "title": "1  Introdução ao aprendizado supervisionado",
    "section": "1.4 Aprendizado Não-supervisionado",
    "text": "1.4 Aprendizado Não-supervisionado\nSeja um conjunto de \\(N\\) observações, \\(\\mathbf{x}_1\\), \\(\\mathbf{x}_2\\), …, \\(\\mathbf{x}_N\\), de \\(k\\) variáveis independentes \\(\\mathbf{x}=[x_1, x_2, ..., x_k]^T\\). O aprendizado não-supervisionado visa obter informações a partir dos próprios dados, sem a necessidade de um supervisor ou variável dependente. Constitui-se de técnicas de agrupamento e de redução de dimensionalidade.\nA Figura 1.9 expõe um gráfico do resultado de um agrupamento por \\(k\\)-médias considerando distintos índices de demografia dos EUA. São plotados os dois índices mais importantes.\n\n\n\n\n\n\n\n\nFigura 1.9: Algoritmo k-means para agrupar estados americanos segundo índices de criminalidade\n\n\n\n\n\nA Figura 1.10 expõe um gráfico de dispersão para idade e peso de órgãos retirados de 30 focas do Cabo que morreram como consequência não intencional da pesca comercial. Devido a alta correção entre as variáveis, \\(R = 0.95\\), foi realizada uma análise de componentes principais para obter uma nova variável ou componente principal que represente ambas as variáveis, reduzindo a dimensionalidade do problema. A nova variável obtida, plotada em azul escuro, representa 98% da variabilidade das variáveis originais.\n\n\n\n\n\n\n\n\nFigura 1.10: Análise de componentes principais para encontrar uma nova variável que represente o peso e a idade",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao aprendizado supervisionado</span>"
    ]
  },
  {
    "objectID": "AS_Intro.html#implementações-em-r",
    "href": "AS_Intro.html#implementações-em-r",
    "title": "1  Introdução ao aprendizado supervisionado",
    "section": "1.5 Implementações em R",
    "text": "1.5 Implementações em R\n\n1.5.1 Exemplo de problema de regressão\nA seguir serão expostas as implementações necessárias para obter os resultados do capítulo.\nCarregando as bibliotecas (pacotes) para análise.\n\nlibrary(AER) # para base de dados\nlibrary(ggplot2) # para gráficos\n\nCarregando base de dados.\n\ndata(USConsump1993)\n# ?USConsump1993\n\n\nConsumo &lt;- data.frame(USConsump1993)\nhead(Consumo)\n\nSeparando 75% dos dados para treino do modelo e 25% para teste.\n\ntr &lt;- round(0.75*nrow(Consumo))\n\nset.seed(9)\ntreino &lt;- sample(nrow(Consumo), tr, replace = F)\n\nConsumo.tr &lt;- Consumo[treino,]\nConsumo.te &lt;- Consumo[-treino,]\n\n\nhead(Consumo.tr)\n\n\nhead(Consumo.te)\n\nTreinando um modelo de regressão linear simples.\n\nlm1 &lt;- lm(expenditure ~ income, data = Consumo.tr)\nsummary(lm1)\n\nPlotando o modelo com dados de treino.\n\nggplot(data = Consumo.tr, aes(x = income, y = expenditure)) + \n   geom_point(color = 'red', size = 2) +\n   geom_smooth(method = \"lm\", formula = y ~ x) +\n   xlab(\"renda\") + \n   ylab(\"consumo\") + theme_bw()\n\nRealizando previsão com o modelo.\n\npredict(lm1, newdata = data.frame(income = 9000))\n\nPrevisão para todos dados de teste.\n\nConsumo.te$exp_pred &lt;- predict(lm1, \n                               newdata = \n                                 data.frame(income = \n                                              Consumo.te$income))\n\n# head(Consumo.te)\n\nPlotando o modelo com os dados de teste.\n\nggplot() + \n  geom_point(data = Consumo.te, aes(x = income, y = expenditure), size = 2) +\n  geom_smooth(method = \"lm\", formula = y ~ x, \n              data = Consumo.tr,\n              aes(x = income, y = expenditure)) +\n  xlab(\"renda\") + \n  ylab(\"consumo\") + theme_bw()\n\nCriando uma função de métricas de desempenho.\n\nmetrics &lt;- function(obs, pred) {\n  \n  RSE &lt;- sum((obs - pred)^2)\n  SST &lt;- sum((obs - mean(obs))^2)\n  R2 &lt;- 1 - RSE/SST \n  \n  MAE &lt;-  mean(abs(obs - pred))\n  \n  RMSE &lt;- sqrt(mean((obs - pred)^2))\n  \n  return(\n    data.frame(RMSE = RMSE,\n               MAE = MAE,\n               R2 = R2))\n}\n\nDesempenho do modelo para dados de teste.\n\nmetrics(Consumo.te$expenditure, Consumo.te$exp_pred)\n\n\n\n1.5.2 Exemplo de problema de classificação\n\nlibrary(mlbench)\n\n\ndata(PimaIndiansDiabetes2)\n# ?PimaIndiansDiabetes2\n\n\nPimaIndiansDiabetes2 &lt;- na.omit(PimaIndiansDiabetes2)\nPimaIndiansDiabetes2$diabetes &lt;- ifelse(PimaIndiansDiabetes2$diabetes==\"neg\",0,1)\n\ndados &lt;- PimaIndiansDiabetes2\n\nhead(dados)\n\n\nset.seed(7)\ntreino &lt;- sample(nrow(dados), 0.75*nrow(dados))\ndados_treino &lt;- dados[treino,]\ndados_test &lt;- dados[-treino,]\n\nObtendo um modelo de regressão logística simples a partir dos dados de treino considerando apenas uma variável regressora, o nível de glicose.\n\nmodel1 &lt;- glm( diabetes ~ glucose, data = dados_treino, family = binomial)\nsummary(model1)\n\nPlotando o modelo com os dados de treino.\n\nggplot(dados_treino, aes(glucose, diabetes)) +\n  geom_point(aes(col = as.factor(diabetes)), alpha = 0.5) +\n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\"), fill = \"grey\", col = \"black\") +\n  labs(x = \"Concentracao de glicose\", y = \"Probabilidade de ter diabetes\", col = \"diabetes\") +\n  theme_bw()\n\nPlotando o modelo com os dados de teste.\n\nggplot() +\n  geom_point(data = dados_test, \n             mapping = aes(glucose, diabetes, col = as.factor(diabetes)), \n             alpha = 0.5) +\n  geom_smooth(data = dados_treino, \n              mapping = aes(glucose, diabetes), \n              method = \"glm\", \n              method.args = list(family = \"binomial\"), \n              col = \"black\") +\n  labs(x = \"Concentracao de glicose\", y = \"Probabilidade de ter diabetes\", col = \"diabetes\") + theme_bw()\n\nPrevisão com os dados de teste.\n\ndados_test$prob &lt;- predict(model1, \n                           newdata = data.frame(glucose = dados_test$glucose), \n                           type = 'response')\ndados_test$y_pred &lt;- ifelse(dados_test$prob &gt; 0.5, 1, 0)\n\nhead(dados_test)\n\nMatriz de confusão para dados de teste.\n\ncm1 &lt;- table(data = dados_test$diabetes, model = dados_test$y_pred)\ncm1\n\nProporção de observações de teste classificadas corretamente.\n\nmean(dados_test$diabetes == dados_test$y_pred)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao aprendizado supervisionado</span>"
    ]
  },
  {
    "objectID": "AS_Intro.html#referências",
    "href": "AS_Intro.html#referências",
    "title": "1  Introdução ao aprendizado supervisionado",
    "section": "Referências",
    "text": "Referências\nBishop, Christopher M. “Neural networks for pattern recognition”. Oxford university press, 1995.\nBox, George EP, and Kenneth B. Wilson. “On the experimental attainment of optimum conditions.” Breakthroughs in statistics: methodology and distribution. New York, NY: Springer New York, 1992. 270-310.\nChen, Tianqi, and Carlos Guestrin. “Xgboost: A scalable tree boosting system.” Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining. 2016.\nChipman, Hugh A., and V. Roshan Joseph. “A conversation with Jeff Wu.” (2016): 624-636.\nFisher, Ronald Aylmer. “Statistical methods for research workers.” Statistical methods for research workers. 6th Ed (1936).\nStudent. “The probable error of a mean.” Biometrika (1908): 1-25.\nTukey, John W. “Exploratory data analysis.” Reading/Addison-Wesley (1977).\nVapnik, Vladimir. “The nature of statistical learning theory.” John Wiley google schola 2 (1995): 259-275.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução ao aprendizado supervisionado</span>"
    ]
  },
  {
    "objectID": "AS_vicio_var.html",
    "href": "AS_vicio_var.html",
    "title": "2  Conflito entre vício e variância no aprendizado supervisionado",
    "section": "",
    "text": "2.1 Intuição e conflito entre vício e variância em problemas de regressão\nSeja uma variável regressora ou independente \\(x \\in \\mathbb{R}\\) e uma resposta ou variável dependente igualmente medida em uma escala contínua, \\(y \\in \\mathbb{R}\\).\nDeseja-se aproximar uma função desconhecida, \\(f(x)\\), que relaciona \\(y\\) e \\(x\\). Tal aproximação pode ser feita minimizando uma função perda, \\(L(y,f(x))\\), que mede os erros de previsão.\nA função perda mais comum para problemas de regressão é a função perda quadrática, conforme segue.\n\\[\nL(y,f(x)) = (y-f(x))^2\n\\]\nSeja um conjunto de dados disponível, \\(D = {(x_1,y_1), (x_2,y_2), ..., (x_n,y_n)}\\), conforme o observado graficamente na Figura 3.1. As observações da variável dependente podem ser descritas em relação à função desconhecida adicionada de um termo de erro, \\(y = f(x) + \\varepsilon\\), \\(E(\\varepsilon)=0\\), \\(Var(\\varepsilon) = \\sigma^2_\\varepsilon\\). O erro das medições tem variância \\(Var(\\varepsilon) = \\sigma^2_\\varepsilon\\) e consiste no erro irredutível. Este erro está associado à qualidade dos dados. Portanto, sua variabilidade terá implicação no erro do modelo estimado.\nFigura 2.1: Exemplo de dados para regressão linear simples\nSeja um amplo número de conjuntos de dados \\(D\\) distintos e tomados de forma independente da população, todos de tamanho \\(N\\). Para cada um destes podemos aplicar um algoritmo para obter uma aproximação \\(\\hat f_D(x)\\). Obviamente cada conjunto de dados resultará em uma aproximação diferente, com perda distinta. Para medir o desempenho de um algoritmo ou método pode-se considerar a média de todos modelos obtidos via conjuntos de dados distintos.\nSeja o erro quadrático de um modelo em relação à função desconhecida.\n\\[\n\\{\\hat f_D(x) - f(x)\\}^2\n\\]\nSeja a média de distintos modelos considerando distintos data sets, \\(E_D[\\hat f_D(x)]\\). Somando e subtraindo este termo dentro da perda quadrática, tem-se:\n\\[\n\\begin{align}\n\\{\\hat f_D(x) - f(x)\\}^2 =& \\{\\hat f_D(x) - E_D[\\hat f_D(x)] + E_D[\\hat f_D(x)] - f(x)\\}^2\\\\\n\\{\\hat f_D(x) - f(x)\\}^2 =& \\{\\underbrace{(\\hat f_D(x) - E_D[\\hat f_D(x)])}_\\text{a} + \\underbrace{(E_D[\\hat f_D(x)] - f(x))}_\\text{b}\\}^2\\\\\n\\end{align}\n\\]\nDesenvolvendo o quadrado, tem-se:\n\\[\n\\begin{align}\n\\{\\hat f_D(x) - f(x)\\}^2 = (\\hat f_D(x) - E_D[\\hat f_D(x)])^2 + 2(\\hat f_D(x) - E_D[\\hat f_D(x)])(E_D[\\hat f_D(x)] - f(x)) + (E_D[\\hat f_D(x)] - f(x))^2\n\\end{align}\n\\]\nTomando a média (esperança, \\(E\\)) em relação ao conjuntos de dados, \\(D\\), o segundo termo se anulará, pois \\(E_D \\{\\hat f_D(x) - E_D[\\hat f_D] \\}\\)=0. Logo:\n\\[\nE_D\\{\\hat f_D(x) - f(x)\\}^2 = \\underbrace{E_D\\{\\hat f_D(x) - E_D[\\hat f_D(x)]\\}^2}_\\text{variância} + \\underbrace{\\{E_D[\\hat f_D(x)] - f(x)\\}^2}_\\text{vício}\n\\]\nAgora considerando um valor específico de \\(x\\), \\(x_0\\), em detrimento dos conjuntos de dados \\(D\\), tem-se para o vício, uma vez que \\(E[\\varepsilon]=0\\).\n\\[\n\\begin{aligned}\nVicio_D[\\hat f(x_0)] &= E_D[\\hat f(x_0) - f(x_0)] = E_D[\\hat f(x_0) - (y + \\varepsilon)] \\\\\nVicio_D[\\hat f(x_0)] &= E_D[\\hat f(x_0)] - E_D[y]\n\\end{aligned}\n\\]\nE para a variância:\n\\[\n\\begin{aligned}\nVar_D[\\hat f(x)] &= E_D[(\\hat f_D(x) - E_D{\\hat f(x)})^2]\n\\end{aligned}\n\\]\nSeja o ajuste aos dados disponíveis de três modelos de regressão com distintas complexidades, um linear, um cúbico e um modelo polinomial de décima ordem.\nTais modelos apresentam as seguintes formas:\n\\[\n\\begin{matrix}\ny_1 = \\beta_0 + \\beta_1x\\\\\ny_3 = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\beta_3x^3\\\\\ny_{10} = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\beta_3x^3 + \\beta_4x^4 + ... + \\beta_{10}x^{10} \\end{matrix}\n\\]\nOs modelos são estimados por mínimos quadrados e plotados com os dados na Figura 2.2.\nFigura 2.2: Modelo de regressão simples\nTomando um valor arbitrário de \\(x = x_0\\), e \\(M\\) conjuntos distintos de observações futuras, \\(d_1=(x_1,y_1)_1, ..., (x_n,y_n)_1\\), \\(d_2=(x_1,y_1)_2, ..., (x_n,y_n)_2\\), \\(\\ldots, d_M=(x_1,y_1)_M, ..., (x_n,y_n)_M\\), é possível medir o vício e a variância das estimativas obtidas por \\(\\hat f(x)\\) para um \\(x=x_0\\) específico. A Figura 2.3 compara graficamente o resultado de tais modelos em observações futuras via boxplots, enquanto a Figura 2.4 faz a mesma comparação usando gráficos de densidade amostral. Pelos gráficos pode-se observar que o modelo linear apresenta maior vício e menor variância, enquanto o modelo polinomial de décima ordem apresenta baixo vício, porém maior variância. Este exemplo ilustra o conflito entre vício e variância. Deve-se buscar um modelo que apresente equilíbrio entre ambas medidas.\nFigura 2.3: Box-plots do erro dos modelos em distintos conjuntos de dados\nFigura 2.4: Distribuição do erro dos modelos em distintos conjuntos de dados\nA Tabela 2.1 apresenta valores de vício, enquanto a Tabela 2.2 apresenta valores de variância dos modelos segundo a ordem ou complexidade entre os mesmos. Observa-se que o modelo de menor grau e, portanto, menor complexidade, apresenta alto vício, porém baixa variância. Já o modelo de maior grau, ou maior complexidade, apresenta baixo vício, porém alta variância.\nTabela 2.1: Vício versus complexidade dos modelos\n\n\n\n\n\n\nordem\nvicio\n\n\n\n\n1\n0.3134804\n\n\n3\n0.0002074545\n\n\n10\n0.0002207692\nTabela 2.2: Variância versus complexidade dos modelos\n\n\n\n\n\n\nordem\nvar\n\n\n\n\n1\n0.07020973\n\n\n3\n0.1268333\n\n\n10\n0.2922431\nA Figura 2.5 plota os 1000 modelos lineares obtidos com distintos conjuntos de dados em torno do modelo real. Pode-se observar que o conjunto de dados pode influenciar nos modelos, mas a média de diversos modelos cúbicos certamente apresentará um bom equilíbrio entre vício e variância para estes dados.\nFigura 2.5: Modelos lineares obtidos com distintos conjuntos de dados e modelo real\nA Figura 2.6 plota os 1000 modelos cúbicos obtidos com distintos conjuntos de dados em torno do modelo real. Pode-se observar que o conjunto de dados pode influenciar nos modelos e a média de diversos modelos lineares apresenta alto viés em relação ao modelo real desconhecido.\nFigura 2.6: Modelos cúbicos obtidos com distintos conjuntos de dados e modelo real\nPor fim, a Figura 2.7 plota os 1000 modelos polinomiais de décima ordem obtidos com distintos conjuntos de dados em torno do modelo real. Pode-se observar que o conjunto de dados pode influenciar nos modelos, a média de tais modelos é próxima do modelo desconhecido, porém observa-se alta variabilidade de tais modelos em comparação com os anteriores.\nWarning: Removed 69 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\nFigura 2.7: Modelos polinomiais de décima ordem obtidos com distintos conjuntos de dados e modelo real\nEm casos onde o número de observações é baixo o conflito entre vício e variância pode ficar mais claro. Considerando o exemplo plotado na Figura 2.8, pode-se observar que enquanto o modelo linear apresenta um baixo ajuste, o modelo polinomial de décima ordem apresenta um sobreajuste, uma vez que praticamente interpola os dados disponíveis. Este último apresentará um ajuste muito além do real, uma vez que sua capacidade não se confirmará para dados futuros.\nFigura 2.8: Modelos com ordens distintas ajustados a um conjunto de dados com poucas observações\nA Figura 2.9 apresenta o erro ou valor da função perda de tais modelos para dados de treino e dados de teste ou futuros.\nFigura 2.9: Erro para dados de treino e teste\nPode-se observar que para os dados de treino, à medida que se aumenta a complexidade do modelo ajustado o erro diminui. Entretanto, o mesmo não acontece para os dados futuros ou de teste do modelo.\nQuando um modelo apresenta alto erro tanto para os dados de treino quanto para os dados de teste, ele é considerado um modelo com baixo ajuste. Quando um modelo apresenta baixo erro para os dados de treino, mas alto erro para os dados de teste, ele é considerado um modelo com sobreajuste. É importante diagnosticar o sobreajuste uma vez que o bom ajuste aos dados de treino pode dar a ilusão de um modelo perfeito, porém, a avaliação do modelo em dados futuros confirma a dificuldade de generalização deste tipo de modelo.\nO melhor modelo seria aquele que apresenta mais proximidade entre os erros para os dados de treinamento e teste, que, no exemplo dado, seria o de terceira ordem.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conflito entre vício e variância no aprendizado supervisionado</span>"
    ]
  },
  {
    "objectID": "AS_vicio_var.html#implementações-em-r",
    "href": "AS_vicio_var.html#implementações-em-r",
    "title": "2  Conflito entre vício e variância no aprendizado supervisionado",
    "section": "2.2 Implementações em R",
    "text": "2.2 Implementações em R\nA seguir serão expostas as implementações necessárias para obter os resultados do capítulo.\nCarregando as bibliotecas (pacotes) para análise.\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nCriando uma função para simular o conflito. Na prática tal função é desconhecida; portanto deseja-se aproximá-la apartir dos dados.\n\nf_sin &lt;- function(x) {\n  f &lt;- sin(2*pi*x)\n  return(f)\n}\n\nSimulando dados. x foi simulado segundo uma distribuição uniforme com limites entre 0 e 1. y foi simulado avaliando os valores de x na função criada previamente adicionando um termo de erro segundo a distribuição normal com média nula e desvio-padrão igual a 0.2.\n\nset.seed(7)\nx &lt;- runif(100,0,1)\ndata &lt;- data.frame(x = x,\n                   y = f_sin(x) + rnorm(100, 0, .2))\n\nknitr::kable(head(data))\n\nPlotando.\n\nggplot(data, aes(x,y)) + \n  geom_point(col=\"red\") +\n  theme_bw()\n\nPlotando os dados disponíveis acompanhados de três modelos de regressão polinomial simples, um linear, um cúbico e um de décima ordem.\n\ncolors &lt;- c(\"y\" = \"red3\", \"linear\" = \"grey\", \"cúbico\" = \"green2\", \"poly-10\" = \"purple\")\n\nggplot(data, aes(x,y)) + \n  geom_point(aes(color = \"y\")) +\n  geom_smooth(method = lm, formula = y ~ x, se = F, aes(color = \"linear\")) +\n  geom_smooth(method = lm, formula = y ~ poly(x, degree = 3, raw = TRUE), \n              se = F, aes(color = \"cúbico\")) +\n  geom_smooth(method = lm, formula = y ~ poly(x, degree = 10, raw = TRUE), \n              se = F, aes(color = \"poly-10\")) +\n  labs(color = \"Legend\") +\n    scale_color_manual(values = colors) +\n  theme_bw()\n\nTomando um valor arbitrário de \\(x = 075\\), e conjuntos distintos de observações futuras para medir o vício e a variância de cada modelo.\n\nset.seed(7)\n\npred &lt;- data.frame(model = numeric(3000),\n                   pred = numeric(3000),\n                   vicio = numeric(3000))\n\nx0=0.75\n\ni&lt;-1\n\nE_y &lt;- f_sin(x0)\nE_y\n\n\nfor (k in 1:1000) {\n\ndata_ &lt;- data.frame(x = x,\n                    y = f_sin(x) + rnorm(100, 0, 2))\n\nlm1 &lt;- lm(y ~ x, data_)\nlm3 &lt;- lm(y ~ poly(x, degree = 3, raw = TRUE), data_)\nlm10 &lt;- lm(y ~ poly(x, degree = 10, raw = TRUE), data_)\n                    \npred1 &lt;- predict(lm1, newdata = data.frame(x=x0)) \npred3 &lt;- predict(lm3, newdata = data.frame(x=x0)) \npred10 &lt;- predict(lm10, newdata = data.frame(x=x0))  \n\nvicio1 &lt;- pred1 - E_y\nvicio3 &lt;- pred3 - E_y\nvicio10 &lt;- pred10 - E_y\n\npred$model[i] &lt;- 1\npred$model[i+1] &lt;- 3\npred$model[i+2] &lt;- 10\n\npred$pred[i] &lt;- pred1\npred$pred[i+1] &lt;- pred3\npred$pred[i+2] &lt;- pred10\n\npred$vicio[i]   &lt;- vicio1\npred$vicio[i+1] &lt;- vicio3\npred$vicio[i+2] &lt;- vicio10\n\ni &lt;- i + 3\n\n} \n\n\nggplot(pred, aes(x=as.factor(model),y=vicio, col = as.factor(model))) +\n  geom_boxplot() + \n  geom_jitter(alpha = .1) +\n  geom_hline(yintercept = 0, linetype=2) +\n  labs(x = \"ordem\", y = \"erro\") +\n  theme_bw() +\n  theme(legend.position=\"none\")\n\n\nggplot(pred, aes(x=vicio, fill = as.factor(model))) +\n  geom_density(alpha = .3, col = \"white\") + \n  geom_vline(xintercept = 0, linetype=2) +\n  labs(y=\"densidade\", x = \"erro\", fill = \"ordem\") +\n  theme_bw()\n\nSimulando o conflito para um caso com poucas observações disponíveis.\n\nset.seed(7)\nxa &lt;- runif(15,0,1)\ndataa &lt;- data.frame(x = xa,\n                    y = f_sin(xa) + rnorm(15, 0, .2))\n\ncolors &lt;- c(\"y\" = \"red3\", \"linear\" = \"grey\", \"cúbico\" = \"green2\", \"poly-10\" = \"purple\")\n\nggplot(dataa, aes(x,y)) + \n  geom_point(aes(color = \"y\")) +\n  geom_smooth(method = lm, formula = y ~ x, se = F, aes(color = \"linear\")) +\n  geom_smooth(method = lm, formula = y ~ poly(x, degree = 3, raw = TRUE), \n              se = F, aes(color = \"cúbico\")) +\n  geom_smooth(method = lm, formula = y ~ poly(x, degree = 10, raw = TRUE), \n              se = F, aes(color = \"poly-10\")) +\n  labs(color = \"Legend\") +\n    scale_color_manual(values = colors) +\n  theme_bw()\n\nErro ou valor da função perda de tais modelos para dados de treino e dados de teste.\n\nset.seed(7)\nxa &lt;- runif(20,0,1)\ndataa &lt;- data.frame(x = xa,\n                    y = f_sin(xa) + rnorm(20, 0, .2))\n\nlm1a &lt;- lm(y ~ x, dataa)\nlm2a &lt;- lm(y ~ poly(x, degree = 2, raw = TRUE), dataa)\nlm3a &lt;- lm(y ~ poly(x, degree = 3, raw = TRUE), dataa)\nlm5a &lt;- lm(y ~ poly(x, degree = 5, raw = TRUE), dataa)\nlm10a &lt;- lm(y ~ poly(x, degree = 10, raw = TRUE), dataa)\n\nset.seed(27)\nxb &lt;- runif(20,0,1)\ndatab &lt;- data.frame(x = xb,\n                    y = f_sin(xb) + rnorm(20, 0, .2))\n\nMSE &lt;- function(obs, pred) {\n  MSE &lt;- mean((obs - pred)^2)\n  return(MSE)\n}\n\nMSE1_T &lt;- MSE(dataa$y, lm1a$fitted.values)\nMSE2_T &lt;- MSE(dataa$y, lm2a$fitted.values)\nMSE3_T &lt;- MSE(dataa$y, lm3a$fitted.values)\nMSE5_T &lt;- MSE(dataa$y, lm5a$fitted.values)\nMSE10_T &lt;- MSE(dataa$y, lm10a$fitted.values)\n\nyhat1 &lt;- predict(lm1a, newdata = datab)\nyhat2 &lt;- predict(lm2a, newdata = datab)\nyhat3 &lt;- predict(lm3a, newdata = datab)\nyhat5 &lt;- predict(lm5a, newdata = datab)\nyhat10 &lt;- predict(lm10a, newdata = datab)\n\nMSE1_t &lt;- MSE(datab$y, yhat1)\nMSE2_t &lt;- MSE(datab$y, yhat2)\nMSE3_t &lt;- MSE(datab$y, yhat3)\nMSE5_t &lt;- MSE(datab$y, yhat5)\nMSE10_t &lt;- MSE(datab$y, yhat10)\n\ndata_MSE &lt;- data.frame(model = rep(c(1,2,3,5,10),2),\n                       dados = c(rep(\"treino\",5),rep(\"teste\",5)),\n                       MSE = c(MSE1_T, MSE2_T, MSE3_T, MSE5_T, MSE10_T, \n                               MSE1_t, MSE2_t, MSE3_t, MSE5_t, MSE10_t))\n\nggplot(data_MSE, aes(x=model, y = MSE, \n                     col = dados,\n                     linetype = dados,\n                     pch = dados)) + \n  geom_point(size=2) +\n  geom_line() +\n  labs(x = \"ordem\") +\n  scale_x_continuous(breaks = 1:10) +\n  theme_bw()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conflito entre vício e variância no aprendizado supervisionado</span>"
    ]
  },
  {
    "objectID": "AS_vicio_var.html#referências",
    "href": "AS_vicio_var.html#referências",
    "title": "2  Conflito entre vício e variância no aprendizado supervisionado",
    "section": "Referências",
    "text": "Referências\nBishop, Christopher M., and Hugh Bishop. “Deep learning: foundations and concepts.” (2024).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Conflito entre vício e variância no aprendizado supervisionado</span>"
    ]
  },
  {
    "objectID": "AS_reg_ols.html",
    "href": "AS_reg_ols.html",
    "title": "3  Regressão linear simples, múltipla e polinomial",
    "section": "",
    "text": "3.1 Regressão linear simples\nSeja um problema onde deseja-se prever uma resposta contínua, \\(y \\in \\mathbb{R}\\), em função de uma única variável independente também contínua, \\(x \\in \\mathbb{R}\\). Conforme observado graficamente na Figura 3.1, pode-se considerar em diversos casos a aproximação de uma função linear para tal relação.\nFigura 3.1: Exemplo de conjunto de dados para regressão linear simples\nTal aproximação pode ser descrita pela Equação à seguir, onde \\(\\hat{y}\\) consiste no valor predito de \\(y\\), \\(\\beta_0\\) e \\(\\beta_1\\) são coeficientes chamados de intercepto ou constante e coeficiente linear ou inclinação, respectivamente. Enquanto \\(\\beta_0\\) mede o valor da resposta prevista para \\(x=0\\), \\(\\beta_1\\) consiste na mudança média da resposta para o incremento de uma unidade de \\(x\\).\n\\[\n\\hat{y} = \\beta_0 + \\beta_1x\n\\]\nA Figura 3.2 plota para os dados plotados anteriormente a linha azul do modelo de regressão linear simples obtido.\nFigura 3.2: Modelo de regressão linear simples\nPara este caso inicial os coeficientes de regressão estimados são:\n(Intercept)           x \n  10.927762    2.728817\nA primeira pergunta a ser feita seria como estimar tais coeficientes de regressão. Pode-se pensar em estimativas que minimizem o erro de previsão. Conforme, plotado na Figura 3.3 em linhas verticais vermelhas, o erro de previsão seria a diferença entre o valor experimental e o previsto, \\(\\varepsilon_i = y_i - \\hat{y}_i\\), \\(i = 1, ...., n\\).\nFigura 3.3: Erros do modelo de regressão linear simples\nNeste sentido, as observações da variável dependente ou resposta podem ser descritas conforme segue.\n\\[\n\\begin{aligned}\ny_i = \\hat{y}_i + \\varepsilon_i \\\\\ny_i = \\beta_0 + \\beta_1x_i + \\varepsilon_i \\\\\n\\end{aligned}\n\\]\nTomando \\(n\\) observações retiradas da população de interesse, \\((x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\\), pode-se pensar em um modelo que minimize os erros de previsão para a amostra disponível. Uma vez que o erro é normalmente distribuído, com média nula e variância \\(\\sigma_\\varepsilon^2\\), \\(\\varepsilon \\sim N(0,\\sigma_\\varepsilon^2)\\), pode-se trabalhar a minimização da soma dos quadrados dos erros de previsão, \\(\\sum_{i=1}^{n}\\varepsilon_i^2\\).\nA Tabela 3.1 expõe algumas das observações para os dados plotados anteriormente, bem como os valores preditos e erros associados.\nTabela 3.1: Dados para regressão linear simples e previsões\n\n\n\n\n\n\nx\ny\ny_hat\nerro\n\n\n\n\n1.093333\n9.389117\n13.91127\n-4.5221504\n\n\n4.106335\n29.785746\n22.13320\n7.6525477\n\n\n9.138367\n29.774229\n35.86469\n-6.0904613\n\n\n24.613730\n78.467084\n78.09412\n0.3729679\n\n\n38.963445\n126.273606\n117.25185\n9.0217521\n\n\n56.628047\n155.103936\n165.45531\n-10.3513755\nA sintaxe anteriormente apresentada pode ser escrita de forma matricial, conforme segue, onde \\(\\mathbf{x}_{[2 \\times n]}\\) consiste em uma matriz relacionada às observações independentes, com uma coluna de valores unitários associada à \\(\\beta_0\\) e outra com as observações de \\(x\\), portanto associada a \\(\\beta_1\\). \\(\\mathbf{y}_{[n\\times1]}\\) consiste no vetor de observações da resposta, \\(\\mathbf{\\varepsilon}_{[n\\times1]}\\) consiste no vetor de erros ou resíduos de previsão e \\(\\mathbf{\\beta}_{[2\\times1]}\\) consiste em um vetor de coeficientes.\n\\[\n\\begin{aligned}\n\\mathbf{y} = \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\varepsilon}\n\\end{aligned}\n\\]\nTais elementos matriciais podem ser escritos de forma genérica conforme segue.\n\\[\n\\mathbf{X} =\n\\begin{bmatrix}\n1 & x_{1}\\\\\n1 & x_{2}\\\\\n\\vdots & \\vdots \\\\\n1 & x_{n}\\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 1.093\\\\\n1 & 4.106\\\\\n\\vdots & \\vdots \\\\\n1 & 96.206\\\\\n\\end{bmatrix}\n\\]\n\\[\n\\mathbf{y} =\n\\begin{bmatrix}\ny_{1}\\\\\ny_{2}\\\\\n\\vdots \\\\\ny_{n}\\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n9.389\\\\\n29.786\\\\\n\\vdots \\\\\n290.028\\\\\n\\end{bmatrix}\n\\]\n\\[\n\\mathbf{\\varepsilon} =\n\\begin{bmatrix}\n\\varepsilon_{1}\\\\\n\\varepsilon_{2}\\\\\n\\vdots \\\\\n\\varepsilon_{n}\\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n-4.522\\\\\n7.653\\\\\n\\vdots \\\\\n16.571\\\\\n\\end{bmatrix}\\text{,    e} \\\\\n\\]\n\\[\n\\mathbf{\\beta}^T =\n\\begin{bmatrix}\n\\beta_0 & \\beta_1\\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n10.928  & 2.729 \\\\\n\\end{bmatrix}\n\\]\nTomando tal notação, a soma dos quadrados dos erros pode ser descrita como \\(\\sum_{i=1}^{N}\\varepsilon_i^2 = \\mathbf{\\varepsilon}^T\\mathbf{\\varepsilon}\\). Desenvolvendo tal expressão tem-se:\n\\[\n\\begin{aligned}\nL(\\mathbf{\\beta}) = \\mathbf{\\varepsilon}^T\\mathbf{\\varepsilon} = (\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta}) \\\\\n\\mathbf{y}^T\\mathbf{y} - 2\\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{\\beta}\n\\end{aligned}\n\\]\nPara minimizar \\(L\\) em relação à estimativa de \\(\\mathbf{\\beta}\\), pode-se diferenciar tal quantidade em relação à \\(\\mathbf{\\beta}\\) e igualar a zero:\n\\[\n\\begin{aligned}\n\\frac{\\partial L}{\\partial \\mathbf{\\beta}} = -2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\mathbf{\\beta} = 0 \\\\\n\\hat{\\mathbf{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}(\\mathbf{X}^T\\mathbf{y})\n\\end{aligned}\n\\]\nTal solução constitui as chamadas equações normais de mínimos quadrados.\nAo obter um modelo de regressão é sempre importante observar os resíduos, os quais devem ser normalmente distribuídos, independentes e homocedásticos. Tais pressuposições implicam que o modelo obtido por mínimos quadrados é paramétrico, uma vez que pressupõe-se uma distribuição para os resíduos. Neste curso de aprendizado não supervisionado, serão estudados diversos modelos que não implicam qualquer distribuição acerca dos resíduos ou dados sendo, portanto, livres de distribuição e ditos não-paramétricos. A Figura 3.4 expõe alguns gráficos dos resíduos os erros do modelo obtido, os quais podem servir para avaliação da normalidade do modelo, bem como para identificação de observações não usuais.\nFigura 3.4: Resíduos do modelo de regressão linear simples",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regressão linear simples, múltipla e polinomial</span>"
    ]
  },
  {
    "objectID": "AS_reg_ols.html#regressão-linear-múltipla",
    "href": "AS_reg_ols.html#regressão-linear-múltipla",
    "title": "3  Regressão linear simples, múltipla e polinomial",
    "section": "3.2 Regressão linear múltipla",
    "text": "3.2 Regressão linear múltipla\nNo caso de onde há múltiplas variáveis independentes ou regressoras de interesse, \\(x_1, x_2, ..., x_k\\) pode-se considerar o modelo com um coeficiente linear associado a cada variável, isto é:\n\\[\n\\hat{y}_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\cdots + \\beta_kx_{ik} = \\beta_0 + \\sum_{j=1}^{k}\\beta_jx_{ij},  \n\\]\nou de forma matricial com \\(\\mathbf{X}_{[n\\times (k+1)]}\\) e \\(\\mathbf{\\beta}_{[(k+1) \\times 1]}\\):\n\\[\n\\begin{aligned}\n\\hat{\\mathbf{y}} = \\mathbf{X}\\mathbf{\\beta}\n\\end{aligned},\n\\] com:\n\\[\n\\mathbf{X} =\n\\begin{bmatrix}\n1 & x_{11} & x_{12} & \\cdots & x_{1k}\\\\\n1 & x_{21} & x_{22} & \\cdots & x_{2k} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n1 & x_{n1} & x_{n2} & \\cdots & x_{nk} \\\\\n\\end{bmatrix}, e\\\\\n\\]\n\\[\n\\mathbf{\\beta}^T =\n\\begin{bmatrix}\n\\beta_0 & \\beta_1 & \\cdots & \\beta_k\\\\\n\\end{bmatrix}. \\\\\n\\]\nAs estimativas de mínimos quadrados, deduzidas para o caso simples, \\(\\hat{\\mathbf{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}(\\mathbf{X}^T\\mathbf{y})\\), também atendem ao caso múltiplo. Uma forma de medir o ajuste do modelo obtido aos dados seria a partir do cálculo do coeficiente de determinação múltipla, \\(R^2\\), conforme segue,\n\\[\n\\begin{align}\nR^2 = 1- SS_{E}/SS_T \\\\\nR^2 = 1- \\frac{\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i-\\overline{y})^2},\n\\end{align}\n\\]\nou utilizando outras métricas de ajuste. É interessante que tais métricas sejam também calculadas para dados futuros ou de teste, de forma a evitar sobreajuste do modelo. Uma observação importante é relacionada à utilização da Análise de variância (ANOVA) para obtenção de tais métricas. Além do \\(R^2\\), ao se utilizar a ANOVA, há a possibilidade de calcular o coeficiente de determinação ajustado, \\(R^2_{adj}\\), isto é:\n\\[\nR^2_{adj} = 1 - \\frac{SS_{E}/(n-k)}{SS_T/(n-1)}\n\\]\nEsta métrica é mais honesta uma vez que penaliza o modelo pela adição de mais coeficientes. O \\(R^2\\) sempre aumentará com adição de novos coeficientes, enquanto o \\(R^2_{adj}\\) será mais baixo casos novos termos adicionados não apresentem significância estatística. Entretanto, quando outros métodos de aprendizado não paramétricos são utilizados, especialmente os que não tem origem na estatística mas na computação, tal métrica não pode ser calculada. No contexto de aprendizado supervisionado é mais interessante realizar a validação cruzada e estimar o desempenho do modelo em dados futuros, viabilizando a comparação de tipos distintos de modelos.\nO teste t para os coeficientes de regressão pode ser calculado para medir a significância de cada coeficiente e testar as seguintes hipóteses para cada coeficiente de regressão.\n\\[\n\\begin{matrix}\nH_0: \\beta_j = 0 \\\\\nH_1: \\beta_j \\neq 0, j = 1, ..., k\\\\\n\\end{matrix}\n\\]\nO teste é calculado conforme segue, onde \\(C_j\\) é o valor da \\(j\\)-ésima linha e \\(j\\)-ésima coluna da matriz \\((\\mathbf{X}^T\\mathbf{X})^{-1}\\), correspondente a \\(\\beta_j\\). A hipótese nula é rejeitada se \\(t_{0j} &gt; t_{[\\alpha/2,N-k]}\\), onde \\(\\alpha\\) é o nível de significância de interesse.\n\\[\nt_{0j} = \\frac{\\hat{\\beta}_j}{\\sqrt{C_jSS_E/(N-k)}}\n\\]\nAnalogamente, estimativas intervalares para os coeficientes podem ser obtidas como segue, as quais consistem em intervalos que garantem \\(\\gamma = 1-\\alpha\\) de confiança de encontrar os verdadeiros valores dos coeficientes de regressão.\n\\[\n\\hat{\\beta}_j \\pm t_{[\\alpha/2,N-k]}\\sqrt{C_jSS_E/(N-k)}\n\\]\nA Figura 3.5 ilustra graficamente um modelo de regressão linear múltipla para prever o preço de carros usados em função da idade e quilometragem.\n\n\n\n\n\n\n\n\nFigura 3.5: Modelo de regressão linear múltipla para prever o preço de carros usados em função da quilometragem e idade\n\n\n\n\n\nPara este caso, os coeficientes estimados são apresentados a seguir. Estes foram estimados nas unidades originais das variáveis. Porém, para fins de inferência, é importante padronizar as variáveis regressoras.\n\n\n(Intercept)     Mileage         Age \n21.54307990 -0.05314626 -0.84234949 \n\n\nO modelo pode ser escrito conforme segue.\n\\[\n\\hat{y} = 21,543 - 0,0531x_1 - 0,842x_2\n\\]\nO teste t para os coeficientes de regressão resultante é apresentado a seguir. As duas variáveis regressoras foram antes padronizadas para evitar efeito de escala e unidade de medida. Como \\(|t_{\\alpha/2,N-k}|\\) = 1.987, tem-se que todos os coeficientes são significativos, uma vez que \\(t_{0j} &gt; t_{\\alpha/2,N-k}\\), \\(\\forall j = 1, ..., k\\). Pode-se também considerar o \\(p-value\\) que é a probabilidade de erro na rejeição da hipótese nula, \\(H_0\\), associada ao valor calculado \\(t_0\\). Se \\(p-value &lt; \\alpha\\), rejeita-se \\(H_0\\).\n\n\n\nCall:\nlm(formula = Price ~ Mileage + Age, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.5429 -1.2795 -0.2982  1.5275  7.1967 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  13.7478     0.2483  55.364  &lt; 2e-16 ***\nMileage      -1.9401     0.4406  -4.404 3.02e-05 ***\nAge          -3.3175     0.4406  -7.530 4.42e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.356 on 87 degrees of freedom\nMultiple R-squared:  0.8239,    Adjusted R-squared:  0.8198 \nF-statistic: 203.5 on 2 and 87 DF,  p-value: &lt; 2.2e-16\n\n\nA Tabela 3.2 apresentam os intervalos de confiança de 0,95 para os coeficientes codificados.\n\n\n\n\nTabela 3.2: Intervalos de confiança para os coeficientes de regressão múltipla\n\n\n\n\n\n\n\nX2.5..\nX97.5..\n\n\n\n\n(Intercept)\n13.254224\n14.241331\n\n\nMileage\n-2.815845\n-1.064452\n\n\nAge\n-4.193231\n-2.441839\n\n\n\n\n\n\n\n\nAlém de considerar termos de múltiplas variáveis, é possível considerar a interação entre estas, colocando na matriz \\(\\mathbf{X}\\) colunas com multiplicação ou produto das variáveis de interesse e no modelo termos da forma \\(\\beta_{ij}x_ix_j\\). Para o caso em estudo um modelo de regressão múltipla com interação ficaria conforme segue.\n\n\n\nCall:\nlm(formula = Price ~ Mileage * Age, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.1277 -1.4495 -0.2671  1.7765  6.1130 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  13.0272     0.3381  38.532  &lt; 2e-16 ***\nMileage      -2.0909     0.4247  -4.924 4.06e-06 ***\nAge          -3.7683     0.4477  -8.417 7.55e-13 ***\nMileage:Age   0.8844     0.2952   2.997  0.00357 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.255 on 86 degrees of freedom\nMultiple R-squared:  0.8405,    Adjusted R-squared:  0.835 \nF-statistic: 151.1 on 3 and 86 DF,  p-value: &lt; 2.2e-16\n\n\nÉ importante esclarecer que no caso de modelos de regressão múltipla com termos de interação, a matriz \\(\\mathbf{X}\\) apresentará mais que \\(k+1\\) colunas, sendo adicionadas as colunas das respectivas interações, como sendo o produto das colunas das variáveis envolvidas em cada termo adicionado.\n\n3.2.1 Codificação de variáveis categóricas em regressão múltipla\nRetomando o problema de regressão do preço de revenda de carros considerando o ano e a quilometragem, imagine uma terceira variável regressora que determina o modelo ou tipo do veículo. Tal variável apresenta três categorias, Mazda6, Accord e Maxima. A Tabela 3.3 expõe algumas observações do conjuunto de dados.\n\n\n\n\nTabela 3.3: Dados de preços de carros usados\n\n\n\n\n\n\n\nCarType\nAge\nMileage\nPrice\n\n\n\n\n1\nMazda6\n3\n17.8\n15.9\n\n\n2\nMazda6\n2\n19.0\n16.4\n\n\n50\nAccord\n10\n150.5\n7.9\n\n\n51\nAccord\n5\n65.2\n11.7\n\n\n70\nMaxima\n1\n38.6\n20.0\n\n\n71\nMaxima\n1\n42.1\n20.0\n\n\n\n\n\n\n\n\nPara trabalhar com a variável modelo e qualquer outra variável qualitativa ou categórica em regressão múltipla, pode-se utilizar de variáveis dummy também conhecidas como dicotômicas ou binárias. No caso de três categorias, como no exemplo acima, duas variáveis dummy seriam suficientes. Ao criar uma coluna denominada Mazda6, com 1, se Mazda6 e 0, caso contrário e, de forma análoga, uma coluna para Accord, caso uma determinada observação receba 0 em ambas colunas, o modelo do carro seria o Maxima.\nSeja \\(x_1\\) o ano, \\(x_2\\) a quiometragem,\n\\[\nx_3 = \\bigg\\{\n\\begin{matrix}\n1, \\text{ se Mazda6} \\\\\n0, \\text{ cc}\\\\\n\\end{matrix}\n\\] e\n\\[\nx_4 = \\bigg\\{\n\\begin{matrix}\n1, \\text{ se Accord} \\\\\n0, \\text{ cc}\\\\\n\\end{matrix}\n\\]\nO modelo de regressão pode ser escrito conforme segue.\n\\[\n\\hat{y}_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\beta_3x_{i3} + \\beta_4x_{i4}\n\\]\nFica claro que, neste tipo de modelo, o coeficiente \\(\\beta_3\\) é uma constante adicionada a \\(\\beta_0\\) caso o modelo do carro seja Mazda6. Uma explicação análoga pode ser feita para \\(\\beta_4\\). Portanto, tais termos não mudam a inclinação do modelo, apenas o intercepto. É possível adicionar termos de interação entre variáveis dicotômicas e variáveis contínuas, o que na prática serviria para mudar a inclinação ou coeficientes das variáveis contínuas. Retomando a criação de variáveis dicotômicas para o exemplo em questão, tem-se as colunas com tais variáveis criadas na Tabela 3.4.\n\n\n\n\nTabela 3.4: Dados de carros usados com colunas das variáveis dicotômicas\n\n\n\n\n\n\n\nCarType\nAge\nPrice\nMileage\nMazda6\nAccord\nMaxima\n\n\n\n\n1\nMazda6\n3\n15.9\n17.8\n1\n0\n0\n\n\n2\nMazda6\n2\n16.4\n19.0\n1\n0\n0\n\n\n50\nAccord\n10\n7.9\n150.5\n0\n1\n0\n\n\n51\nAccord\n5\n11.7\n65.2\n0\n1\n0\n\n\n70\nMaxima\n1\n20.0\n38.6\n0\n0\n1\n\n\n71\nMaxima\n1\n20.0\n42.1\n0\n0\n1\n\n\n\n\n\n\n\n\nO modelo com estas variáveis e as duas consideradas anteriormente ficaria conforme exposto a seguir.\n\n\n\nCall:\nlm(formula = Price ~ Mileage + Age + Mazda6 + Accord, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.588 -1.632 -0.178  1.196  6.861 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  14.5616     0.4082  35.672  &lt; 2e-16 ***\nMileage      -1.8975     0.4226  -4.490 2.23e-05 ***\nAge          -3.2185     0.4253  -7.567 4.17e-11 ***\nMazda6       -2.1158     0.5750  -3.680 0.000409 ***\nAccord       -0.3257     0.5873  -0.555 0.580651    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.187 on 85 degrees of freedom\nMultiple R-squared:  0.8517,    Adjusted R-squared:  0.8447 \nF-statistic:   122 on 4 and 85 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regressão linear simples, múltipla e polinomial</span>"
    ]
  },
  {
    "objectID": "AS_reg_ols.html#regressão-polinomial",
    "href": "AS_reg_ols.html#regressão-polinomial",
    "title": "3  Regressão linear simples, múltipla e polinomial",
    "section": "3.3 Regressão polinomial",
    "text": "3.3 Regressão polinomial\nÉ possível em regressão simples ou múltipla realizar transformações nos preditores de forma a incluir termos polinomiais associados à uma ou mais variáveis independentes. Para o caso simples, um modelo de regressão polinomial pode ser escrito conforme segue, onde \\(p\\) é a ordem do modelo de regressão polinomial.\n\\[\n\\hat{y} = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\beta_3x^3 + ... \\beta_px^p\n\\]\nConsiderando a notação matricial, a matriz \\(\\mathbf{X}\\) fica conforme segue, podendo-se utilizar novamente as equações normais de mínimos quadrados para estimar os coeficientes, \\(\\hat{\\mathbf{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}(\\mathbf{X}^T\\mathbf{y})\\).\n\\[\n\\mathbf{X} =\n\\begin{bmatrix}\n1 & x_{1} & x_{1}^2 & \\cdots & x_{1}^p\\\\\n1 & x_{2} & x_{2}^2 & \\cdots & x_{2}^p \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n1 & x_{n} & x_{n}^2 & \\cdots & x_{n}^p \\\\\n\\end{bmatrix}\\\\\n\\]\nSejam os dados da massa de um paciente em kg medidos ao longo de 8 meses de um programa de perda de peso, conforme Figura 3.6.\n\n\n\n\n\n\n\n\nFigura 3.6: Dados de perda de peso em função do tempo no programa\n\n\n\n\n\nConsiderando um modelo linear para tais dados, os resíduos obtidos são plotados na Figura 3.7. Pode-se observar claramente um padrão de não linearidade nos resíduos em relação aos valores ajustados, indicando o ajuste de um modelo não linear.\n\n\n\n\n\n\n\n\nFigura 3.7\n\n\n\n\n\nPode-se pensar, portanto, em um modelo de regressão quadrático para aproximar o peso em função de dias. A curva plotada na Figura 3.6 consiste em tal modelo.\n\n\n\n\n\n\n\n\nFigura 3.8\n\n\n\n\n\nO modelo quadrático obtido e associado ao gráfico anterior é exposto a seguir.\n\n\n\nCall:\nlm(formula = Weight ~ Days + I(Days^2), data = rehab)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9034 -0.5842 -0.1188  0.4774  2.6315 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.833e+02  3.521e-01  520.72   &lt;2e-16 ***\nDays        -4.565e-01  6.520e-03  -70.03   &lt;2e-16 ***\nI(Days^2)    6.930e-04  2.614e-05   26.51   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9386 on 49 degrees of freedom\nMultiple R-squared:  0.9981,    Adjusted R-squared:  0.998 \nF-statistic: 1.287e+04 on 2 and 49 DF,  p-value: &lt; 2.2e-16\n\n\nOs resíduos para o modelo quadrático são plotados na Figura 3.7.\n\n\n\n\n\n\n\n\nFigura 3.9\n\n\n\n\n\nFinalmente, tal modelo pode ser escrito conforme segue.\n\\[\n\\hat{y} = 183,3 -0,456x + 6,930\\times10^{-4}x^2\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regressão linear simples, múltipla e polinomial</span>"
    ]
  },
  {
    "objectID": "AS_reg_ols.html#implementações-em-r",
    "href": "AS_reg_ols.html#implementações-em-r",
    "title": "3  Regressão linear simples, múltipla e polinomial",
    "section": "3.4 Implementações em R",
    "text": "3.4 Implementações em R\nA seguir será expostos as implementações necessárias para obter os resultados do capítulo. Alguns exemplos aqui expostos são distintos dos do capítulo, sendo os resultados das implementações apresentados integralmente.\n\n3.4.1 Caso 1 - regressão linear simples\nUm exemplo de regressão linear simples já foi proposto no primeiro capítulo do livro.\n\n\n3.4.2 Regressão linear múltipla\nCarregando pacote para dados e gráfico.\n\nlibrary(caTools)\nlibrary(GGally)\n\nA ?fig-dataecn expõe algumas das observações dados macroeconômicos para regressão linear múltipla.\n\ndata(longley)\n# ?longley\n# dim(longley)\nhead(longley) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGNP.deflator\nGNP\nUnemployed\nArmed.Forces\nPopulation\nYear\nEmployed\n\n\n\n\n1947\n83.0\n234.289\n235.6\n159.0\n107.608\n1947\n60.323\n\n\n1948\n88.5\n259.426\n232.5\n145.6\n108.632\n1948\n61.122\n\n\n1949\n88.2\n258.054\n368.2\n161.6\n109.773\n1949\n60.171\n\n\n1950\n89.5\n284.599\n335.1\n165.0\n110.929\n1950\n61.187\n\n\n1951\n96.2\n328.975\n209.9\n309.9\n112.075\n1951\n63.221\n\n\n1952\n98.1\n346.999\n193.2\n359.4\n113.270\n1952\n63.639\n\n\n\n\n\nNa Figura 3.10 visualiza-se a correlação entre as variáveis presentes no conjunto de dados.\n\nggpairs(longley) + theme_bw()\n\n\n\n\n\n\n\nFigura 3.10\n\n\n\n\n\nPadronizando os dados para evitar efeitos de escala e unidades de medida.\n\nlongley_scaled &lt;- data.frame(scale(longley))\n\nSeprando dados de treino e teste.\n\nset.seed(45)\ntr &lt;- round(0.8*nrow(longley_scaled),0)\ntreino &lt;- sample(nrow(longley_scaled), tr, replace = F)\nlongley.tr &lt;- longley_scaled[treino,]\nlongley.te &lt;- longley_scaled[-treino,]\n\nRegressão múltipla para o número de pessoas empregadas em função das outras variáveis.\n\nlm_mult &lt;- lm(Employed ~., longley.tr)\nsummary(lm_mult)\n\n\nCall:\nlm(formula = Employed ~ ., data = longley.tr)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.116622 -0.033371  0.002484  0.031364  0.121138 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  -0.006058   0.025665  -0.236  0.82123   \nGNP.deflator  0.154006   0.487336   0.316  0.76269   \nGNP          -1.252420   1.218841  -1.028  0.34379   \nUnemployed   -0.594009   0.165152  -3.597  0.01141 * \nArmed.Forces -0.233556   0.061149  -3.819  0.00877 **\nPopulation   -0.092484   0.677281  -0.137  0.89585   \nYear          2.629894   0.676141   3.890  0.00808 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09174 on 6 degrees of freedom\nMultiple R-squared:  0.9959,    Adjusted R-squared:  0.9917 \nF-statistic: 241.1 on 6 and 6 DF,  p-value: 7.003e-07\n\n\nObtendo os coeficientes com as equações normais de mínimos quadrados passo a passo.\n\nX &lt;- model.matrix(~., data =longley.tr[,-7])\nX\n\n     (Intercept) GNP.deflator        GNP Unemployed Armed.Forces Population\n1951           1   -0.5079204 -0.5908091 -1.1710587   0.70742726 -0.7689652\n1957           1    0.6225934  0.5540580 -0.2753583   0.27490604  0.4342950\n1960           1    1.1600508  1.1560203  0.7894229  -0.13318708  1.1420190\n1953           1   -0.2484582 -0.2244927 -1.4161189   1.35117978 -0.3349577\n1956           1    0.2704662  0.3167321 -0.3973534   0.35968594  0.1883239\n1949           1   -1.2492409 -1.3043364  0.5229601  -1.42356602 -1.0998977\n1959           1    1.0117867  0.9558390  0.6631474  -0.07858307  0.8542141\n1952           1   -0.3318568 -0.4094719 -1.3497707   1.41871632 -0.5971736\n1950           1   -1.1287763 -1.0372705  0.1687464  -1.37470980 -0.9337126\n1954           1   -0.1557931 -0.2473611  0.4116664   1.06810111 -0.1732292\n1958           1    0.8449896  0.5719362  1.5920219   0.04355747  0.6506518\n1947           1   -1.7310992 -1.5434331 -0.8960348  -1.46092666 -1.4111352\n1962           1    1.4102465  1.6821336  0.8707530   0.31657752  1.8195537\n           Year\n1951 -0.7351470\n1957  0.5251050\n1960  1.1552311\n1953 -0.3150630\n1956  0.3150630\n1949 -1.1552311\n1959  0.9451891\n1952 -0.5251050\n1950 -0.9451891\n1954 -0.1050210\n1958  0.7351470\n1947 -1.5753151\n1962  1.5753151\nattr(,\"assign\")\n[1] 0 1 2 3 4 5 6\n\n\n\ny &lt;- longley.tr$Employed\nbeta_mat &lt;- solve(t(X)%*%X)%*%t(X)%*%y\nbeta_mat\n\n                     [,1]\n(Intercept)  -0.006058494\nGNP.deflator  0.154006386\nGNP          -1.252419887\nUnemployed   -0.594008870\nArmed.Forces -0.233556488\nPopulation   -0.092484035\nYear          2.629893534\n\n\nExistem algoritmos de seleção de variáveis que permitem a melhora do ajuste do modelo considerando a remoção de coeficientes não significativos. Um deles é a eliminação para trás. O critério de informação de Akaike, o qual leva em conta o erro dos modelos e a complexidade, é usado para selecionar os modelos.\n\nlm_mult_red &lt;- step(lm_mult, direction = \"back\")\n\nStart:  AIC=-58.16\nEmployed ~ GNP.deflator + GNP + Unemployed + Armed.Forces + Population + \n    Year\n\n               Df Sum of Sq      RSS     AIC\n- Population    1  0.000157 0.050653 -60.120\n- GNP.deflator  1  0.000840 0.051337 -59.946\n&lt;none&gt;                      0.050496 -58.160\n- GNP           1  0.008886 0.059382 -58.053\n- Unemployed    1  0.108874 0.159370 -45.219\n- Armed.Forces  1  0.122775 0.173271 -44.132\n- Year          1  0.127324 0.177820 -43.795\n\nStep:  AIC=-60.12\nEmployed ~ GNP.deflator + GNP + Unemployed + Armed.Forces + Year\n\n               Df Sum of Sq     RSS     AIC\n- GNP.deflator  1   0.00533 0.05598 -60.820\n&lt;none&gt;                      0.05065 -60.120\n- GNP           1   0.04735 0.09801 -53.540\n- Year          1   0.13005 0.18070 -45.586\n- Armed.Forces  1   0.16981 0.22046 -43.001\n- Unemployed    1   0.32078 0.37143 -36.219\n\nStep:  AIC=-60.82\nEmployed ~ GNP + Unemployed + Armed.Forces + Year\n\n               Df Sum of Sq     RSS     AIC\n&lt;none&gt;                      0.05598 -60.820\n- GNP           1  0.044043 0.10002 -55.275\n- Year          1  0.154642 0.21062 -45.594\n- Armed.Forces  1  0.188671 0.24465 -43.647\n- Unemployed    1  0.315797 0.37178 -38.207\n\nsummary(lm_mult_red)\n\n\nCall:\nlm(formula = Employed ~ GNP + Unemployed + Armed.Forces + Year, \n    data = longley.tr)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.124653 -0.039041 -0.002178  0.039448  0.114711 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.006174   0.023368  -0.264  0.79830    \nGNP          -1.336743   0.532824  -2.509  0.03644 *  \nUnemployed   -0.604450   0.089976  -6.718  0.00015 ***\nArmed.Forces -0.216546   0.041703  -5.193  0.00083 ***\nYear          2.779766   0.591313   4.701  0.00154 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08365 on 8 degrees of freedom\nMultiple R-squared:  0.9954,    Adjusted R-squared:  0.9931 \nF-statistic: 434.8 on 4 and 8 DF,  p-value: 2.19e-09\n\n\nAvaliando o modelo considerando os dados de treino.\n\nmetrics &lt;- function(obs, pred) {\n  \n  RSE &lt;- sum((obs - pred)^2)\n  SST &lt;- sum((obs - mean(obs))^2)\n  R2 &lt;- 1 - RSE/SST \n  \n  MAE &lt;-  mean(abs(obs - pred))\n  \n  RMSE &lt;- sqrt(mean((obs - pred)^2))\n  \n  return(\n    data.frame(RMSE = RMSE,\n               MAE = MAE,\n               R2 = R2))\n}\n\n\nmetrics(longley.tr$Employed, lm_mult_red$fitted.values)\n\n        RMSE        MAE        R2\n1 0.06562145 0.05009978 0.9954209\n\n\nAvaliando os resíduos do modelo.\n\nshapiro.test(lm_mult_red$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  lm_mult_red$residuals\nW = 0.96685, p-value = 0.854\n\n\nA Figura 3.11 expõe os gráficos de resíduos do modelo.\n\npar(mfrow=c(2,2))\nplot(lm_mult_red)\n\n\n\n\n\n\n\nFigura 3.11\n\n\n\n\n\nAvaliando o modelo com dados de teste.\n\npred_mult.te &lt;- predict(lm_mult_red, newdata=longley.te)\nmetrics(longley.te$Employed, pred_mult.te)\n\n        RMSE        MAE        R2\n1 0.08438488 0.06111778 0.9922761\n\n\n\n\n3.4.3 Regressão múltipla com variáveis categóricas\nPacote para dados e para codificação de variáveis dummy ou dicotômicas.\n\nlibrary(AER)\nlibrary(fastDummies)\nlibrary(dplyr)\n\nCarregando conjunto de dados com variáveis contínuas e uma categórica.\n\ndata(Grunfeld)\n# ?Grunfeld\nhead(Grunfeld)\n\n  invest  value capital           firm year\n1  317.6 3078.5     2.8 General Motors 1935\n2  391.8 4661.7    52.6 General Motors 1936\n3  410.6 5387.1   156.9 General Motors 1937\n4  257.7 2792.2   209.2 General Motors 1938\n5  330.8 4313.2   203.4 General Motors 1939\n6  461.2 4643.9   207.2 General Motors 1940\n\n\nA Figura 3.12 expõe gráficos de densidade, correlação aos pares e coeficientes de correlação de Pearson para cada nível da variável categórica firm (firma). Foram consideradas apenas três empresas para não dificultar a interpretação. Consegue fazer para as outras empresas?\n\nggpairs(Grunfeld[1:60,], columns = 1:4, aes(color = firm, alpha = 0.5)) + theme_bw()\n\n\n\n\n\n\n\nFigura 3.12\n\n\n\n\n\n\nlevels(Grunfeld$firm)\n\n [1] \"General Motors\"    \"US Steel\"          \"General Electric\" \n [4] \"Chrysler\"          \"Atlantic Refining\" \"IBM\"              \n [7] \"Union Oil\"         \"Westinghouse\"      \"Goodyear\"         \n[10] \"Diamond Match\"     \"American Steel\"   \n\n\nCodificando a variável firm em variáveis binárias.\n\nGrunfeld2 &lt;- dummy_cols(Grunfeld, select_columns = c(\"firm\"))\n\nGrunfeld2 &lt;- Grunfeld2[,-c(4,16)] # removendo coluna da variável \"firm\" e coluna da última variável binária criada, a qual é desnecessária\n\nhead(Grunfeld)\n\n  invest  value capital           firm year\n1  317.6 3078.5     2.8 General Motors 1935\n2  391.8 4661.7    52.6 General Motors 1936\n3  410.6 5387.1   156.9 General Motors 1937\n4  257.7 2792.2   209.2 General Motors 1938\n5  330.8 4313.2   203.4 General Motors 1939\n6  461.2 4643.9   207.2 General Motors 1940\n\n\nPadronizando variáveis regressoras contínuas.\n\nGrunfeld2[,2:4] &lt;- scale(Grunfeld2[,2:4])\n\nDividindo dados de treino e teste.\n\nset.seed(87)\ntr &lt;- round(0.8*nrow(Grunfeld2),0)\ntreino &lt;- sample(nrow(Grunfeld2), tr, replace = F)\nGrunfeld.tr &lt;- Grunfeld2[treino,]\nGrunfeld.te &lt;- Grunfeld2[-treino,]\n\nModelo de regressão linear múltipla com variáveis contínuas e categóricas.\n\nlm_invest &lt;- lm(invest ~ .-year, Grunfeld.tr)\nsummary(lm_invest)\n\n\nCall:\nlm(formula = invest ~ . - year, data = Grunfeld.tr)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-177.977  -12.831   -0.035   13.412  197.819 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               158.512     14.627  10.837  &lt; 2e-16 ***\nvalue                     141.105     14.368   9.821  &lt; 2e-16 ***\ncapital                    76.787      5.533  13.877  &lt; 2e-16 ***\n`firm_General Motors`     -25.829     47.263  -0.546  0.58547    \n`firm_US Steel`           125.052     26.010   4.808 3.44e-06 ***\n`firm_General Electric`  -202.654     25.570  -7.926 3.38e-13 ***\nfirm_Chrysler              -7.712     17.307  -0.446  0.65646    \n`firm_Atlantic Refining`  -76.358     17.943  -4.256 3.51e-05 ***\nfirm_IBM                   -3.460     16.713  -0.207  0.83625    \n`firm_Union Oil`          -29.516     16.107  -1.833  0.06869 .  \nfirm_Westinghouse         -34.198     17.161  -1.993  0.04795 *  \nfirm_Goodyear             -56.218     17.078  -3.292  0.00122 ** \n`firm_Diamond Match`       10.857     15.651   0.694  0.48883    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 46.73 on 163 degrees of freedom\nMultiple R-squared:  0.9443,    Adjusted R-squared:  0.9402 \nF-statistic: 230.5 on 12 and 163 DF,  p-value: &lt; 2.2e-16\n\n\nOutra forma de inclusão de variáveis dummy em modelos de regressão múltipla é na interação. A diferença é que no caso anterior as variáveis dummy mudam apenas a constante, enquanto neste caso, vão mudar a inclinação.\nSupondo o caso da empresa General Electric que no gráfico pairs aparenta ter inclinação distinta das demais.\n\nlm_invest2 &lt;- lm(invest ~ . + I(capital*`firm_General Electric`), Grunfeld.tr)\nsummary(lm_invest2)\n\n\nCall:\nlm(formula = invest ~ . + I(capital * `firm_General Electric`), \n    data = Grunfeld.tr)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-179.22  -12.00    1.06   12.39  184.42 \n\nCoefficients:\n                                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                           163.389     14.679  11.131  &lt; 2e-16 ***\nvalue                                 137.675     14.076   9.781  &lt; 2e-16 ***\ncapital                                87.834      7.203  12.194  &lt; 2e-16 ***\nyear                                   -4.245      4.641  -0.915 0.361718    \n`firm_General Motors`                 -34.125     47.124  -0.724 0.470018    \n`firm_US Steel`                       121.936     25.838   4.719 5.11e-06 ***\n`firm_General Electric`              -193.220     26.307  -7.345 9.71e-12 ***\nfirm_Chrysler                          -8.519     16.928  -0.503 0.615497    \n`firm_Atlantic Refining`              -92.113     18.817  -4.895 2.37e-06 ***\nfirm_IBM                               -3.521     16.284  -0.216 0.829092    \n`firm_Union Oil`                      -38.768     16.397  -2.364 0.019253 *  \nfirm_Westinghouse                     -33.498     16.749  -2.000 0.047185 *  \nfirm_Goodyear                         -64.341     17.112  -3.760 0.000237 ***\n`firm_Diamond Match`                   12.949     15.266   0.848 0.397568    \nI(capital * `firm_General Electric`)  -44.741     14.660  -3.052 0.002662 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 45.53 on 161 degrees of freedom\nMultiple R-squared:  0.9478,    Adjusted R-squared:  0.9433 \nF-statistic: 208.9 on 14 and 161 DF,  p-value: &lt; 2.2e-16\n\n\nA Figura 3.13 a seguir ilustra a mudança da inclinação para a empresa GE em relação a outras três selecionadas arbitrariamente. Em um caso com tantos níveis de variáveis dummy como este é difícil explorar todas possibilidades. Uma sugestão seria fazer um modelo com todas interações possíveis e posteriormente usar a função step para realizar eliminação para trás (backward elimination) para remover os termos não significativos.\n\nGrunfeld_select &lt;- Grunfeld |&gt; \n  filter(firm %in% c(\"General Motors\",\n                     \"US Steel\",\n                     \"General Electric\",\n                     \"IBM\"))\n\ng &lt;- ggplot(Grunfeld_select, aes(x=capital,\n                  y=invest)) + \n  geom_jitter(aes(color = firm, size=value), alpha =0.5) + \n  geom_smooth(aes(col=firm), method=\"lm\", se=F) +\n  theme_bw()\n\ng\n\n\n\n\n\n\n\nFigura 3.13\n\n\n\n\n\nAvaliando o modelo considerando os dados de treino.\n\nmetrics(Grunfeld.tr$invest, lm_invest$fitted.values)\n\n      RMSE      MAE        R2\n1 44.97067 25.96427 0.9443401\n\n\nAvaliando o modelo considerando os dados de teste.\n\npred.invest &lt;- predict(lm_invest, newdata = Grunfeld.te)\n\nmetrics(Grunfeld.te$invest, pred.invest)\n\n      RMSE      MAE        R2\n1 66.90304 36.76732 0.9391516\n\n\nAdicionando termo de interaçao ao modelo.\n\nlm_invest_int &lt;- lm(invest ~ . + value*capital, Grunfeld.tr)\nsummary(lm_invest_int)\n\n\nCall:\nlm(formula = invest ~ . + value * capital, data = Grunfeld.tr)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-146.016  -11.418   -2.304   12.401  192.342 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               66.0183    16.3423   4.040 8.27e-05 ***\nvalue                    102.4319    12.5440   8.166 8.77e-14 ***\ncapital                   -0.3027    10.8409  -0.028 0.977757    \nyear                      21.3233     4.8910   4.360 2.31e-05 ***\n`firm_General Motors`    157.6237    45.0929   3.496 0.000611 ***\n`firm_US Steel`          249.4813    26.0617   9.573  &lt; 2e-16 ***\n`firm_General Electric`  -58.0340    27.4135  -2.117 0.035797 *  \nfirm_Chrysler             39.1575    15.1944   2.577 0.010861 *  \n`firm_Atlantic Refining`  69.6205    23.1453   3.008 0.003053 ** \nfirm_IBM                  25.2365    14.0326   1.798 0.073983 .  \n`firm_Union Oil`          53.0164    16.8449   3.147 0.001964 ** \nfirm_Westinghouse         -2.1632    14.5174  -0.149 0.881735    \nfirm_Goodyear             29.0504    17.4580   1.664 0.098054 .  \n`firm_Diamond Match`      -7.8207    12.9832  -0.602 0.547777    \nvalue:capital             29.3232     3.2597   8.996 6.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 38.2 on 161 degrees of freedom\nMultiple R-squared:  0.9633,    Adjusted R-squared:  0.9601 \nF-statistic: 301.5 on 14 and 161 DF,  p-value: &lt; 2.2e-16\n\n\nConsegue fazer os próximos passos para avaliar o modelo com interação nos dados de teste?\n\n\n3.4.4 Regressão polinomial\nCarregando pacote e dados.\n\nlibrary(MASS)\n\n\nrehab &lt;- wtloss\n# ?wtloss\n\nVisualizando o comportamento do peso (massa) em função de dias de treinamento.\n\nggplot(rehab, aes(x = Days, y = Weight)) + \n  geom_point(color = \"deepskyblue3\", size = 2) +\n  xlab(\"Dias\") + \n  ylab(\"Peso\") + theme_bw()\n\nSeparando dados de treino e teste.\n\nset.seed(53)\ntr &lt;- round(0.75*nrow(rehab),0)\ntreino &lt;- sample(nrow(rehab), tr, replace = F)\nrehab.tr &lt;- rehab[treino,]\nrehab.te &lt;- rehab[-treino,]\n\nEstimando modelos linear e quadrático.\n\nlm1 &lt;- lm(Weight ~ Days, rehab.tr)\nsummary(lm1)\n\n\nlm2 &lt;- lm(Weight ~ Days + I(Days^2), rehab.tr)\nsummary(lm2)\n\nDesempenho para dados de treino.\n\nmetrics(rehab.tr$Weight, lm1$fitted.values)\n\n\nmetrics(rehab.tr$Weight, lm2$fitted.values)\n\nDesempenho dos modelos para dados de teste.\n\npred.lm1 &lt;- predict(lm1, newdata = rehab.te)\nmetrics(rehab.te$Weight, pred.lm1)\n\n\npred.lm2 &lt;- predict(lm2, newdata = rehab.te)\nmetrics(rehab.te$Weight, pred.lm2)\n\nPlotando o modelo com dados de teste.\n\nggplot() + \n  geom_point(data = rehab.tr, mapping = aes(x = Days, y = Weight), color = \"deepskyblue3\", size = 2) +\n  geom_smooth(data = rehab.te, mapping = aes(x = Days, y = Weight), \n              method = \"lm\", formula = y ~ x + I(x^2), se = F, col = \"mediumvioletred\") +\n  ggtitle(\"Peso vs Dias (dados de teste)\") + \n  xlab(\"Dias\") + \n  ylab(\"Peso\") + theme_bw()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regressão linear simples, múltipla e polinomial</span>"
    ]
  },
  {
    "objectID": "AS_reg_ols.html#referências",
    "href": "AS_reg_ols.html#referências",
    "title": "3  Regressão linear simples, múltipla e polinomial",
    "section": "Referências",
    "text": "Referências\nHastie, T., Tibshirani, R., Friedman, J. H., & Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction (Vol. 2, pp. 1-758). New York: springer.\nGareth, J., Daniela, W., Trevor, H., & Robert, T. (2013). An introduction to statistical learning: with applications in R. Spinger.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regressão linear simples, múltipla e polinomial</span>"
    ]
  },
  {
    "objectID": "AS_tree.html",
    "href": "AS_tree.html",
    "title": "4  Regressão por árvores de decisão e floresta aleatória",
    "section": "",
    "text": "4.1 Árvores de decisão para regressão\nAs árvores de decisão foram inicialmente propostas para problemas de classificação, porém, podem ser adaptadas de forma simples para problemas de regressão. A regressão por árvores de decisão ou simplesmente as árvores de regressão envolvem dividir o espaço preditor em várias regiões retangulares simples tomando como predição a média ou outra medida simples das observações de treinamento da região. O conjunto de regras de divisão usado para segmentar o espaço preditor pode ser resumido em um diagrama semelhante a uma árvore, conforme a Figura 4.1.\nFigura 4.1: Diagrama de árvore de decisão (ou regressão neste caso)\nO gráfico de superfície da Figura 4.2 expõe o modelo obtido em função das duas variáveis preditoras, podendo-se observar que o valor previsto é constante em cada região retangular correspondente ao diagrama ilustrado anteriormente.\nFigura 4.2: Gráfico de superfície para um modelo de regressão baseado em árvore\nPode-se observar o mesmo modelo de árvore de regressão de forma bidimensional, conforme Figura 4.3, ficando mais clara a divisão do espaço preditor.\nFigura 4.3: Gráfico de contorno para um modelo de regressão baseado em árvore\nSejam \\(k\\) variáveis de entrada e uma resposta, ou seja, \\((\\mathbf{x}_i,y_i)\\), com \\(\\mathbf{x} = (x_{i1}, x_{i2},..., x_{iK})\\), para \\(i=1,...,N\\) observações de treino, o algoritmo de (Classification and Regression Trees - CART) para regressão define a cada iteração a variável preditora e o seu nível para particionar o espaço dos preditores. Considerando \\(J\\) regiões \\(R_1, R_2, ..., R_J\\), o valor predito será uma constante \\(\\gamma_j\\), em cada região, \\(j=1,...,J\\). Portanto, o modelo de árvore de regressão pode ser definido conforme a Equação 4.1, \\(\\{R_j,\\gamma_j\\}, j=1,\\ldots,J\\), onde \\(I(\\mathbf{x} \\in R_j)\\) é uma função indicativa que recebe 1 se \\(\\mathbf{x}\\) pertence à região \\(R_j\\) e 0 caso contrário. O melhor \\(\\gamma_j\\) para minimizar a soma dos quadrados é a média das observações na região, \\(\\hat{\\gamma}_j = (\\bar{y}_i | \\mathbf{x}_i \\in R_j\\)).\n\\[\n\\begin{aligned}\nf(\\mathbf{x}) = T(\\mathbf{x},R_j,\\gamma_j) = \\sum_{j=1}^J \\gamma_jI(\\mathbf{x} \\in R_j)\n\\end{aligned}\n\\tag{4.1}\\]\nConsiderando todos os dados de treinamento, as divisões são definidas tomando uma variável para divisão, \\(x_k\\), \\(k = 1,..., K\\), e um ponto de divisão \\(x_k = s\\), \\(R_1(k,s)\\) = \\({\\mathbf{x}|x_k \\leq s}\\) e \\(R_2(k,s)\\) = \\({\\mathbf{x}|x_k&gt;s}\\). Portanto, o algoritmo CART busca a variável para o particionamento e o valor desta na divisão resolvendo a Equação 4.2.\n\\[\n\\begin{aligned}\n    \\min_{k,s} \\Bigl[ \\min_{\\gamma_1} \\sum_{x_i \\in R_1(k,s)} (y_i - \\gamma_1)^2 +  \\min_{\\gamma_2} \\sum_{x_i \\in R_2(k,s)} (y_i - \\gamma_2)^2 \\Bigr].\n\\end{aligned}\n\\tag{4.2}\\]\nO algoritmo CART repete o particionamento recursivamente até um determinado critério de parada ser alcançado, por exemplo, até um número mínimo de observações em cada partição ser atingido.\nSeja um conjunto de dados da liga maior americana de Baseball para as temporadas de 1986 e 1987. São disponibilizadas 322 observações de jogadores da liga, incluindo número de batidas, número de corridas, tempo em anos na liga, etc. A variável de interesse a ser predita é o salário do jogador. A Figura 4.4 exibe um gráfico de correlação entra tais variáveis.\nFigura 4.4: Gráfico de correlação para o conjunto de dados Hitters\nSeja um modelo de árvore de regressão para prever o salário em função das demais variáveis considerando metade dos dados selecionados aleatoriamente para treino do modelo. Po-de se observar que o modelo de árvore de decisão detalhado abaixo pode ser descrito como um conjunto de regras “se”, ’se não”. Por exemplo, tomando as seguintes partições CRuns &lt; 325.5 82  6125000  296.3 e CRuns &gt; 208.5 22   514100  498.8 *, tem-se o valor previsto do salário do jogador igual a 498.8 unidades monetárias com soma dos quadrados dos erros igual a 514100.\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n  1) root 132 25650000  510.7  \n    2) CRuns &lt; 325.5 82  6125000  296.3  \n      4) CRuns &lt; 208.5 60  4378000  222.1  \n        8) Hits &lt; 45 5  3103000  558.3 *\n        9) Hits &gt; 45 55   659200  191.5  \n         18) CRBI &lt; 114.5 36   100900  131.2 *\n         19) CRBI &gt; 114.5 19   178200  306.0 *\n      5) CRuns &gt; 208.5 22   514100  498.8 *\n    3) CRuns &gt; 325.5 50  9578000  862.2  \n      6) RBI &lt; 103 44  4352000  765.5  \n       12) PutOuts &lt; 1113.5 39  2944000  704.6  \n         24) Runs &lt; 38 7   297000  431.0 *\n         25) Runs &gt; 38 32  2008000  764.5  \n           50) CRuns &lt; 444.5 10   466300  594.7  \n            100) CWalks &lt; 256.5 5    24560  759.4 *\n            101) CWalks &gt; 256.5 5   170400  430.0 *\n           51) CRuns &gt; 444.5 22  1123000  841.7  \n            102) CHmRun &lt; 142.5 9   426800  987.2 *\n            103) CHmRun &gt; 142.5 13   373200  740.9 *\n       13) PutOuts &gt; 1113.5 5   134200 1241.0 *\n      7) RBI &gt; 103 6  1797000 1571.0 *\nO diagrama do modelo é plotado na Figura 4.5. Acima tem-se CRBI&lt;325,5 que consiste na primeira partição ou nó raiz. Qualquer subconjunto de partições interligadas da árvore pode ser chamado de sub-árvore. Cada partição gera duas regiões às quais podem ou não ser particionadas novamente. As partições finais são chamadas de nós terminais ou folhas. Neste caso tem-se 10 nós terminais, ou seja, 10 valores previstos distintos para 10 regiões distintas do espaço de preditores.\nFigura 4.5: Diaframa do modelo de regressão baseado em árvore para o conjunto de dados Hitters\nPode-se realizar uma validação cruzada para “podar” ou “secar” a árvore com a finalidade de diminuir o sobreajuste. A Figura 4.6 plota o erro em relação ao tamanho ou número de partições da árvore. Observa-se que um modelo com três ou quatro partições apresenta bom resultado.\nFigura 4.6: Validação cruzada para secar/podar a árvore\nA árvore podada obtida e plotada na Figura 4.7 apresenta apenas 4 folhas, com estrutura mais simples, facilitando a interpretação e buscando melhor generalização.\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 132 25650000  510.7  \n   2) CRuns &lt; 325.5 82  6125000  296.3 *\n   3) CRuns &gt; 325.5 50  9578000  862.2  \n     6) RBI &lt; 103 44  4352000  765.5  \n      12) PutOuts &lt; 1113.5 39  2944000  704.6 *\n      13) PutOuts &gt; 1113.5 5   134200 1241.0 *\n     7) RBI &gt; 103 6  1797000 1571.0 *\nFigura 4.7: Modelo de regressão por árvore após poda",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regressão por árvores de decisão e floresta aleatória</span>"
    ]
  },
  {
    "objectID": "AS_tree.html#bagged-trees-ou-bagging",
    "href": "AS_tree.html#bagged-trees-ou-bagging",
    "title": "4  Regressão por árvores de decisão e floresta aleatória",
    "section": "4.2 Bagged trees ou bagging",
    "text": "4.2 Bagged trees ou bagging\nBagged trees ou bagging ou, em português, poderia ser traduzido como “árvores ensacadas” é um método baseado em árvore de decisão que pode ser aplicado tanto para regressão quanto para classificação. O método consiste em agregar várias árvores de decisão as quais são estimadas a partir de reamostragem por reposição dos dados de treino. Ou seja, antes de estimar cada árvore, realiza-se um sorteio por reposição dos dados de treino, procedimento este chamado de bootstrap e, posteriormente estima-se a árvore para cada amostra obtida por sorteio com reposição. O modelo final é a média de todas as árvores obtidas via bootstrap. Geralmente um número alto de reamostragens é realizado e, portanto, um número alto de árvores é obtido, resultando em um modelo com maior flexibilidade, porém mais difícil de interpretar. O nome bagging vem de bootstrap aggregated ou agregação por bootstrap.\nConsidere um conjunto de dados com \\(N = 10\\) observações, com três preditores e uma variável independente, exibido integralmente na Tabela 4.1.\n\n\n\n\nTabela 4.1: Conjunto de dados arbitrário para regressão\n\n\n\n\n\n\n\n\n\ni\nx1\nx2\nx3\ny\n\n\n\n\n1\n-0.08\n1.24\n4.28\n6.74\n\n\n2\n0.84\n2.29\n2.99\n7.32\n\n\n3\n-0.46\n2.42\n2.60\n5.25\n\n\n4\n-0.55\n0.71\n3.02\n4.02\n\n\n5\n0.74\n2.07\n4.74\n8.80\n\n\n6\n-0.11\n1.19\n1.89\n4.06\n\n\n7\n-0.17\n3.51\n1.94\n6.45\n\n\n8\n-1.09\n1.73\n4.95\n6.45\n\n\n9\n-3.01\n3.56\n3.60\n5.14\n\n\n10\n-0.59\n1.76\n0.98\n3.24\n\n\n\n\n\n\n\n\n\n\nUm bootstrap deste conjunto de dados pode resultar na reamostragem exibida na Tabela 4.2. Pode-se observar que as observações 4, 5, 6 e 9 foram sorteadas duas vezes, enquanto as observações 1, 3, 7 e 8 não foram sorteadas.\n\n\n\n\nTabela 4.2: Bootstrap do conjunto de dados arbitrário para regressão\n\n\n\n\n\n\n\n\n\ni\nx1\nx2\nx3\ny\n\n\n\n\n9\n-3.01\n3.56\n3.60\n5.14\n\n\n6\n-0.11\n1.19\n1.89\n4.06\n\n\n10\n-0.59\n1.76\n0.98\n3.24\n\n\n9\n-3.01\n3.56\n3.60\n5.14\n\n\n4\n-0.55\n0.71\n3.02\n4.02\n\n\n5\n0.74\n2.07\n4.74\n8.80\n\n\n4\n-0.55\n0.71\n3.02\n4.02\n\n\n5\n0.74\n2.07\n4.74\n8.80\n\n\n6\n-0.11\n1.19\n1.89\n4.06\n\n\n2\n0.84\n2.29\n2.99\n7.32\n\n\n\n\n\n\n\n\n\n\nA Figura 4.8 exibe o diagrama de uma árvore de decisão para a amostra obtida via bootstrap. O bagging repete este processso diversas vezes, reamostrando os dados por bootstrap e estimando para cada “nova” amostra uma nova árvore. Ao final é tomada como predição a média de todas as árvores.\n\n\n\n\n\n\n\n\nFigura 4.8: Modelo de regressão por árvore após poda\n\n\n\n\n\nPara o mesmo conjunto de dados, sejam 6 árvores obtidas após reamostragens dos dados via bootstrap exibidas na Figura 4.9.\n\n\n\n\n\n\n\n\nFigura 4.9: Modelo de regressão por árvore após poda\n\n\n\n\n\nSeja uma observação futura arbitrária com valores exibidos na Tabela 4.3.\n\n\n\n\nTabela 4.3: Observação futura\n\n\n\n\n\n\nx1\nx2\nx3\n\n\n\n\n0.14\n3.27\n4.57\n\n\n\n\n\n\n\n\nConsiderando as 6 árvores obtidas anteriormente, as previsãos destas para a observação da Tabela 4.3 são: 7,302, 5,730, 6,732, 7,680, 7,680 e 5,490. O modelo bagging retorna como resultado a média das previsões das árvores, isto é, \\(\\hat{y} = \\frac{1}{6}\\sum_{i=b}^6 T(\\mathbf{x}_0)\\) = 6,774.\nPara o conjunto de dados Hitters seja um modelo de regressão por Bagging considerando duas variáveis, Cruns e RBI. A Figura 4.10 plota um gráfico de superfície do modelo, o qual é muito mais complexo e pretende apresentar uma melhor acuracidade que um modelo baseado em árvore por justamente congregar diversas árvores baseadas na reamostragem dos dados de treino. Porém, Como tais modelos apresentam maior variância que os modelos de árvore para regressão, é importante avaliar se há sobreajuste.\n\n\n\n\n\n\n\n\nFigura 4.10: Gráfico de superfície para um modelo de regressão via bagging para o conjunto de dados Hitters",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regressão por árvores de decisão e floresta aleatória</span>"
    ]
  },
  {
    "objectID": "AS_tree.html#floresta-aleatória",
    "href": "AS_tree.html#floresta-aleatória",
    "title": "4  Regressão por árvores de decisão e floresta aleatória",
    "section": "4.3 Floresta aleatória",
    "text": "4.3 Floresta aleatória\nO modelo de floresta aleatória consiste em uma evolução do modelo bagging com a finalidade de diminuir a variância deste. Basicamente, em casos onde há multicolineariedade ou correlação entre as variáves regressoras, o bagging pode acarretar na seleção de apenas alguns dos preditores disponíveis durante o particionamento binário recursivo, de forma que as árvores geradas considerando a reamostragem das observações de treino possam apresentar alta correlação entre si, acarretando em alta variabilidade das previsões finais. No modelo de floresta aleatória, antes de cada partição, são selecionadas aleatoriamente \\(m\\) variáveis regressoras, \\(m&lt;k\\), às quais serão consideradas no processo de particionamento binário. Geralmente para regressão recomenda-se \\(m=k/3\\).\nA Figura 4.11 apresenta o gráfico de superfície de um modelo de floresta aleatória para o conjunto de dados Hitters considerando duas variáveis, \\(k=2\\), Cruns e RBI. Para cada particionamento considerou-se \\(m=2/3 \\simeq 1\\).\n\n\n\n\n\n\n\n\nFigura 4.11: Gráfico de superfície para um modelo de regressão via floresta aleatória para o conjunto de dados Hitters",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regressão por árvores de decisão e floresta aleatória</span>"
    ]
  },
  {
    "objectID": "AS_tree.html#implementações-em-r",
    "href": "AS_tree.html#implementações-em-r",
    "title": "4  Regressão por árvores de decisão e floresta aleatória",
    "section": "4.4 Implementações em R",
    "text": "4.4 Implementações em R\nA seguir serão expostas as implementações necessárias para obter os resultados do capítulo.\n\n4.4.1 Árvores para regressão\nCarregando pacotes.\n\nlibrary(ISLR)\nlibrary(tree)\nlibrary(GGally)\nlibrary(randomForest)\n\nCarregando base de dados sobre liga de Baseball americana para as temporadas de de 1986 a 1987.\n\ndata(Hitters, package = \"ISLR\")\ndados &lt;- na.omit(Hitters)\n\nVisualizando dados.\n\nr &lt;- cor(dados[,-c(14,15,20)])\nlibrary(corrplot)\ncorrplot::corrplot(r, method=\"color\", \n                   type=\"upper\", order=\"hclust\", \n                   addCoef.col = NULL, tl.srt=45, \n                   diag=FALSE)\n\n\nggpairs(dados[,c(3,4,6,8,9,12,13,16,19)], aes(color = dados$Division, alpha = .2)) + theme_bw()\n\nSorteando observações de treino.\n\nset.seed(1)\ntr &lt;- round(0.5*nrow(dados))\ntreino &lt;- sample(1:nrow(dados), tr, replace = F)\n\nÁrvore de regressão para prever o salário do jogador em função das variáveis de desempenho deste.\n\ntree1 &lt;- tree(Salary ~ ., dados, subset = treino)\ntree1\n\nPlotando o diagrama da árvore de regressão.\n\nplot(tree1)\ntext(tree1, cex = 0.6)\n\n\nmetrics &lt;- function(obs, pred) {\n  \n  RSE &lt;- sum((obs - pred)^2)\n  SST &lt;- sum((obs - mean(obs))^2)\n  R2 &lt;- 1 - RSE/SST \n  \n  MAE &lt;-  mean(abs(obs - pred))\n  \n  RMSE &lt;- sqrt(mean((obs - pred)^2))\n  \n  return(\n    data.frame(RMSE = RMSE,\n               MAE = MAE,\n               R2 = R2))\n}\n\nDesempenho do modelo para dados de teste.\n\npred.teste &lt;- predict(tree1, newdata = dados[-treino,])\nmetrics(dados$Salary[-treino], pred.teste)\n\nValidação cruzada para podar a árvore.\n\nset.seed(3)\ncv.tree1 &lt;- cv.tree(tree1)\nplot(cv.tree1$size, cv.tree1$dev, type = \"b\", col = \"blue\")\n\n\nprune1 &lt;- prune.tree(tree1, best = 4)\nprune1\n\n\nplot(prune1)\ntext(prune1, cex = 0.6)\n\nAvaliando modelo podado.\n\npred.teste2 &lt;- predict(prune1, newdata = dados[-treino,])\nmetrics(dados$Salary[-treino], pred.teste2)\n\n\n\n4.4.2 Bagging\n\nbag &lt;- randomForest(Salary ~ ., dados, subset = treino, mtry = 19,\n                    importance = TRUE, ntree = 500)\n\n\npred.bag &lt;- predict(bag, newdata = dados[-treino,])\nmetrics(dados$Salary[-treino], pred.bag)\n\n\n\n4.4.3 Random Forest\nNo caso da foresta aleatória deve-se considerar mtry=k/3 para problemas de regressão, sendo sorteadas \\(m\\) features das \\(k\\) disponíveis para serem consideradas no particionamento binário recursivo, de forma a “decorrelacionar” as árvores.\n\nrf &lt;- randomForest(Salary ~ ., dados, subset = treino, mtry = 6,\n                   importance = TRUE, ntree = 500)\n\n\npred.rf &lt;- predict(rf, newdata = dados[-treino,])\nmetrics(dados$Salary[-treino], pred.rf)\n\n\n\n4.4.4 Comparando os resultados com regressão linear múltipla\n\nlm1 &lt;- lm(Salary ~ ., dados, subset = treino)\nsummary(lm1)\n\n\npred.lm &lt;- predict(lm1, newdata = dados[-treino,])\nmetrics(dados$Salary[-treino], pred.lm)\n\nObviamente, há muitas possibilidades para melhorar o modelo de regressão múltipla que não foram consideradas nesta rápida implementação. Por exemplo, as variáveis numéricas não foram escalonadas, pode-se tentar reduzir o modelo com eliminação para trás aplicando a função step. Pode-se testar também os métodos de regessão rígida e LASSO, entre outras possibilidades, como considerar termos de interação e polinomiais. Consegue testar algumas possibilidades?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regressão por árvores de decisão e floresta aleatória</span>"
    ]
  },
  {
    "objectID": "AS_tree.html#referências",
    "href": "AS_tree.html#referências",
    "title": "4  Regressão por árvores de decisão e floresta aleatória",
    "section": "Referências",
    "text": "Referências\nHastie, T., Tibshirani, R., Friedman, J. H., & Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction (Vol. 2, pp. 1-758). New York: springer.\nGareth, J., Daniela, W., Trevor, H., & Robert, T. (2013). An introduction to statistical learning: with applications in R. Spinger.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regressão por árvores de decisão e floresta aleatória</span>"
    ]
  },
  {
    "objectID": "AS_cv.html",
    "href": "AS_cv.html",
    "title": "6  Validação cruzada e grid search",
    "section": "",
    "text": "6.1 Conceitos iniciais\nSeja a modelagem de uma resposta \\(y\\) a qual supõe-se dependente de um vetor de variáveis de entrada ou preditoras \\(\\mathbf{x} = [x_1, ..., x_k]^T\\). Um modelo desconhecido pode ser denotado \\(f(\\mathbf{\\mathbf{x}, \\mathbf{\\alpha}})\\), onde \\(\\mathbf{\\alpha}\\) consiste em um (ou um conjunto de) hyperparâmetro que minimiza o erro de previsão. Por exemplo, para a regressão rígida, o hiperparâmetro consiste na constante de regularização ou penalização de encolhimento, \\(\\lambda\\). Já a regressão por vetores de suporte apresenta dois hiperparâmetros principais, o valor do erro máximo permitido na função perda, \\(\\varepsilon\\) e o valor da penalização na otimização, \\(C\\). Um conjunto de observações de treino, \\(\\mathcal{T} = (\\mathbf{x}_1,y_1), ..., (\\mathbf{x}_N,y_N)\\), obtidos da distribuição de probabilidade conjunta de \\(\\mathbf{x},y\\), \\(P(\\mathbf{x},y)\\), está disponível para o propósito de estimativas. O modelo estimado, \\(\\hat{f}(\\mathbf{x}, \\mathbf{\\alpha})\\), é obtido pela minimização de uma função perda, que mede a distância entre o modelo e as observações de treino. No caso de problemas de regressão, \\(y \\in \\mathcal{R}^1\\), a escolha mais comum é a função perda quadrática, \\(L(y,\\hat{f}(\\mathbf{x}, \\mathbf{\\alpha})) = (y-\\hat{f}(\\mathbf{x}, \\mathbf{\\alpha}))^2\\). O erro de generalização ou de teste pode ser calculado segundo a Equação a seguir, onde \\((\\mathbf{x}_0,y_0)\\) é uma nova observação.\n\\[\n    Err_\\mathcal{T} = E[L(y_0,\\hat{f}(\\mathbf{x}_0, \\mathbf{\\alpha}))|\\mathcal{T}]\n\\]\nO erro de treino, \\(\\overline{err} = (1/N)\\sum_{i=1}^{N}L(y_i,\\hat{h}(\\mathbf{x}_i, \\mathbf{\\alpha}))\\), sempre será maior que o erro de generalização, \\(Err_\\mathcal{T}\\). A forma mais eficiente e prática de estimar o erro de generalização é através da validação cruzada (HASTIE et al., 2009).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Validação cruzada e *grid search*</span>"
    ]
  },
  {
    "objectID": "AS_cv.html#validação-cruzada-por-dados-de-treino-e-teste",
    "href": "AS_cv.html#validação-cruzada-por-dados-de-treino-e-teste",
    "title": "6  Validação cruzada e grid search",
    "section": "6.2 Validação cruzada por dados de treino e teste",
    "text": "6.2 Validação cruzada por dados de treino e teste\nA validação cruzada por dados de treino e teste é basicamente o que foi feito nos métodos aprendidos até aqui. As \\(N\\) observações disponíveis, \\(\\mathcal{T}\\), são divididas em observações de treino e teste. O modelo é estimado usando as observações de treino e testado nas observações de teste. Conforme explicado, a própria função perda pode ser usada para avaliar a capacidade de generalização do modelo, mas, na prática, algumas outras métricas podem ser úteis para avaliar o erro em dados de teste.\nA métrica \\(R^2\\), \\(R^2 \\in [0,1]\\), denominada coeficiente de determinação, consiste na proporção da variabilidade dos dados explicados pelo modelo. Quanto maior é o valor de \\(R^2\\) para os dados de teste, maior a capacidade de generalização do modelo.\n\\[\n    R^2(y,\\hat{y})  = 1 - \\frac{\\sum_{i=1}^{n}{(y_i - \\hat{y}_i)}^2}{\\sum_{i=1}^{n}{(y_i - \\bar{y})}^2}\n\\]\nSe o modelo for pior do que um modelo simples que prevê a média, \\(\\hat{y} = \\overline{y}\\), o \\(R^2\\) pode ser negativo. Isso ocorre porque o denominador, que representa a variabilidade total da resposta, pode ser menor do que o numerador, que expressa a variabilidade explicada pelo modelo. Essa situação pode ocorrer na validação cruzada, na qual os modelos são testados em dados futuros, e, posteriormente, a métrica é calculada.\nA raiz do erro quadrático médio (root mean square error - RMSE) consiste na raiz da média dos quadrados dos erros de previsão de teste. O RMSE tem a mesma unidade da resposta em estudo, porém é mais impactado por outliers, demonstrando a influência destes no erro de previsão. Esta métrica é uma boa opção quando deseja-se mensurar o erro na mesma unidade da resposta.\n\\[\n    RMSE (y,\\hat{y})  = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}{(y_i - \\hat{y}_i)}^2}\n\\]\nO erro médio absoluto (mean absolute error - MAE) mede a média dos valores absolutos das diferenças entre o valor real e o valor predito. Essa métrica é mais robusta a outliers e também mede o erro na mesma unidade de medida da resposta.\n\\[\n    MAE (y,\\hat{y})  = \\frac{1}{n} \\sum_{i=1}^{n}\\left|y_i - \\hat{y}_i\\right|\n\\]\nHá dois problemas mais latentes na abordagem treino e teste. O erro estimado nos dados de teste apresentam alta variabilidade dependendo das observações que foram sorteadas para treino e teste. Devido uma parte significativa das observações ser separada para teste, geralmente a abordagem em questão tende a superestimar o erro do modelo quando aplicado em todo conjunto de dados (GARETH et al., 2013).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Validação cruzada e *grid search*</span>"
    ]
  },
  {
    "objectID": "AS_cv.html#validação-cruzada-por-k-dobras",
    "href": "AS_cv.html#validação-cruzada-por-k-dobras",
    "title": "6  Validação cruzada e grid search",
    "section": "6.3 Validação cruzada por \\(k\\)-dobras",
    "text": "6.3 Validação cruzada por \\(k\\)-dobras\nA validação cruzada via \\(k\\)-dobras (\\(k\\)-fold cross-validation) é a abordagem mais recomendada dadas suas propriedades e simplicidade. Particionando os dados disponíveis em \\(K\\) folds, \\(k = 1, ..., K\\), com aproximadamente o mesmo número de observações em cada partição, \\(n_k \\sim N/K\\). Para a \\(k\\)-ésima partição o modelo é estimado usando os dados das outras \\(K-1\\) partições ou dobras, enquanto o erro é estimado na \\(k\\)-ésima dobra. Seja a função aproximada com a \\(k\\)-ésima dobra excluída, \\(\\hat{f}^{-k(i)}(\\mathbf{x}_i,\\alpha)\\). O erro de generalização ou teste é calculado via validação cruzada segundo a Equação a seguir. Mais detalhes podem ser encontrados em Hastie et al. (2009).\n\\[\n    CV(\\hat{h}, \\alpha) = \\frac{1}{N} \\sum_{i=1}^N L(y_i, \\hat{h}^{-k(i)}(\\mathbf{x}_i,\\alpha))\n\\]\nA Figura 6.1 ilustra a validação cruzada por \\(k\\)-dobras para \\(K=5\\) dobras.\n\n\n\n\n\n\n\n\nFigura 6.1: Validação cruzada via k-dobras",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Validação cruzada e *grid search*</span>"
    ]
  },
  {
    "objectID": "AS_cv.html#validação-cruzada-por-leave-one-out",
    "href": "AS_cv.html#validação-cruzada-por-leave-one-out",
    "title": "6  Validação cruzada e grid search",
    "section": "6.4 Validação cruzada por leave-one-out",
    "text": "6.4 Validação cruzada por leave-one-out\nO método leave-one-out (LOO) é um caso particular do \\(k\\)-dobras para \\(k = N\\), ou seja, considera uma observação em cada partição ou dobra. É geralmente utilizado para casos onde há poucas observações disponíveis. Para a primeira dobra um modelo é estimado para as \\(N-1\\) observações remanescentes e testado na primeira observação. Tomando por exemplo a métrica \\(MSE\\), o erro é estimado como \\((y_1-\\hat{y}_1)^2\\), uma estimativa com pouco viés mas com alta variância devido o tamanho amostral unitário, portanto limitado.\nA maior vantagem do \\(k\\)-dobras para o LOO é o tempo computacional. Tomando \\(k=10\\), por exemplo, serão estimados 10 modelos, enquanto a validação via LOO estimará \\(N\\), o que pode implicar em alto tempo computacional para tamanhos amostrais grandes, especialmente para métodos de aprendizado mais complexos (GARETH et al., 2013). Para métodos de origem na estatística, como os modelos de regressão por mínimos quadrados, é possível estimar o erro via LOO sem necessariamente fazer um loop, mas considerando o erro de previsão do modelo a partir da matriz chapéu. Para mais detalhes ver Hastie et al. (2009).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Validação cruzada e *grid search*</span>"
    ]
  },
  {
    "objectID": "AS_cv.html#implementações-em-r",
    "href": "AS_cv.html#implementações-em-r",
    "title": "6  Validação cruzada e grid search",
    "section": "6.7 Implementações em R",
    "text": "6.7 Implementações em R\nA seguir serão expostas as implementações necessárias para obter os resultados do capítulo.\n\nlibrary(DAAG)\nlibrary(GGally)\nlibrary(glmnet)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(plyr)\nlibrary(gt)\n\n\ndados &lt;- cfseal[ colSums(is.na(cfseal))==0]\n\n\nggpairs(dados) + theme_bw()\n\n\nset.seed(7)\ndados_ &lt;- dados\nfold &lt;- sample(rep(1:10,3), replace = F) \n\ndados_$fold &lt;- fold \n\n\ndados_ |&gt; \n  gt() |&gt; \n  data_color(\n    columns = fold,\n    target_column = everything(),\n    method = \"numeric\")\n\n\ndados_ |&gt; \n  arrange(fold) |&gt; \n  gt() |&gt; \n  data_color(\n    columns = fold,\n    target_column = everything(),\n    method = \"numeric\")\n\n\nX &lt;- data.frame(model.matrix(age ~ ., dados_)[,-1])\ny &lt;- data.frame(age = dados_$age,\n                fold = dados_$fold)\n\n\ngrid &lt;- 10^seq(10, -2, length = 100)\n\nmetrics &lt;- function(obs, pred) {\n  \n  RSE &lt;- sum((obs - pred)^2)\n  SST &lt;- sum((obs - mean(obs))^2)\n  R2 &lt;- 1 - RSE/SST \n  \n  MAE &lt;-  mean(abs(obs - pred))\n  \n  RMSE &lt;- sqrt(mean((obs - pred)^2))\n  \n  return(\n    data.frame(RMSE = RMSE,\n               MAE = MAE,\n               R2 = R2))\n}\n\n\nfor (i in 1:10){\n  rig &lt;- glmnet(as.matrix(X |&gt;\n                            filter(fold != i) |&gt;\n                            select(weight, heart, liver, stomach, kidney)), \n                (y |&gt; \n                  filter(fold != i))$age, \n                alpha = 0, \n                lambda = grid)\n  \n  assign(paste0(\"rid.\", i), rig)\n  \n  pred &lt;- predict(rig,\n                  s = grid,\n                  newx = as.matrix(X |&gt;\n                                     filter(fold == i) |&gt;\n                                     select(-fold)))\n  \n  perf &lt;- rbind(apply(pred, 2, metrics, obs = (y|&gt; filter(fold == i))$age))\n  perf &lt;- data.frame(matrix(unlist(perf), \n                            nrow=length(perf), \n                            byrow=TRUE))\n  colnames(perf) &lt;- c(\"RMSE\",\"MAE\",\"R2\")\n  perf$grid &lt;- grid\n  \n  assign(paste0(\"perf.\", i), perf)\n  \n  # p &lt;- ggplot(perf, aes(x=grid,y=RMSE)) +\n  # geom_point() + \n  # scale_x_log10() +\n  # theme_bw()\n  # print(p)\n}\n\n\nperf &lt;-\n  rbind.fill(list(perf.1,perf.2,perf.3,perf.4,perf.5,\n                  perf.6,perf.7,perf.8,perf.9,perf.10))\n\nperf$fold &lt;- rep(1:10,each=100)\n\n# ggplot(perf, aes(x=grid,y=RMSE, col=as.factor(fold))) +\n#   geom_point() + \n#   scale_x_log10() +\n#   labs(x=\"lambda\", col=\"fold\") +\n#   theme_bw()\n\n\nggplot(perf, aes(x=grid,y=RMSE,col=as.factor(fold))) +\n  geom_point() + \n  facet_grid(vars(fold)) +\n  scale_x_log10() +\n  labs(x=\"log(lambda)\", col=\"fold\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\nperf &lt;- perf |&gt;\n  group_by(grid) |&gt;\n  dplyr::summarize(RMSE_ = mean(RMSE),\n                   SDRMSE_ = sd(RMSE),\n                   MAE_ = mean(MAE),\n                   SDMAE_ = sd(MAE),\n                   R2_ = mean(R2),\n                   SDR2_ = sd(R2))\n\n\nggplot(perf, aes(x=grid,y=RMSE_)) +\n  geom_errorbar(aes(ymin=RMSE_-1.96*SDRMSE_/sqrt(10), \n                    ymax=RMSE_+1.96*SDRMSE_/sqrt(10)), col=\"grey\") +\n  geom_point() + \n  scale_x_log10() +\n  labs(x=\"lambda\",y=\"RMSE\") +\n  theme_bw()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Validação cruzada e *grid search*</span>"
    ]
  },
  {
    "objectID": "AS_cv.html#referências",
    "href": "AS_cv.html#referências",
    "title": "6  Validação cruzada e grid search",
    "section": "Referências",
    "text": "Referências\nHastie, T., Tibshirani, R., Friedman, J. H., & Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction (Vol. 2, pp. 1-758). New York: springer.\nGareth, J., Daniela, W., Trevor, H., & Robert, T. (2013). An introduction to statistical learning: with applications in R. Springer.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Validação cruzada e *grid search*</span>"
    ]
  },
  {
    "objectID": "AS_ridge_LASSO.html",
    "href": "AS_ridge_LASSO.html",
    "title": "5  Regressão rígida e LASSO",
    "section": "",
    "text": "5.1 Regressão rígida\nQuando um modelo de regressão linear múltipla apresenta muitas variáveis correlacionadas, seus coeficientes podem ser mal estimados, acarretando em alta variância das estimativas. Tais modelos costumam ter seus coeficientes inflados devido à correlação. A regressão rígida tem por finalizadade encolher ou diminuir coeficientes de variáveis correlacionadas, com a finalidade de minimizar a variância do modelo.\nPara encolher os coeficientes, a regressão rígida usa uma alteração na função perda otimizada nas estimativas de mínimos quadrados.\nSeja a função perda quadrática descrita anteriormente para mínimos quadrados em regressão múltipla.\n\\[\n\\begin{align}\nL(\\mathbf{\\beta}) =& \\sum_{i=1}^N \\varepsilon_i^2 = \\sum_{i=1}^N(y_i-\\hat{y}_i)^2 =\\sum_{i=1}^N(y_i-\\beta_0-\\sum_{j=1}^k\\beta_jx_{ij})^2\\\\\nL(\\mathbf{\\beta}) =& \\mathbf{\\varepsilon}^T\\mathbf{\\varepsilon} = (\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta}) \\\\\n=&\\mathbf{y}^T\\mathbf{y} - 2\\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{\\beta}\n\\end{align}\n\\]\nMinimizando tal quantidade pela escolha de \\(\\beta\\), conforme visto anteriormente, resulta nas equações normais de mínimos quadrados, \\(\\hat{\\beta}=(\\mathbf{X}^T\\mathbf{X})^{-1}(\\mathbf{X}^T\\mathbf{y})\\). Adicionado o termo de penalização de encolhimento dos coeficientes, \\(\\lambda\\beta^T\\beta\\), tem-se:\n\\[\n\\begin{aligned}\nL(\\mathbf{\\beta,\\lambda}) = \\mathbf{y}^T\\mathbf{y} - 2\\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{\\beta} + \\lambda\\beta^T\\beta\n\\end{aligned}\n\\]\nEm notação algébrica a quantidade acima, pode ser escrita como:\n\\[\n\\begin{aligned}\nL(\\mathbf{\\beta,\\lambda}) =\\sum_{i=1}^N(y_i-\\beta_0-\\sum_{j=1}^k\\beta_jx_{ij})^2 + \\lambda\\sum_{j=1}^k\\beta_j^2\n\\end{aligned}\n\\]\nOnde \\(\\lambda\\) é uma constante não negativa de penalização de encolhimento (shrinkage penalty), que previne a inflação dos coeficientes. O termo \\(\\lambda\\sum_{j=1}^k\\beta_j^2\\) também é chamado de de termo de regularização. Se \\(\\lambda=0\\), tem-se a estimativa padrão de mínimos quadrados que, no caso de presença de multicolineariedade, implicarão em um modelo com alta variância. Se \\(\\lambda \\rightarrow \\infty\\), os coeficientes tenderão a 0, \\(\\beta_j \\rightarrow 0\\), \\(\\forall j\\), o que implicaria, por outro lado, em um modelo com alto vício ou viés. A regressão rígida visa, portanto, trabalhar o conflito entre vício e variância em modelos de regressão múltipla. Deve-se buscar um valor de \\(\\lambda\\) que estabeleça uma melhor relação entre tais medidas de erro.\nTomando a notação matricial, resolvendo para \\(\\beta\\), os coeficientes de mínimos quadrados para regressão rígida ficam:\n\\[\n\\begin{aligned}\n\\frac{\\partial L}{\\partial \\mathbf{\\beta}} = -2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\mathbf{\\beta} +2\\lambda\\beta= 0 \\\\\n\\hat{\\mathbf{\\beta}} = (\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})^{-1}(\\mathbf{X}^T\\mathbf{y})\n\\end{aligned}\n\\]\nSeja um conjunto de dados com informações de custos de empresas de manufatura. Pode-se observar na Figura 5.1 a presença de alta correlação entre algumas das variáveis de custo consideradas.\nFigura 5.1: Gráfico de correlações aos pares para os custos de manufatura\nO gráfico de correlação da Figura 5.2 expõe as correlações aos pares em um mapa de calor, de forma que as positivas tendem para o azul escuro, enquanto as negativas para vermelho escuro.\nFigura 5.2: Mapa de calor de correlações para os custos de manufatura\nEm um modelo de regressão rígida considerando o custo como resposta e as demais variáveis como preditores, o gráfico da Figura 5.3 expõe os níveis dos coeficientes em relação à constante de encolhimento \\(\\lambda\\). Observa-se que o aumento desta constante implica no encolhimento dos coeficientes. Deve-se tentar encontrar o valor de \\(\\lambda\\) que minimize o erro do modelo. Por exemplo, para o custo de energia, pode-se observar que um \\(\\lambda=0\\) implica em alta magnitude dos coeficientes, sendo observado o encolhimento destes com o aumento de \\(\\lambda\\).\nFigura 5.3: Coeficientes versus penalização de encolhimento na regressão rígida\nA partir de validação cruzada pode-se selecionar o valor de \\(\\lambda\\) dado um grid de valores. A Figura 5.4 plota os resultados. Para o exemplo o melhor nível seria \\(\\lambda^*=12.022\\).\nFigura 5.4: Validação cruzada e grid search para buscar o lambda ótimo na regressão rígida\nOs coeficientes de regressão rígida para o \\(\\lambda\\) ótimo são expostos à seguir.\n9 x 1 sparse Matrix of class \"dgCMatrix\"\n                        s1\n(Intercept)      123.25007\ncapitalcost    -3408.22133\nlaborcost       1363.79881\nenergycost     -8394.77369\nmaterialscost   -386.30713\ncapitalprice      72.51032\nlaborprice       117.87583\nenergyprice       95.02103\nmaterialsprice   201.05565",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regressão rígida e LASSO</span>"
    ]
  },
  {
    "objectID": "AS_ridge_LASSO.html#regressão-via-lasso",
    "href": "AS_ridge_LASSO.html#regressão-via-lasso",
    "title": "5  Regressão rígida e LASSO",
    "section": "5.2 Regressão via LASSO",
    "text": "5.2 Regressão via LASSO\nUm problema da regressão rígida, se comparada aos métodos de seleção de variáveis, como a eliminação para trás, é que esse método não faz seleção de coeficientes ou termos no modelo. Ou seja, a regressão rígida não reduz o modelo a partir da exclusão de variáveis correlacionadas, o que implica na manutenção de preditores redundantes no modelo. Como visto, a regressão rígida apenas encolhe os coeficientes.\nUma alternativa à regressão rígida que promove a seleção de coeficientes é a regressão pelo operador de seleção e contração mínima absoluta ou Least Absolute Shrinkage and Selection Operator - LASSO. A regressão LASSO muda a penalização da função perda considerando a norma \\(L_1\\), \\(\\sum_{j=1}^k|\\beta_j|\\) em detrimento da norma \\(L_2\\), \\(\\sum_{j=1}^k\\beta_j^2\\), usada no caso rígido.\n\\[\n\\begin{aligned}\nL(\\mathbf{\\beta,\\lambda}) = \\sum_{i=1}^N(y_i-\\beta_0-\\sum_{j=1}^k\\beta_jx_{ij})^2 + \\lambda\\sum_{j=1}^k|\\beta_j|\n\\end{aligned}\n\\]\nNo caso da regressão via LASSO, não há solução fechada ou equações normais, como no caso rígido. Logo, a formulação acima deve ser minimizada via programação quadrática para estimar os coeficientes.\nA Figura 5.5 plota os valores dos coeficientes no exemplo anterior em função do parâmetro de encolhimento e seleção de variáveis, \\(\\lambda\\), na regressão via LASSO. É possível observar que à medida que \\(\\lambda\\) aumenta, alguns coeficientes vão sendo anulados e, portanto, excluídos do modelo.\n\n\n\n\n\n\n\n\nFigura 5.5: Coeficientes versus penalização de encolhimento na regressão via LASSO\n\n\n\n\n\nTomando o exemplo anterior para o caso LASSO, o valor ótimo do parâmetro de encolhimento e selção é \\(\\lambda^*=2,2\\), conforme Figura 5.6.\n\n\n\n\n\n\n\n\nFigura 5.6: Validação cruzada e grid search para buscar o lambda ótimo na regressão via LASSO\n\n\n\n\n\nOs coeficientes de regressão via LASSO para o \\(\\lambda\\) ótimo são expostos à seguir. Pode-se observar que neste caso é realizada a seleção dos coeficientes mais importantes, sendo removidos aqueles das variáveis que apresentam multicolineariedade e, logo, menor importância em relação às selecionadas.\n\n\n9 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept)      179.2195\ncapitalcost     -175.5755\nlaborcost          .     \nenergycost     -6210.1544\nmaterialscost      .     \ncapitalprice       .     \nlaborprice       276.2006\nenergyprice        .     \nmaterialsprice     .",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regressão rígida e LASSO</span>"
    ]
  },
  {
    "objectID": "AS_ridge_LASSO.html#formulações-alternativas-para-regressão-rígida-e-lasso",
    "href": "AS_ridge_LASSO.html#formulações-alternativas-para-regressão-rígida-e-lasso",
    "title": "5  Regressão rígida e LASSO",
    "section": "5.3 Formulações alternativas para regressão rígida e LASSO",
    "text": "5.3 Formulações alternativas para regressão rígida e LASSO\nUma formulação alternativa para o problema de regressão rígida pode ser expressa como segue, onde \\(s\\) seria um parâmetro associado a \\(\\lambda\\) que limita o crescimento dos coeficientes.\n\\[\n\\begin{aligned}\nMin \\Biggl\\{\\sum_{i=1}^N(y_i-\\beta_0-\\sum_{j=1}^k\\beta_jx_{ij})^2\\Biggl\\} \\\\\nst.:\\sum_{j=1}^k\\beta_j^2 \\leq s\n\\end{aligned}\n\\]\nPara o caso da regressão via LASSO a formulação fica conforme segue.\n\\[\n\\begin{aligned}\nMin \\Biggl\\{\\sum_{i=1}^N(y_i-\\beta_0-\\sum_{j=1}^k\\beta_jx_{ij})^2\\Biggl\\} \\\\\nst.:\\sum_{j=1}^k|\\beta_j| \\leq s\n\\end{aligned}\n\\]\nA Figura 5.7 ilustra a função perda quadrática sendo minimizada segundo a formulação de regressão rígida exposta. Pode-se observar que o valor dos coeficientes de mínimos quadrados são encolhidos quando a restrição de regressão rígida é imposta.\n\n\n\n\n\n\n\n\nFigura 5.7: Perda na regressão rígida versus mínims quadrados",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regressão rígida e LASSO</span>"
    ]
  },
  {
    "objectID": "AS_ridge_LASSO.html#implementações-em-r",
    "href": "AS_ridge_LASSO.html#implementações-em-r",
    "title": "5  Regressão rígida e LASSO",
    "section": "5.4 Implementações em R",
    "text": "5.4 Implementações em R\nA seguir serão expostas as implementações necessárias para obter os resultados do capítulo.\n\n5.4.1 Obtenção dos dados e processamento inicial\n\nlibrary(AER)\nlibrary(corrplot)\nlibrary(GGally)\nlibrary(glmnet)\n\n\ndata(\"ManufactCosts\")\ndados &lt;- data.frame(ManufactCosts)\nggpairs(dados) + theme_bw()\n\n\nr &lt;- cor(dados)\ncorrplot::corrplot(r, \n                   method = \"color\",\n                   type = \"upper\", \n                   order = \"hclust\",\n                   addCoef.col = \"black\",\n                   tl.srt = 45, diag = F)\n\n\nX &lt;- model.matrix(cost ~ ., dados)[,-1]\ny &lt;- dados$cost\n\ntr &lt;- round(0.5*nrow(dados))\nset.seed(45)\ntreino &lt;- sample(1:nrow(dados), tr, replace = F)\nX.treino &lt;- X[treino,]\ny.treino &lt;- y[treino]\n\n\n\n5.4.2 Regressão rígida\nObtenção do modelo para todo o grid do hiperparâmetro lambda.\n\ngrid &lt;- 10^seq(10, 1, length = 100)\n\nrid1 &lt;- glmnet(X.treino, y.treino, alpha = 0, lambda = grid)\n\nplot(rid1, xvar = \"lambda\", col = 1:8)\nlegend(\"bottomright\", lwd = 1, col = 1:8,\n       legend = colnames(X.treino), cex = 0.7)\n\nValidação cruzada e grid search para seleção do lambda ótimo.\n\nrid.cv &lt;- cv.glmnet(X.treino, y.treino, alpha = 0)\nplot(rid.cv)\nbestlam &lt;- rid.cv$lambda.min # melhor lambda\n\nObtendo os coeficientes de regressão rígica com lambda ótimo.\n\nout &lt;- glmnet(X, y, alpha = 0)\npredict(out, type = \"coefficients\", s = bestlam)\n\n\nmetrics &lt;- function(obs, pred) {\n  \n  RSE &lt;- sum((obs - pred)^2)\n  SST &lt;- sum((obs - mean(obs))^2)\n  R2 &lt;- 1 - RSE/SST \n  \n  MAE &lt;-  mean(abs(obs - pred))\n  \n  RMSE &lt;- sqrt(mean((obs - pred)^2))\n  \n  return(\n    data.frame(RMSE = RMSE,\n               MAE = MAE,\n               R2 = R2))\n}\n\nDesempenho para dados de teste.\n\npred.teste &lt;- predict(out, type = \"response\",\n                      newx = X[-treino,], \n                      s = bestlam)\nmetrics(y[-treino], pred.teste)\n\n\n\n5.4.3 Regressão via LASSO\nObtenção do modelo para todo o grid do hiperparâmetro lambda.\n\ngrid &lt;- 10^seq(10, -2, length = 100)\nlasso1 &lt;- glmnet(X.treino, y.treino, alpha = 1, lambda = grid)\n\nplot(lasso1, xvar = \"lambda\", col = 1:8)\nlegend(\"bottomright\", lwd = 1, col = 1:8,\n       legend = colnames(X.treino), cex = 0.7)\n\nValidação cruzada e grid search para seleção do lambda ótimo.\n\nlasso.cv &lt;- cv.glmnet(X.treino, y.treino, alpha = 1)\nplot(lasso.cv)\nbestlam2 &lt;- lasso.cv$lambda.min # lambda otimo\n\nObtendo os coeficientes de regressão rígica com lambda ótimo.\n\nout2 &lt;- glmnet(X, y, alpha = 1)\npredict(out2, type = \"coefficients\", s = bestlam2)\n\nDesempenho para dados de teste.\n\npred.teste2 &lt;- predict(out2, type = \"response\",\n                      newx = X[-treino,], \n                      s = bestlam2)\nmetrics(y[-treino], pred.teste2)\n\n\n\n5.4.4 Regressão por mínimos quadrados ordinários\n\nlm1 &lt;- lm(cost ~ ., dados, subset = treino)\n\nDesempenho para dados de teste.\n\npred.teste3 &lt;- predict(lm1, \n                       newdata = dados[-treino,])\n\nmetrics(y[-treino], pred.teste3)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regressão rígida e LASSO</span>"
    ]
  },
  {
    "objectID": "AS_ridge_LASSO.html#referências",
    "href": "AS_ridge_LASSO.html#referências",
    "title": "5  Regressão rígida e LASSO",
    "section": "Referências",
    "text": "Referências\nHastie, T., Tibshirani, R., Friedman, J. H., & Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction (Vol. 2, pp. 1-758). New York: springer.\nGareth, J., Daniela, W., Trevor, H., & Robert, T. (2013). An introduction to statistical learning: with applications in R. Spinger.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regressão rígida e LASSO</span>"
    ]
  },
  {
    "objectID": "AS_cv.html#grid-search",
    "href": "AS_cv.html#grid-search",
    "title": "6  Validação cruzada e grid search",
    "section": "6.5 Grid search",
    "text": "6.5 Grid search\nGrid search consiste em uma busca dos níveis adequados dos hiperparâmetros considerando por exemplo um grid ou uma malha de combinações possíveis dos níveis de interesse dos hiperparâmetros em questão. Dependendo do método de aprendizado utilizado a busca torna-se mais difícil e custosa computacionalmente, devido ao número de hiperparâmetros a serem otimizados.\nPor exemplo, no caso do aprendizado utilizando regressão por vetores de suporte, deve-se definir o melhor kernel, além dos níveis ótimos da constante de regularização, \\(C\\), e do erro máximo da função perda insensitiva \\(\\varepsilon\\). Dependendo do tipo de kernel utilizado, outros hiperparâmetros devem ser otimizados.\nSeja o caso onde supostamente sabe-se que o kernel polinomial é o mais adequado e deseja-se otimizar \\(C\\) e o grau do polinômio. Um possível grid regular de hiperparâmetros a ser testado é plotado na Figura 6.2.\n\n\n\n\n\n\n\n\nFigura 6.2: Exemplo de grid regular para máquinas de vetores de suporte\n\n\n\n\n\nPode-se constatar que o grid regular consiste em um planejamento fatorial, ou seja, todas as combinações dos níveis dos hiperparâmetros são consideradas. Há também grids irregulares, especialmente para hiperparâmetros em escala real, sendo empregados planejamentos de preenchimento de espaço viavel, tais como amostragem por hipercubo latino.\nExistem diversas técnicas para realizar o grid search de forma mais eficiente, evitando a busca em regiões que tendem a apresentar baixo desempenho dos modelos. Há pacotes computacionais que viabilizam a aplicação de tais técnicas.\nO grid search deve ser usado junto com a validação cruzada. Por exemplo, ao se usar a validação cruzada via \\(k\\)-dobras, uma busca exaustiva testará cada combinação dos níveis de interesse dos hiperparâmetros em cada dobra.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Validação cruzada e *grid search*</span>"
    ]
  },
  {
    "objectID": "AS_cv.html#exemplo-validação-cruzada-por-k-dobras-com-grid-search-para-seleção-do-hiperparâmetro-de-encolhimento-em-regressão-rígida",
    "href": "AS_cv.html#exemplo-validação-cruzada-por-k-dobras-com-grid-search-para-seleção-do-hiperparâmetro-de-encolhimento-em-regressão-rígida",
    "title": "6  Validação cruzada e grid search",
    "section": "6.6 Exemplo: Validação cruzada por \\(k\\)-dobras com grid search para seleção do hiperparâmetro de encolhimento em regressão rígida",
    "text": "6.6 Exemplo: Validação cruzada por \\(k\\)-dobras com grid search para seleção do hiperparâmetro de encolhimento em regressão rígida\nSejam os dados de medições de massa de órgãos de 30 focas mortas não intencionalmente em consequência da pesca comercial.\nConsideremos um problema de previsão da idade destes animais em função da massa total, massa do coração, do fígado, do estômago e do rim. A Tabela 6.1 expõe algumas linhas do conjunto de dados.\n\n\n\n\nTabela 6.1: Conjunto de dados de idade e massa de órgãos de focas\n\n\n\n\n\n\n\n\n\nage\nweight\nheart\nliver\nstomach\nkidney\n\n\n\n\n33\n27.5\n127.7\n855.0\n338.2\n215\n\n\n10\n24.3\n93.2\n435.4\n120.4\n127\n\n\n10\n22.0\n84.5\n530.0\n237.6\n112\n\n\n10\n18.5\n85.4\n542.0\n193.1\n139\n\n\n12\n28.0\n182.0\n949.0\n400.0\n238\n\n\n18\n23.8\n130.0\n1100.0\n443.2\n293\n\n\n\n\n\n\n\n\n\n\nObviamente, espera-se uma alta correlação entre tais variáveis, todas medidas em escala de massa, conforme observado na Figura 6.3.\n\n\n\n\n\n\n\n\nFigura 6.3: Gráfico de correlações aos pares para o conjunto de dados de focas\n\n\n\n\n\nDevido ao número baixo de observações e a alta correlação entre os preditores, uma escolha razoável para modelagem seria a regressão rígida, de forma a buscar-se encolher os coeficientes em razão de uma possível inflação decorrente da multicolineariedade dos preditores. Neste método é importante realizar a escolha do hiperparâmetro de encolhimento \\(\\lambda\\). Uma vez que deve-se selecionar o nível ideal deste hiperparâmetro para garantir melhor genaralização, na validação cruzada via \\(k\\)-dobras, é importante testar um conjunto de funções, \\(\\hat{f}^{-k(i)}(\\mathbf{x}_i,\\lambda_l)\\), onde \\(\\lambda_l\\) seria o nível do hiperparâmetro a ser testado, considerando um vetor ou grid de níveis tentativos, \\(l=1,...,L\\). Deve-se testar cada modelo em cada partição ou dobra, ou seja, serão testados ao final \\(K\\times L\\) modelos, sendo o melhor valor de \\(\\lambda\\) escolhido comparando o valor médio do erro obtido na validação,\n\\[\n    CV_l(\\hat{h}, \\lambda_l) = \\frac{1}{N} \\sum_{i=1}^N L(y_i, \\hat{h}^{-k(i)}(\\mathbf{x}_i,\\lambda_l))\n\\] para todo \\(l=1,...,L\\).\nTomando o conjunto de dados, inicialmente faz-se o sorteio das observações de cada partição. Neste caso, para \\(N=30\\) e tomando \\(K=10\\) dobras, tem-se exatamente \\(n_k=30/10=3\\) observações por dobra. A Tabela 6.2 expõe todo o conjunto de dados considerando o particionamento realizado de forma aleatória, identificando cada dobra com uma cor distinta e com a coluna fold.\n\n\n\n\nTabela 6.2: Conjunto de dados de focas particionado via \\(k\\)-fold\n\n\n\n\n\n\n\n\n\nage\nweight\nheart\nliver\nstomach\nkidney\nfold\n\n\n\n\n33\n27.5\n127.7\n855.0\n338.2\n215\n10\n\n\n10\n24.3\n93.2\n435.4\n120.4\n127\n9\n\n\n10\n22.0\n84.5\n530.0\n237.6\n112\n8\n\n\n10\n18.5\n85.4\n542.0\n193.1\n139\n7\n\n\n12\n28.0\n182.0\n949.0\n400.0\n238\n2\n\n\n18\n23.8\n130.0\n1100.0\n443.2\n293\n5\n\n\n19\n18.5\n98.8\n627.2\n265.9\n181\n2\n\n\n23\n18.0\n117.3\n632.9\n280.8\n172\n8\n\n\n31\n34.0\n180.0\n1040.0\n425.0\n245\n3\n\n\n33\n28.5\n150.4\n1047.1\n476.2\n248\n3\n\n\n44\n35.8\n145.5\n1525.0\n475.6\n332\n5\n\n\n46\n30.3\n140.0\n1210.0\n360.3\n232\n1\n\n\n46\n33.0\n168.1\n1195.0\n384.9\n249\n2\n\n\n50\n42.0\n205.0\n1415.0\n560.0\n305\n9\n\n\n50\n37.5\n205.0\n1099.8\n543.0\n274\n6\n\n\n60\n55.0\n320.9\n2250.0\n961.8\n427\n10\n\n\n59\n50.5\n255.0\n2115.0\n850.0\n415\n6\n\n\n66\n57.5\n370.0\n2195.0\n720.0\n450\n4\n\n\n68\n67.5\n465.0\n2990.0\n865.0\n530\n1\n\n\n68\n53.5\n332.8\n2124.2\n730.0\n457\n5\n\n\n72\n73.0\n449.0\n2715.0\n1044.0\n605\n4\n\n\n73\n54.5\n282.5\n2795.0\n919.0\n541\n3\n\n\n73\n68.5\n355.0\n3875.0\n1390.0\n625\n6\n\n\n80\n74.0\n310.0\n3130.0\n985.8\n585\n7\n\n\n84\n68.0\n532.0\n2937.0\n1209.0\n620\n7\n\n\n96\n97.0\n667.0\n4241.0\n1542.0\n757\n10\n\n\n106\n101.0\n440.0\n3580.0\n1260.0\n760\n9\n\n\n108\n128.0\n1012.0\n5172.0\n1734.0\n888\n4\n\n\n118\n95.0\n505.0\n3720.0\n1245.0\n785\n8\n\n\n120\n179.0\n1075.0\n8309.0\n2500.0\n1410\n1\n\n\n\n\n\n\n\n\n\n\nOrdenando segundo a coluna das dobras ou folds, tem-se a Tabela 6.3.\n\n\n\n\nTabela 6.3: Conjunto de dados de focas ordenado segundo as partições\n\n\n\n\n\n\n\n\n\nage\nweight\nheart\nliver\nstomach\nkidney\nfold\n\n\n\n\n46\n30.3\n140.0\n1210.0\n360.3\n232\n1\n\n\n68\n67.5\n465.0\n2990.0\n865.0\n530\n1\n\n\n120\n179.0\n1075.0\n8309.0\n2500.0\n1410\n1\n\n\n12\n28.0\n182.0\n949.0\n400.0\n238\n2\n\n\n19\n18.5\n98.8\n627.2\n265.9\n181\n2\n\n\n46\n33.0\n168.1\n1195.0\n384.9\n249\n2\n\n\n31\n34.0\n180.0\n1040.0\n425.0\n245\n3\n\n\n33\n28.5\n150.4\n1047.1\n476.2\n248\n3\n\n\n73\n54.5\n282.5\n2795.0\n919.0\n541\n3\n\n\n66\n57.5\n370.0\n2195.0\n720.0\n450\n4\n\n\n72\n73.0\n449.0\n2715.0\n1044.0\n605\n4\n\n\n108\n128.0\n1012.0\n5172.0\n1734.0\n888\n4\n\n\n18\n23.8\n130.0\n1100.0\n443.2\n293\n5\n\n\n44\n35.8\n145.5\n1525.0\n475.6\n332\n5\n\n\n68\n53.5\n332.8\n2124.2\n730.0\n457\n5\n\n\n50\n37.5\n205.0\n1099.8\n543.0\n274\n6\n\n\n59\n50.5\n255.0\n2115.0\n850.0\n415\n6\n\n\n73\n68.5\n355.0\n3875.0\n1390.0\n625\n6\n\n\n10\n18.5\n85.4\n542.0\n193.1\n139\n7\n\n\n80\n74.0\n310.0\n3130.0\n985.8\n585\n7\n\n\n84\n68.0\n532.0\n2937.0\n1209.0\n620\n7\n\n\n10\n22.0\n84.5\n530.0\n237.6\n112\n8\n\n\n23\n18.0\n117.3\n632.9\n280.8\n172\n8\n\n\n118\n95.0\n505.0\n3720.0\n1245.0\n785\n8\n\n\n10\n24.3\n93.2\n435.4\n120.4\n127\n9\n\n\n50\n42.0\n205.0\n1415.0\n560.0\n305\n9\n\n\n106\n101.0\n440.0\n3580.0\n1260.0\n760\n9\n\n\n33\n27.5\n127.7\n855.0\n338.2\n215\n10\n\n\n60\n55.0\n320.9\n2250.0\n961.8\n427\n10\n\n\n96\n97.0\n667.0\n4241.0\n1542.0\n757\n10\n\n\n\n\n\n\n\n\n\n\nTestando-se um grid de 100 níveis para \\(\\lambda\\) variando de \\(10^{10}\\) até \\(10^{-2}\\), pode-se observar na Figura 6.4 o valor de RMSE de teste em cada dobra para cada \\(\\lambda\\) testado.\n\n\n\n\n\n\n\n\nFigura 6.4: Erro do modelo de regressão rígida versus penalização de encolhimento em cada partição para prever a idade das focas\n\n\n\n\n\nA variação em RMSE observada entre as dobras está relacionada ao número baixo de observações no exemplo em questão. Entretanto, fica claro que um \\(\\lambda\\) mais baixo apresentará melhor desempenho. Conforme já esclarecido, deve-se tomar a média do erro de teste para todos as dobras ou partições, conforme plotado na Figura 6.5 com intervalo de confiança.\n\n\n\n\n\n\n\n\nFigura 6.5: Intervalo de confiança para o erro do modelo de regressão rígida versus penalização de encolhimento\n\n\n\n\n\nObservação: A escolha de hiperparâmetros em alguns métodos, sobretudo os de origem estatística, pode ser baseada também em critérios de informação, como o AIC. Entretanto, neste curso, dar-se-á ênfase na validação cruzada uma vez que esta técnica é útil tanto para os métodos de origem na estatística quanto para os de origem na computação, além de ser simples de implementar.\nÉ importante recordar que após a realização da validação cruzada e otimização dos hiperparâmetros geralmente recomenda-se estimar o modelo com os hiperparâmetros otimizados em todo o conjunto de dados.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Validação cruzada e *grid search*</span>"
    ]
  }
]